# 计算机网络



## 输入一个域名

1. 浏览器依次查找浏览器缓存、操作系统缓存、Host文件、路由器缓存，如果都找不到域名对应的IP地址的话，就去查找DNS服务器。DNS服务器如果有对应的缓存的话，就返回，如果没有就要迭代查询根域名服务器、顶级域名服务器、权威域名服务器，然后返回IP地址给DNS服务器，DNS服务器会给主机返回这个IP地址，并且把这个地址缓存下来
2. 建立TCP连接，三次握手
3. 发送HTPP请求，服务器处理请求，返回响应结果
4. 关闭TCP连接，四次挥手

https://github.com/skyline75489/what-happens-when-zh_CN#tls 超级底层

## Cookie和Session的区别

- **作用范围不同**，Cookie 保存在客户端，Session 保存在服务器端。
- **有效期不同**，Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。
- **隐私策略不同**，Cookie 存储在客户端，容易被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。
- **存储大小不同**， 单个 Cookie 保存的数据（一般）不能超过 4K；对于 Session 来说存储没有上限，但出于对服务器的性能考虑，Session 内不要存放过多的数据，并且需要设置 Session 删除机制。

Cookie是在服务端生成的，保存在客户端的一种信息载体。浏览器再次向服务器发起请求的时候，请求里面就会携带这个cookie。通常，cookie是用于告知服务端两个请求是否来自同一浏览器，比如要保持用户的登录状态的时候，就要使用到cookie。

用户在提交第一次请求的时候，服务器就会生成cookie，封装到响应头里面去。客户端接收到这个响应后，就会把cookie保存到客户端。客户端再次发送请求的时候，请求里面就会携带上保存在客户端的cookie数据，发送到服务端。

Session 也是一种会话状态的跟踪技术，不同的是cookie将会话状态保存在客户端，而session把会话状态保存在了服务器。什么是会话，当用户打开浏览器，发出第一次请求，一直到关闭浏览器，一次会话就算结束。

两者如何结合：

用户第一次请求服务器的时候，服务器创建对应的 Session ，请求返回时将此 Session 的唯一标识信息 SessionID放到cookie里面，返回给客户端，浏览器接收到这个 SessionID之后，就会放在浏览器缓存里面

用户再次提交请求的时候，就会把缓存里的cookie发送到服务器那边

session是一种机制，除了利用cookie来实现session机制，还可以通过在URL末尾那里附加ID来追踪session

---

token

特定时间有大量用户访问服务器的时候，服务器可能需要存储大量SessionID，但是如果有多台服务器，一台服务器存储了SessionID，又会面临需要分享SessionID给其他服务器的情况，因为可能出现这台服务器的超载，需要分配一些用户到其他服务器，其他服务器需要通用的SessionID才可以避免用户再次输入用户名和密码。但是服务器这样分享也不是办法（你这一台服务器上有这个sessionid，但是其他的服务器不一定有）

1. 客户端发送请求给服务器，服务器会签发一个token，把这个token发给客户端
2. 客户端收到 token 以后，会把它存储起来，比如放在 cookie 里或者 localStorage 里
3. 客户端每次向服务端请求资源的时候需要带着服务端签发的 token
4. 服务端收到请求，然后去验证客户端请求里面带着的 token ，如果验证成功，就向客户端返回请求的数据

- **每一次请求都需要携带 token，需要把 token 放到 HTTP 的 Header 里**
- **基于 token 的用户认证是一种服务端无状态的认证方式，服务端不用存放 token 数据。==用解析 token 的计算时间换取 session 的存储空间==，从而减轻服务器的压力，减少频繁的查询数据库**
- **token 完全由应用管理，所以它可以避开同源策略**

## HTTP

### HTTP有哪些方法？

HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD 方法。

HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。

| 序号 | 方法    | 描述                                                         |
| :--- | :------ | :----------------------------------------------------------- |
| 1    | GET     | 请求指定的页面信息，并返回实体主体。                         |
| 2    | HEAD    | 类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头 |
| 3    | POST    | 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。 |
| 4    | PUT     | 从客户端向服务器传送的数据取代指定的文档的内容。             |
| 5    | DELETE  | 请求服务器删除指定的页面。                                   |
| 6    | CONNECT | HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。    |
| 7    | OPTIONS | 允许客户端查看服务器的性能。                                 |
| 8    | TRACE   | 回显服务器收到的请求，主要用于测试或诊断。                   |
| 9    | PATCH   | 是对 PUT 方法的补充，用来对已知资源进行局部更新 。           |

请求报文有4部分组成:**请求行 请求头部** 空行 **请求体**

请求报文有4部分组成:`响应行 响应头  空行  响应体`

![img](D:\mdImage\1460000006760778)

### HTTP状态码

![image-20220223171355033](.\img\image-20220223171355033.png)

2XX 成功  200 OK，表示从客户端发来的请求在服务器端被正确处理

3XX 重定向 

301 moved permanently，永久性重定向，表示资源已被分配了新的 URL 

302 found，临时性重定向，表示资源临时被分配了新的 URL

4XX 客户端错误 表示在服务器上没有找到请求的资源

5XX 服务器错误 500 internal sever error，表示服务器端在执行请求时发生了错误





https://www.runoob.com/http/http-status-codes.html

| 分类 | 分类描述                                       |
| :--- | :--------------------------------------------- |
| 1**  | 信息，服务器收到请求，需要请求者继续执行操作   |
| 2**  | 成功，操作被成功接收并处理                     |
| 3**  | 重定向，需要进一步的操作以完成请求             |
| 4**  | 客户端错误，请求包含语法错误或无法完成请求     |
| 5**  | 服务器错误，服务器在处理请求的过程中发生了错误 |

| 状态码 | 状态码英文名称                  | 中文描述                                                     |
| :----- | :------------------------------ | :----------------------------------------------------------- |
| 100    | Continue                        | 继续。客户端应继续其请求                                     |
| 101    | Switching Protocols             | 切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议，例如，切换到HTTP的新版本协议 |
|        |                                 |                                                              |
| 200    | OK                              | 请求成功。一般用于GET与POST请求                              |
| 201    | Created                         | 已创建。成功请求并创建了新的资源                             |
| 202    | Accepted                        | 已接受。已经接受请求，但未处理完成                           |
| 203    | Non-Authoritative Information   | 非授权信息。请求成功。但返回的meta信息不在原始的服务器，而是一个副本 |
| 204    | No Content                      | 无内容。服务器成功处理，但未返回内容。在未更新网页的情况下，可确保浏览器继续显示当前文档 |
| 205    | Reset Content                   | 重置内容。服务器处理成功，用户终端（例如：浏览器）应重置文档视图。可通过此返回码清除浏览器的表单域 |
| 206    | Partial Content                 | 部分内容。服务器成功处理了部分GET请求                        |
|        |                                 |                                                              |
| 300    | Multiple Choices                | 多种选择。请求的资源可包括多个位置，相应可返回一个资源特征与地址的列表用于用户终端（例如：浏览器）选择 |
| 301    | Moved Permanently               | 永久移动。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 |
| 302    | Found                           | 临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI |
| 303    | See Other                       | 查看其它地址。与301类似。使用GET和POST请求查看               |
| 304    | Not Modified                    | 未修改。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源 |
| 305    | Use Proxy                       | 使用代理。所请求的资源必须通过代理访问                       |
| 306    | Unused                          | 已经被废弃的HTTP状态码                                       |
| 307    | Temporary Redirect              | 临时重定向。与302类似。使用GET请求重定向                     |
|        |                                 |                                                              |
| 400    | Bad Request                     | 客户端请求的语法错误，服务器无法理解                         |
| 401    | Unauthorized                    | 请求要求用户的身份认证                                       |
| 402    | Payment Required                | 保留，将来使用                                               |
| 403    | Forbidden                       | 服务器理解请求客户端的请求，但是拒绝执行此请求               |
| 404    | Not Found                       | 服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置"您所请求的资源无法找到"的个性页面 |
| 405    | Method Not Allowed              | 客户端请求中的方法被禁止                                     |
| 406    | Not Acceptable                  | 服务器无法根据客户端请求的内容特性完成请求                   |
| 407    | Proxy Authentication Required   | 请求要求代理的身份认证，与401类似，但请求者应当使用代理进行授权 |
| 408    | Request Time-out                | 服务器等待客户端发送的请求时间过长，超时                     |
| 409    | Conflict                        | 服务器完成客户端的 PUT 请求时可能返回此代码，服务器处理请求时发生了冲突 |
| 410    | Gone                            | 客户端请求的资源已经不存在。410不同于404，如果资源以前有现在被永久删除了可使用410代码，网站设计人员可通过301代码指定资源的新位置 |
| 411    | Length Required                 | 服务器无法处理客户端发送的不带Content-Length的请求信息       |
| 412    | Precondition Failed             | 客户端请求信息的先决条件错误                                 |
| 413    | Request Entity Too Large        | 由于请求的实体过大，服务器无法处理，因此拒绝请求。为防止客户端的连续请求，服务器可能会关闭连接。如果只是服务器暂时无法处理，则会包含一个Retry-After的响应信息 |
| 414    | Request-URI Too Large           | 请求的URI过长（URI通常为网址），服务器无法处理               |
| 415    | Unsupported Media Type          | 服务器无法处理请求附带的媒体格式                             |
| 416    | Requested range not satisfiable | 客户端请求的范围无效                                         |
| 417    | Expectation Failed              | 服务器无法满足Expect的请求头信息                             |
|        |                                 |                                                              |
| 500    | Internal Server Error           | 服务器内部错误，无法完成请求                                 |
| 501    | Not Implemented                 | 服务器不支持请求的功能，无法完成请求                         |
| 502    | Bad Gateway                     | 作为网关或者代理工作的服务器尝试执行请求时，从远程服务器接收到了一个无效的响应 |
| 503    | Service Unavailable             | 由于超载或系统维护，服务器暂时的无法处理客户端的请求。延时的长度可包含在服务器的Retry-After头信息中 |
| 504    | Gateway Time-out                | 充当网关或代理的服务器，未及时从远端服务器获取请求           |
| 505    | HTTP Version not supported      | 服务器不支持请求的HTTP协议的版本，无法完成处理               |



#### 详细的描述状态码之（1**）

- `100`: **客户端应当继续发送请求**。这个临时响应是用来通知客户端它的部分请求已经被服务器接收，且仍未被拒绝。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。服务器必须在请求完成后向客户端发送一个最终响应。
- `101`: **服务器已经理解了客户端的请求,并将通过`Upgrade 消息头`通知客户端采用不同的协议来完成这个请求**。在发送完这个响应最后的空行后，服务器将会切换到在`Upgrade 消息头`中定义的那些协议。只有在切换新的协议更有好处的时候才应该采取类似措施。例如，切换到新的`HTTP` 版本比旧版本更有优势，或者切换到一个实时且同步的协议以传送利用此类特性的资源。
- `102`: 由`WebDAV（RFC 2518）`扩展的状态码，代表**处理将被继续执行**。




#### 详细的描述状态码之（2**）

- `200`: **请求已成功**，请求所希望的响应头或数据体将随此响应返回。
- `201`: **请求已经被实现**，而且有一个新的资源已经依据请求的需要而建立，且其 `URI` 已经随`Location` 头信息返回。假如需要的资源无法及时建立的话，应当返回 `'202 Accepted'`。
- `202`: **服务器已接受请求，但尚未处理**。正如它可能被拒绝一样，最终该请求可能会也可能不会被执行。**在异步操作的场合下，没有比发送这个状态码更方便的做法了**。返回`202状态码`的响应的目的是允许服务器接受其他过程的请求（`例如某个每天只执行一次的基于批处理的操作`），而不必让客户端一直保持与服务器的连接直到批处理操作全部完成。在接受请求处理并返回`202状态码`的响应应当在返回的实体中包含一些`指示处理当前状态`的信息，以及指向处理状态监视器或状态预测的指针，以便用户能够估计操作是否已经完成。
- `203`: **服务器已成功处理了请求**，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。
- `204`: **服务器成功处理了请求，但不需要返回任何实体内容，并且希望返回更新了的元信息。响应可能通过实体头部的形式，返回新的或更新后的元信息**。如果存在这些头部信息，则应当与所请求的变量相呼应。如果客户端是浏览器的话，那么用户浏览器应保留发送了该请求的页面，而不产生任何文档视图上的变化，即使按照规范新的或更新后的元信息应当被应用到用户浏览器活动视图中的文档。由于 `204` 响应被禁止包含任何消息体，因此它始终以消息头后的第一个空行结尾。
- `205`: **服务器成功处理了请求，且没有返回任何内容**。但是与204响应不同，返回此状态码的响应要求请求者重置文档视图。该响应主要是被用于接受用户输入后，立即重置表单，以便用户能够轻松地开始另一次输入。
- `206`: **服务器已经成功处理了部分 GET 请求**。类似于 `FlashGet` 或者迅雷这类的 `HTTP` 下载工具都是使用此类响应实现断点续传或者将一个大文档分解为多个下载段同时下载。该请求必须包含 `Range` 头信息来指示客户端希望得到的内容范围，并且可能包含 `If-Range` 来作为请求条件。响应必须包含如下的头部域:`Content-Range`用以指示本次响应中返回的内容的范围；如果是 `Content-Type` 为 `multipart/byteranges` 的多段下载，则每一 `multipart` 段中都应包含 `Content-Range` 域用以指示本段的内容范围。假如响应中包含`Content-Length`，那么它的数值必须匹配它返回的内容范围的真实字节数。`Date ETag 和/或Content-Location`，假如同样的请求本应该返回`200响应`。`Expires, Cache-Control，和/或 Vary`，假如其值可能与之前相同变量的其他响应对应的值不同的话。假如本响应请求使用了 `If-Range` 强缓存验证，那么本次响应不应该包含其他实体头；假如本响应的请求使用了 `If-Range`弱缓存验证，那么本次响应禁止包含其他实体头；这避免了缓存的实体内容和更新了的实体头信息之间的不一致。否则，本响应就应当包含所有本应该返回`200响应`中应当返回的所有实体头部域。假如 `ETag` 或 `Last-Modified`头部不能精确匹配的话，则客户端缓存应禁止将`206响应`返回的内容与之前任何缓存过的内容组合在一起。任何不支持 `Range` 以及 `Content-Range`头的缓存都禁止缓存`206响应`返回的内容。
- `207`: 由 `WebDAV(RFC 2518)`扩展的状态码，代表之后的消息体将是一个`XML`消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。

#### 详细的描述状态码之（3**）

- `300`:被请求的资源有一系列可供选择的回馈信息，每个都有自己特定的地址和浏览器驱动的商议信息。用户或浏览器能够自行选择一个首选的地址进行重定向。
- `301`:被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个 URI 之一。
- `302`:请求的资源现在临时从不同的URI响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。
- `303`:对应当前请求的响应可以在另一个 URI 上被找到，而且客户端应当采用 GET 的方式访问那个资源。这个方法的存在主要是为了允许由脚本激活的`POST`请求输出重定向到一个新的资源。这个新的 `URI` 不是原始资源的替代引用。同时，`303响应`禁止被缓存。
- `304`:如果客户端发送了一个带条件的`GET请求`且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个状态码。
- `305`:被请求的资源必须通过指定的代理才能被访问。Location 域中将给出指定的代理所在的 URI 信息，接收者需要重复发送一个单独的请求，通过这个代理才能访问相应资源。只有原始服务器才能建立305响应。
- `306`:在最新版的规范中，306状态码已经不再被使用。
- `307`:请求的资源现在临时从不同的URI响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求

#### 详细的描述状态码之（4**）

相对于其它状态码4的就比较多了，天地万物，且听在下娓娓道来。

- `400`:1、语义有误，当前请求无法被服务器理解。除非进行修改，否则客户端不应该重复提交这个请求。2、请求参数有误。
- `401`:当前请求需要用户验证。该响应必须包含一个适用于被请求资源的 `WWW-Authenticate` 信息头用以询问用户信息。客户端可以重复提交一个包含恰当的`Authorization`头信息的请求。如果当前请求已经包含了 `Authorization`证书，那么`401响应`代表着服务器验证已经拒绝了那些证书。如果`401响应`包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。
- `402`:该状态码是为了将来可能的需求而预留的。
- `403`:服务器已经理解请求，但是拒绝执行它。与`401响应`不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。
- `404`:请求失败，请求所希望得到的资源未被在服务器上发现。没有信息能够告诉用户这个状况到底是暂时的还是永久的。
- `405`:请求行中指定的请求方法不能被用于请求相应的资源。该响应必须返回一个`Allow 头信息`用以表示出当前资源能够接受的请求方法的列表。
- `406`:请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体。
- `407`:与`401响应`类似，只不过客户端必须在代理服务器上进行身份验证。
- `408`:请求超时。客户端没有在服务器预备等待的时间内完成一个请求的发送。客户端可以随时再次提交这一请求而无需进行任何更改。
- `409`:由于和被请求的资源的当前状态之间存在冲突，请求无法完成。
- `410`:被请求的资源在服务器上已经不再可用，而且没有任何已知的转发地址。
- `411`:服务器拒绝在没有定义Content-Length头的情况下接受请求。在添加了表明请求消息体长度的有效 `Content-Length`头之后，客户端可以再次提交该请求。
- `412`:服务器在验证在请求的头字段中给出先决条件时，没能满足其中的一个或多个。
- `413`:服务器拒绝处理当前请求，因为该请求提交的实体数据大小超过了服务器愿意或者能够处理的范围。
- `414`:请求的`URI` 长度超过了服务器能够解释的长度，因此服务器拒绝对该请求提供服务。
- `415`:对于当前请求的方法和所请求的资源，请求中提交的实体并不是服务器中所支持的格式，因此请求被拒绝。
- `416`:如果请求中包含了`Range`请求头，并且`Range`中指定的任何数据范围都与当前资源的可用范围不重合，同时请求中又没有定义 `If-Range` 请求头，那么服务器就应当返回`416状态码`。
- `417`: 在请求头 `Expect`中指定的预期内容无法被服务器满足，或者这个服务器是一个代理服务器，它有明显的证据证明在当前路由的下一个节点上，Expect 的内容无法被满足。
- `421`: 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。
- `423`: 请求格式正确，但是由于含有语义错误，无法响应。
- `424`: 由于之前的某个请求发生的错误，导致当前请求失败，例如 `PROPPATCH`。
- `425`: 在`WebDav Advanced Collections` 草案中定义，但是未出现在《WebDAV 顺序集协议》（RFC 3658）中。
- `426`: 客户端应当切换到TLS/1.0。
- `449`: 由微软扩展，代表请求应当在执行完适当的操作后进行重试。

#### 详细的描述状态码之（5**）

- `500`: 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器的程序码出错时出现。
- `501`: 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。
- `502`: 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。
- `503`: 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。注意：503状态码的存在并不意味着服务器在过载的时候必须使用它。某些服务器只不过是希望拒绝客户端的连接。
- `504`: 作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。
- `505`: 服务器不支持，或者拒绝支持在请求中使用的HTTP版本。这暗示着服务器不能或不愿使用与客户端相同的版本。响应中应当包含一个描述了为何版本不被支持以及服务器支持哪些协议的实体。
- `506`: 由《透明内容协商协议》（RFC 2295）扩展，代表服务器存在内部配置错误：被请求的协商变元资源被配置为在透明内容协商中使用自己，因此在一个协商处理中不是一个合适的重点。
- `507`: 服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。`WebDAV (RFC 4918)`
- `509`: 服务器达到带宽限制。这不是一个官方的状态码，但是仍被广泛使用。
- `510`: 获取资源所需要的策略并没有没满足。`（RFC 2774）`


1xx消息
这一类型的状态码，代表请求已被接受，需要继续处理。这类响应是临时响应，只包含状态行和某些可选的响应头信息，并以空行结束。由于HTTP/1.0协议中没有定义任何1xx状态码，所以除非在某些试验条件下，服务器禁止向此类客户端发送1xx响应。[4] 这些状态码代表的响应都是信息性的，标示客户应该采取的其他行动。

100 Continue
服务器已经接收到请求头，并且客户端应继续发送请求主体（在需要发送身体的请求的情况下：例如，POST请求），或者如果请求已经完成，忽略这个响应。服务器必须在请求完成后向客户端发送一个最终响应。要使服务器检查请求的头部，客户端必须在其初始请求中发送Expect: 100-continue作为头部，并在发送正文之前接收100 Continue状态代码。响应代码417期望失败表示请求不应继续。[2]

101 Switching Protocols
服务器已经理解了客户端的请求，并将通过Upgrade消息头通知客户端采用不同的协议来完成这个请求。在发送完这个响应最后的空行后，服务器将会切换到在Upgrade消息头中定义的那些协议。[5] 只有在切换新的协议更有好处的时候才应该采取类似措施。例如，切换到新的HTTP版本（如HTTP/2）比旧版本更有优势，或者切换到一个实时且同步的协议（如WebSocket）以传送利用此类特性的资源。

102 Processing
WebDAV请求可能包含许多涉及文件操作的子请求，需要很长时间才能完成请求。该代码表示服务器已经收到并正在处理请求，但无响应可用。[6]这样可以防止客户端超时，并假设请求丢失。

2xx成功
这一类型的状态码，代表请求已成功被服务器接收、理解、并接受。[2]

200 OK
请求已成功，请求所希望的响应头或数据体将随此响应返回。实际的响应将取决于所使用的请求方法。在GET请求中，响应将包含与请求的资源相对应的实体。在POST请求中，响应将包含描述或操作结果的实体。[7]

201 Created
请求已经被实现，而且有一个新的资源已经依据请求的需要而建立，且其URI已经随Location头信息返回。假如需要的资源无法及时创建的话，应当返回'202 Accepted'。[8]

202 Accepted
服务器已接受请求，但尚未处理。最终该请求可能会也可能不会被执行，并且可能在处理发生时被禁止。[9]

203 Non-Authoritative Information
服务器是一个转换代理服务器（transforming proxy，例如网络加速器），以200 OK状态码为起源，但回应了原始响应的修改版本。[10][11]

204 No Content
服务器成功处理了请求，没有返回任何内容。[12]

205 Reset Content
服务器成功处理了请求，但没有返回任何内容。与204响应不同，此响应要求请求者重置文档视图。[13]

206 Partial Content
服务器已经成功处理了部分GET请求。类似于FlashGet或者迅雷这类的HTTP 下载工具都是使用此类响应实现断点续传或者将一个大文档分解为多个下载段同时下载。[14]

207 Multi-Status
代表之后的消息体将是一个XML消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。[15]

208 Already Reported
DAV绑定的成员已经在（多状态）响应之前的部分被列举，且未被再次包含。

226 IM Used
服务器已经满足了对资源的请求，对实体请求的一个或多个实体操作的结果表示。[16]

3xx重定向
这类状态码代表需要客户端采取进一步的操作才能完成请求。通常，这些状态码用来重定向，后续的请求地址（重定向目标）在本次响应的Location域中指明。[2]

当且仅当后续的请求所使用的方法是GET或者HEAD时，用户浏览器才可以在没有用户介入的情况下自动提交所需要的后续请求。客户端应当自动监测无限循环重定向（例如：A→B→C→……→A或A→A），因为这会导致服务器和客户端大量不必要的资源消耗。按照HTTP/1.0版规范的建议，浏览器不应自动访问超过5次的重定向。[17]

300 Multiple Choices
被请求的资源有一系列可供选择的回馈信息，每个都有自己特定的地址和浏览器驱动的商议信息。用户或浏览器能够自行选择一个首选的地址进行重定向。[18] 除非这是一个HEAD请求，否则该响应应当包括一个资源特性及地址的列表的实体，以便用户或浏览器从中选择最合适的重定向地址。这个实体的格式由Content-Type定义的格式所决定。浏览器可能根据响应的格式以及浏览器自身能力，自动作出最合适的选择。当然，RFC 2616规范并没有规定这样的自动选择该如何进行。 如果服务器本身已经有了首选的回馈选择，那么在Location中应当指明这个回馈的URI；浏览器可能会将这个Location值作为自动重定向的地址。此外，除非额外指定，否则这个响应也是可缓存的。

301 Moved Permanently
被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个URI之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。[19]除非额外指定，否则这个响应也是可缓存的。 新的永久性的URI应当在响应的Location域中返回。除非这是一个HEAD请求，否则响应的实体中应当包含指向新的URI的超链接及简短说明。 如果这不是一个GET或者HEAD请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：对于某些使用HTTP/1.0协议的浏览器，当它们发送的POST请求得到了一个301响应的话，接下来的重定向请求将会变成GET方式。

302 Found
要求客户端执行临时重定向（原始描述短语为“Moved Temporarily”）。[20]由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的URI应当在响应的Location域中返回。除非这是一个HEAD请求，否则响应的实体中应当包含指向新的URI的超链接及简短说明。 如果这不是一个GET或者HEAD请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：虽然RFC 1945和RFC 2068规范不允许客户端在重定向时改变请求的方法，但是很多现存的浏览器将302响应视作为303响应，并且使用GET方式访问在Location中规定的URI，而无视原先请求的方法。[21]因此状态码303和307被添加了进来，用以明确服务器期待客户端进行何种反应。[22]

303 See Other
对应当前请求的响应可以在另一个URI上被找到，当响应于POST（或PUT / DELETE）接收到响应时，客户端应该假定服务器已经收到数据，并且应该使用单独的GET消息发出重定向。[23]这个方法的存在主要是为了允许由脚本激活的POST请求输出重定向到一个新的资源。这个新的URI不是原始资源的替代引用。同时，303响应禁止被缓存。当然，第二个请求（重定向）可能被缓存。 新的URI应当在响应的Location域中返回。除非这是一个HEAD请求，否则响应的实体中应当包含指向新的URI的超链接及简短说明。 注意：许多HTTP/1.1版以前的浏览器不能正确理解303状态。如果需要考虑与这些浏览器之间的互动，302状态码应该可以胜任，因为大多数的浏览器处理302响应时的方式恰恰就是上述规范要求客户端处理303响应时应当做的。

304 Not Modified
表示资源在由请求头中的If-Modified-Since或If-None-Match参数指定的这一版本之后，未曾被修改。在这种情况下，由于客户端仍然具有以前下载的副本，因此不需要重新传输资源。[24]

305 Use Proxy
被请求的资源必须通过指定的代理才能被访问。Location域中将给出指定的代理所在的URI信息，接收者需要重复发送一个单独的请求，通过这个代理才能访问相应资源。只有原始服务器才能创建305响应。许多HTTP客户端（像是Mozilla[25]和Internet Explorer）都没有正确处理这种状态代码的响应，主要是出于安全考虑。[26] 注意：RFC 2068中没有明确305响应是为了重定向一个单独的请求，而且只能被原始服务器建立。忽视这些限制可能导致严重的安全后果。

306 Switch Proxy
在最新版的规范中，306状态码已经不再被使用。最初是指“后续请求应使用指定的代理”。[27]

307 Temporary Redirect
在这种情况下，请求应该与另一个URI重复，但后续的请求应仍使用原始的URI。 与302相反，当重新发出原始请求时，不允许更改请求方法。 例如，应该使用另一个POST请求来重复POST请求。[28]

308 Permanent Redirect
请求和所有将来的请求应该使用另一个URI重复。 307和308重复302和301的行为，但不允许HTTP方法更改。 例如，将表单提交给永久重定向的资源可能会顺利进行。[29]

4xx客户端错误
这类的状态码代表了客户端看起来可能发生了错误，妨碍了服务器的处理。除非响应的是一个HEAD请求，否则服务器就应该返回一个解释当前错误状况的实体，以及这是临时的还是永久性的状况。这些状态码适用于任何请求方法。浏览器应当向用户显示任何包含在此类错误响应中的实体内容。[30]

如果错误发生时客户端正在传送数据，那么使用TCP的服务器实现应当仔细确保在关闭客户端与服务器之间的连接之前，客户端已经收到了包含错误信息的数据包。如果客户端在收到错误信息后继续向服务器发送数据，服务器的TCP栈将向客户端发送一个重置数据包，以清除该客户端所有还未识别的输入缓冲，以免这些数据被服务器上的应用程序读取并干扰后者。

400 Bad Request
由于明显的客户端错误（例如，格式错误的请求语法，太大的大小，无效的请求消息或欺骗性路由请求），服务器不能或不会处理该请求。[31]

401 Unauthorized
参见：HTTP基本认证、HTTP摘要认证 类似于403 Forbidden，401语义即“未认证”，即用户没有必要的凭据。[32]该状态码表示当前请求需要用户验证。该响应必须包含一个适用于被请求资源的WWW-Authenticate信息头用以询问用户信息。客户端可以重复提交一个包含恰当的Authorization头信息的请求。[33]如果当前请求已经包含了Authorization证书，那么401响应代表着服务器验证已经拒绝了那些证书。如果401响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。 注意：当网站（通常是网站域名）禁止IP地址时，有些网站状态码显示的401，表示该特定地址被拒绝访问网站。

402 Payment Required
该状态码是为了将来可能的需求而预留的。该状态码最初的意图可能被用作某种形式的数字现金或在线支付方案的一部分，但几乎没有哪家服务商使用，而且这个状态码通常不被使用。如果特定开发人员已超过请求的每日限制，Google Developers API会使用此状态码。[34]

403 Forbidden
主条目：HTTP 403 服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。如果这不是一个HEAD请求，而且服务器希望能够讲清楚为何请求不能被执行，那么就应该在实体内描述拒绝的原因。当然服务器也可以返回一个404响应，假如它不希望让客户端获得任何信息。

404 Not Found
主条目：HTTP 404 请求失败，请求所希望得到的资源未被在服务器上发现，但允许用户的后续请求。[35]没有信息能够告诉用户这个状况到底是暂时的还是永久的。假如服务器知道情况的话，应当使用410状态码来告知旧资源因为某些内部的配置机制问题，已经永久的不可用，而且没有任何可以跳转的地址。404这个状态码被广泛应用于当服务器不想揭示到底为何请求被拒绝或者没有其他适合的响应可用的情况下。

405 Method Not Allowed
请求行中指定的请求方法不能被用于请求相应的资源。该响应必须返回一个Allow头信息用以表示出当前资源能够接受的请求方法的列表。例如，需要通过POST呈现数据的表单上的GET请求，或只读资源上的PUT请求。 鉴于PUT，DELETE方法会对服务器上的资源进行写操作，因而绝大部分的网页服务器都不支持或者在默认配置下不允许上述请求方法，对于此类请求均会返回405错误。

406 Not Acceptable
参见：内容协商 请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体，该请求不可接受。[36] 除非这是一个HEAD请求，否则该响应就应当返回一个包含可以让用户或者浏览器从中选择最合适的实体特性以及地址栏表的实体。实体的格式由Content-Type头中定义的媒体类型决定。浏览器可以根据格式及自身能力自行作出最佳选择。但是，规范中并没有定义任何作出此类自动选择的标准。

407 Proxy Authentication Required
与401响应类似，只不过客户端必须在代理服务器上进行身份验证。[37]代理服务器必须返回一个Proxy-Authenticate用以进行身份询问。客户端可以返回一个Proxy-Authorization信息头用以验证。

408 Request Timeout
请求超时。根据HTTP规范，客户端没有在服务器预备等待的时间内完成一个请求的发送，客户端可以随时再次提交这一请求而无需进行任何更改。[38]

409 Conflict
表示因为请求存在冲突无法处理该请求，例如多个同步更新之间的编辑冲突。

410 Gone
表示所请求的资源不再可用，将不再可用。当资源被有意地删除并且资源应被清除时，应该使用这个。在收到410状态码后，用户应停止再次请求资源。[39]但大多数服务端不会使用此状态码，而是直接使用404状态码。

411 Length Required
服务器拒绝在没有定义Content-Length头的情况下接受请求。在添加了表明请求消息体长度的有效Content-Length头之后，客户端可以再次提交该请求。[40]

412 Precondition Failed
服务器在验证在请求的头字段中给出先决条件时，没能满足其中的一个或多个。[41]这个状态码允许客户端在获取资源时在请求的元信息（请求头字段数据）中设置先决条件，以此避免该请求方法被应用到其希望的内容以外的资源上。

413 Request Entity Too Large
前称“Request Entity Too Large”，表示服务器拒绝处理当前请求，因为该请求提交的实体数据大小超过了服务器愿意或者能够处理的范围。[42]此种情况下，服务器可以关闭连接以免客户端继续发送此请求。 如果这个状况是临时的，服务器应当返回一个Retry-After的响应头，以告知客户端可以在多少时间以后重新尝试。

414 Request-URI Too Long
前称“Request-URI Too Long”，[43]表示请求的URI长度超过了服务器能够解释的长度，因此服务器拒绝对该请求提供服务。通常将太多数据的结果编码为GET请求的查询字符串，在这种情况下，应将其转换为POST请求。[44]这比较少见，通常的情况包括： 本应使用POST方法的表单提交变成了GET方法，导致查询字符串过长。 重定向URI“黑洞”，例如每次重定向把旧的URI作为新的URI的一部分，导致在若干次重定向后URI超长。 客户端正在尝试利用某些服务器中存在的安全漏洞攻击服务器。这类服务器使用固定长度的缓冲读取或操作请求的URI，当GET后的参数超过某个数值后，可能会产生缓冲区溢出，导致任意代码被执行[45]。没有此类漏洞的服务器，应当返回414状态码。

415 Unsupported Media Type
对于当前请求的方法和所请求的资源，请求中提交的互联网媒体类型并不是服务器中所支持的格式，因此请求被拒绝。例如，客户端将图像上传格式为svg，但服务器要求图像使用上传格式为jpg。

416 Requested Range Not Satisfiable
前称“Requested Range Not Satisfiable”。[46]客户端已经要求文件的一部分（Byte serving），但服务器不能提供该部分。例如，如果客户端要求文件的一部分超出文件尾端。[47]

417 Expectation Failed
在请求头Expect中指定的预期内容无法被服务器满足，或者这个服务器是一个代理服显的证据证明在当前路由的下一个节点上，Expect的内容无法被满足。[48]

418 I’m a teapot
本操作码是在1998年作为IETF的传统愚人节笑话, 在RFC 2324超文本咖啡壶控制协议’中定义的，并不需要在真实的HTTP服务器中定义。当一个控制茶壶的HTCPCP收到BREW或POST指令要求其煮咖啡时应当回传此错误。[49]这个HTTP状态码在某些网站（包括Google.com）与项目（如Node.js、ASP.NET和Go语言）中用作彩蛋。[50]

420 Enhance Your Caim
Twitter Search与Trends API在客户端被限速的情况下返回。

421 Misdirected Request
该请求针对的是无法产生响应的服务器（例如因为连接重用）。[51]

422 Unprocessable Entity
请求格式正确，但是由于含有语义错误，无法响应。[15]

423 Locked
当前资源被锁定。[15]

424 Failed Dependency
由于之前的某个请求发生的错误，导致当前请求失败，例如PROPPATCH。[15]

425 Unordered Collection
在WebDAV Advanced Collections Protocol中定义，但Web Distributed Authoring and Versioning (WebDAV) Ordered Collections Protocol中并不存在。

426 Upgrade Required
客户端应当切换到TLS/1.0，并在HTTP/1.1 Upgrade header中给出。[15]

428 Precondition Required
原服务器要求该请求满足一定条件。这是为了防止“‘未更新’问题，即客户端读取（GET）一个资源的状态，更改它，并将它写（PUT）回服务器，但这期间第三方已经在服务器上更改了该资源的状态，因此导致了冲突。”[52]

429 Too Many Requests
用户在给定的时间内发送了太多的请求。旨在用于网络限速。[52]

431 Request Header Fields Too Large
服务器不愿处理请求，因为一个或多个头字段过大。[52]

444 No Response
Nginx上HTTP服务器扩展。服务器不向客户端返回任何信息，并关闭连接（有助于阻止恶意软件）。

450 Blocked by Windows Parental Controls
这是一个由Windows家庭控制（Microsoft）HTTP阻止的450状态代码的示例，用于信息和测试。

451 Unavailable For Legal Reasons
主条目：HTTP 451 该访问因法律的要求而被拒绝，由IETF在2015核准后新增加。[53][54][55]

494 Request Header Too Large
在错误代码431提出之前Nginx上使用的扩展HTTP代码。

5xx服务器错误
表示服务器无法完成明显有效的请求。[56]这类状态码代表了服务器在处理请求的过程中有错误或者异常状态发生，也有可能是服务器意识到以当前的软硬件资源无法完成对请求的处理。除非这是一个HEAD请求，否则服务器应当包含一个解释当前错误状态以及这个状况是临时的还是永久的解释信息实体。浏览器应当向用户展示任何在当前响应中被包含的实体。这些状态码适用于任何响应方法。[57]

500 Internal Server Error
通用错误消息，服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。没有给出具体错误信息。[58]

501 Not Implemented
服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。[59]（例如，网络服务API的新功能）

502 Bad Gateway
作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。[60]

503 Service Unavailable
由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是暂时的，并且将在一段时间以后恢复。[61]如果能够预计延迟时间，那么响应中可以包含一个Retry-After头用以标明这个延迟时间。如果没有给出这个Retry-After信息，那么客户端应当以处理500响应的方式处理它。

504 Gateway Timeout
作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。[62] 注意：某些代理服务器在DNS查询超时时会返回400或者500错误。

505 HTTP Version Not Supported
服务器不支持，或者拒绝支持在请求中使用的HTTP版本。[63]这暗示着服务器不能或不愿使用与客户端相同的版本。响应中应当包含一个描述了为何版本不被支持以及服务器支持哪些协议的实体。

506 Variant Also Negotiates
由《透明内容协商协议》（RFC 2295）扩展，代表服务器存在内部配置错误，[64]被请求的协商变元资源被配置为在透明内容协商中使用自己，因此在一个协商处理中不是一个合适的重点。

507 Insufficient Storage
服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。[15]

508 Loop Detected
服务器在处理请求时陷入死循环。 （可代替 208状态码）

510 Not Extended
获取资源所需要的策略并没有被满足。[65]

511 Network Authentication Required
客户端需要进行身份验证才能获得网络访问权限，旨在限制用户群访问特定网络。（例如连接WiFi热点时的强制网络门户）[52]

非官方状态码
420 Enhance Your Calm
据说早期 Twitter API 会在短期内提交太多需求的时候回传这个 Status Code，不过在新版 API 改为使用 429 Too Many Requests。

498 Invalid Token
499 Token Required
这两个是以前一个叫做 ArcGIS for Server 的系统会回应的 Status Code。一般来说验证信息错误还是会回传 401 Unathorized。

520 Unknown Error
Cloudflare 会用的未知错误。

521 Web Server Is Down
指目标服务器挂了，一般在一些CDN上会出现（例如Cloudflare）







### HTTP 常见字段

*Host* 字段：客户端发送请求时，⽤来指定服务器的域名

*Connection* 字段：最常⽤于客户端要求服务器使⽤ TCP 持久连接，以便其他请求复⽤。HTTP/1.1 版本的默认连接都是持久连接，但为了兼容⽼版本的 HTTP，需要指定 Connection ⾸部字段的值为Keep-Alive 。⼀个可以复⽤的 TCP 连接就建⽴了，直到客户端或服务器主动关闭连接。

Accept 字段：客户端请求的时候，可以使⽤ Accept 字段声明⾃⼰可以接受哪些`数据格式`。`Accept: */*`，客户端声明⾃⼰可以接受任何格式的数据。

Accept-Encoding字段：说明⾃⼰可以接受哪些压缩⽅法。 `Accept-Encoding: gzip, deflate`

*Content-Length* 字段：服务器在返回数据时，会有 Content-Length 字段，表明本次回应的`数据长度。`

*Content-Type* 字段：⽤于服务器回应时，告诉客户端，本次`数据是什么格式`。

*Content-Encoding* 字段：说明数据的压缩⽅法。表示服务器返回的数据使⽤了什么压缩格式。`Content-Encoding: gzip`表示服务器返回的数据采⽤了 gzip ⽅式压缩，告知客户端需要⽤此⽅式解压。

### **HTTP长连接和短连接**

HTTP短连接：浏览器和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。**HTTP1.0默认使用的是短连接**。

HTTP长连接：指的是**复用TCP连接**。多个HTTP请求可以复用同一个TCP连接，这就节省了TCP连接建立和断开的消耗。

**HTTP/1.1起，默认使用长连接**。要使用长连接，客户端和服务器的HTTP首部的Connection都要设置为keep-alive，才能支持长连接。

### GET和POST

get获取数据，post提交数据

Get ⽅法的含义是请求**从服务器获取资源**，这个资源可以是静态的⽂本、⻚⾯、图⽚视频等。

⽽ POST ⽅法则是相反操作，它向 URI 指定的资源提交数据，数据就放在报⽂的 body ⾥。



数据**传输方式**不同：**GET请求通过URL传输数据，而POST的数据通过请求体传输**。

数据类型不同：**GET只允许 ASCII 字符，而POST无限制**（那Body里面肯定没限制啊）

安全幂等：**GET** **⽅法就是安全且幂等的**，因为它是「只读」操作，⽆论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。POST 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是**不安全**的

缓存：GET请求会被浏览器**主动缓存**，而POST不会，除非手动设置。



PUT和POST：PUT 和POST方法的区别是,**PUT方法是幂等的**：连续调用一次或者多次的效果相同（无副作用），而POST方法是非幂等的。

PUT和PATCH：PUT和PATCH都是更新资源，而PATCH用来对已知资源进行局部更新。

### HTTP/1.1、HTTP/2、HTTP/3 演变

HTTP/1.1 相⽐ HTTP/1.0 性能上的改进：

使⽤ TCP ⻓连接的⽅式改善了 HTTP/1.0 短连接造成的性能开销。

⽀持管道（pipeline）⽹络传输，只要第⼀个请求发出去了，不必等其回来，就可以发第⼆个请求出去，可以减少整体的响应时间。

但 HTTP/1.1 还是有性能瓶颈：

+ 请求 / 响应头部（Header）未经压缩就发送，⾸部信息越多延迟越⼤。只能压缩 Body 的部分；

+ 发送冗⻓的⾸部。每次互相发送相同的⾸部造成的浪费较多；

+ 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端⼀直请求不到数据，也就是`队头阻塞`；
+ 请求只能从客户端开始，服务器只能被动响应。

HTTP/2 相⽐ HTTP/1.1 性能上的改进：*1.* 头部压缩 *2.* ⼆进制格式 *3.* 数据流 *4.* 多路复⽤ HTTP/2 是可以在**⼀个连接中并发多个请求或回应，⽽不⽤按照顺序⼀⼀对应**。

### HTTP缓存

`Cache-control` 头：

1. 缓存中不得存储任何关于客户端请求和服务端响应的内容。每次由客户端发起的请求都会下载完整的响应内容。

```
Cache-Control: no-store
```

2. 缓存但重新验证：如下头部定义，此方式下，每次有请求发出时，缓存会将此请求发到服务器（译者注：该请求应该会带有与本地缓存相关的验证字段），服务器端会验证请求中所描述的缓存是否过期，若未过期（注：实际就是返回304），则缓存才使用本地缓存副本。

```
Cache-Control: no-cache
```



### URI和URL的区别

- URI，全称是Uniform Resource Identifier)，中文翻译是统一资源标志符，主要作用是唯一标识一个资源。
- URL，全称是Uniform Resource Location)，中文翻译是统一资源定位符，主要作用是提供资源的路径。打个经典比喻吧，URI像是身份证，可以唯一标识一个人，而URL更像一个住址，可以通过URL找到这个人。



## 跨域



## HTTPS

## 混合加密

HTTPS 采用的是**对称加密**和**非对称加密**结合的「混合加密」方式：

+ 在通信建立前采用**非对称加密**的方式交换「会话秘钥」，后续就不再使用非对称加密。

+ 在通信过程中全部使用**对称加密**的「会话秘钥」的方式加密明文数据。

采用「混合加密」的方式的原因：

+ **对称加密**只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。

+ **非对称加密**使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢

### 数字证书

服务端可以向证书颁发机构CA申请证书，以避免中间人攻击（防止证书被篡改）。证书包含三部分内容：**证书内容、证书签名算法和签名**，签名是为了验证身份。

服务端把证书传输给浏览器，浏览器从证书里取公钥。证书可以证明该公钥对应本网站。

**数字签名的制作过程**：（证书内容hash值 + 私钥 = 数字签名）

1. CA使用证书签名算法对证书内容进行**hash运算**。
2. 对hash后的值**用CA的私钥加密**，得到**数字签名**。

**浏览器验证过程**：

1. 获取证书，得到证书内容、证书签名算法和数字签名。
2. 用CA机构的公钥**对数字签名解密**（由于是浏览器信任的机构，所以浏览器会保存它的公钥）。
3. 用证书里的签名算法**对证书内容进行hash运算**。
4. 比较解密后的数字签名和对证书内容做hash运算后得到的哈希值，相等则表明证书可信

### TLS握手过程

*1. ClientHello* 

首先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求。

在这一步，客户端主要向服务器发送以下信息：

1）客户端支持的 SSL/TLS 协议版本，如 TLS 1.2 版本。

2）客户端生产的随机数（ Client Random ），后面用于生产「会话秘钥」。

3）客户端支持的密码套件列表，如 RSA 加密算法。

*2. SeverHello*

服务器收到客户端请求后，向客户端发出响应，也就是 SeverHello 。服务器回应的内容有如下内容：

1）确认 SSL/ TLS 协议版本，如果浏览器不支持，则关闭加密通信。

2）服务器生产的随机数（ Server Random ），后面用于生产「会话秘钥」。

3）确认的密码套件列表，如 RSA 加密算法。

4）服务器的数字证书。

*3.*客户端回应

**客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。**

如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息：

1）一个随机数（ pre-master key ）。该随机数会被服务器公钥加密。

2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。

3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。上面第一项的随机数是整个握手阶段的第三个随机数，这样服务器和客户端就同时有三个随机数，接着就`用双方协商的加密算法`，**各自生成**本次通信的「会话秘钥」。

*4.* 服务器的最后回应

服务器收到客户端的第三个随机数（ pre-master key ）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。然后，向客户端发生最后的信息：

1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。

2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。



1. 客户端和服务端打招呼，并且把自己支持的TLS版本，加密套件发给服务端，同时还生成了一个随机数给服务端（第1随机数）
2. 服务端打招呼，服务端确认支持的TLS版本以及选择的加密套件，并且服务器也生成一个随机数发给客户端（第2随机数）。接着，服务端还把证书和公钥发送给客户端。都发送完毕后，就告知客户端
3. 客户端生成随机数（第3随机数，预主密钥）。这个预主密钥是用刚刚收到的公钥进行加密后，再发送出去的。客户端这边的TLS协商表示没问题了，加密开始。
4. 服务端收到加密后的预主密钥后，会用自己的私钥进行解密。这样服务端就能知道预主密钥了。而且，只有客户端和服务端知道这个预主密钥（因为是服务端给客户端发送的公钥，客户端用这个公钥对随机数进行加密，服务端再把这个经过加密的随机数进行解密，得到这个随机数）
5. 最后，客户端用预主密钥，第1随机数和第2随机数计算出会话密钥。服务端也用预主密钥，第1随机数和第2随机数计算出会话密钥。各自得到的会话密钥是相同的。（预主密钥，是不是就是指，预备主密钥，但不是真正的密钥。）
6. 前面的步骤都是非对称加密。为了得到这个会话密钥，后面的会话大家都只使用这个会话密钥对数据进行加密。也就是说，后面使用的是对称加密。大家都使用同一个私钥。会话后面不使用非对称加密是因为消耗资源大，而且客户端和服务端得到会话密钥后，就没有其他人知道会话密钥是啥了。会话密钥只应用在当前会话，如果要建立别的会话，会话密钥也是不相同的。



## 三次握手

因为TCP是面向连接的协议，所以使用TCP之前必须先建立连接。建立连接是通过三次握手来进行的。

三次握手的主要目的是为了：确定客户端和服务端都可以正常的接收和发送TCP数据包。

1. 刚开始的时候，客户端和服务端都处于 `CLOSED` 状态。
2. 服务端主动监听某个端口，处于 `LISTEN`状态。
3. 第一次握手：客户端给服务端发送一个 `SYN 报文`。客户端会生成一个初始序列号，我们把它记为`x`，然后把这个初始序列号放在TCP报文首部中的序号字段。同时，把`SYN标志位`置为1。发送完这个报文之后，客户端处于 `SYN-SENT` 状态。
4. 第二次握手：服务端收到客户端的`SYN报文`后，会给客户端发送`SYN+ACK报文`。`SYN+ACK报文`的确认号等于SYN报文序号加1，也就是x+1。同时，服务端也会生成一个初始序列号，我们把它记为`y`，放到报文首部的序号字段。最后，把ACK和SYN标志位置为1。发送完这个报文之后，服务端处于`SYN_REVD`状态。
5. 第三次握手：客户端收到服务端发送回来的报文后，会再给服务端发送一个`ACK报文`。`ACK报文`的序号是x+1，确认号字段是y+1。ACK标志位为1。发送完这个报文之后，客户端处于`ESTABLISHED`状态。
6. 服务器收到 ACK 报文之后，也处于 `ESTABLISHED` 状态，此时，双方就建立起了连接，可以互相发送数据了。



1. 刚开始的时候，客户端和服务端都处于 `CLOSED` 状态。
2. 服务端主动监听某个端口，处于 `LISTEN`状态。
3. 客户端给服务端发送一个 `SYN 报文`。`SYN 报文`里的`SYN标志位`是1，代表它想建立连接。`SYN 报文`里的序号字段，会填入一个初始序列号。发送完这个报文之后，客户端处于 `SYN-SENT` 状态。
4. 服务端收到客户端的`SYN报文`后，会给客户端发送`SYN+ACK报文`。`SYN+ACK报文`里的`SYN和ACK`标志位是1，代表它已经收到了客户端发送过来的报文，并且它也需要建立连接。刚才我们提到，客户端发送过来的`SYN 报文`里面有一个序号，我们把这个序号称为`x`。现在，服务端报文里的确认号字段就是x+1，代表我已经收到你这个序号之前的数据了，我希望下一次收到序号为x+1的数据包。然后服务端这边也会生成一个序列号。发给客户端。
5. 客户端收到服务端发送回来的报文后，会再给服务端发送一个`ACK报文`。`ACK报文`的序号是x+1，确认号字段是y+1。ACK标志位为1。发送完这个报文之后，客户端处于`ESTABLISHED`状态。
6. 服务器收到 ACK 报文之后，也处于 `ESTABLISHED` 状态，此时，双方就建立起了连接，可以互相发送数据了。

## 四次挥手

四次挥手的话，客户端和服务端都可以主动断开连接。

（ FIN_WAIT_1-> CLOSED_WAIT ->FIN_WAIT_2    LAST_ACK-> TIME_WAIT->CLOSED   ）

我们以客户端主动断开连接为例子。

1. 客户端打算关闭连接，此时会发送一个 TCP 首部 FIN 标志位被置为 1 的报文，也即 FIN报文，之后客户端进入 `FIN_WAIT_1` 状态。
2. 服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 `CLOSED_WAIT` 状态。
3. 客户端收到服务端的 ACK 应答报文后，之后进入 `FIN_WAIT_2` 状态。
4. 等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 `LAST_ACK` 状态。
5. 客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 `TIME_WAIT` 状态
6. 服务器收到了 ACK 应答报文后，就进入了 `CLOSED` 状态，服务端完成连接的关闭。
7. 客户端在经过 2MSL 一段时间后，自动进入 `CLOSED` 状态，这个时候呢，客户端也关闭了连接。

## 为什么不能是两次握手

防止已经失效的连接请求报文段被服务端接收，从而产生一系列的问题。

如果建立连接只需要两次握手，服务端在收到连接请求后就进入ESTABLISHED状态，客户端并获得服务端的应答后进入ESTABLISHED状态。

现在假设出现一种场景，客户端发送的连接请求A，因为网络拥塞，还没到服务端，客户端就超时重发请求B，如果服务端正确接收B并确认应答，双方便开始通信，通信结束后释放连接。此时，如果那个失效的连接请求A抵达了服务端，由于只有两次握手，服务端收到请求就会进入ESTABLISHED状态，等待发送数据或主动发送数据。但如果此时的客户端已进入CLOSED状态，服务端将会一直等待下去，这样浪费服务端连接资源。

## 客户端和服务端的初始序列号 ISN 不相同

如果一个已经失效的连接被重用了，但是该旧连接的历史报文还残留在网络中，如果序列号相同，那么就无法分辨出该报文是不是历史报文，如果历史报文被新的连接接收了，则会产生数据错乱。

所以，每次建立连接前重新初始化一个序列号主要是为了通信双方能够根据序号将不属于本连接的报文段丢弃。

## 初始序列号 ISN 如何产生

起始 ISN 是基于计时器的，经过一定时间之后+1。后来提出了一个算法是，通过计时器和hash算法求得的，这个hash算法是通过计算源IP、目的IP、源端口、目的端口计算出一个hash值。

```
ISN = M + F (localhost, localport, remotehost, remoteport)
```

## SYN攻击

TCP 连接建立是需要三次握手，假设攻击者短时间伪造不同 IP 地址的 SYN 报文，服务端每接收到一个 SYN 报文，就进入 SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，久而久之就会**占满服务端的** **SYN** **接收队列（未连接队列）**，使得服务器不能为正常用户服务。

避免 *SYN* 攻击方式一

通过修改 Linux 内核参数，控制队列大小和当队列满时应做什么处理。比如说，超出这个服务器处理能力了，就对新的 SYN 直接回报RST，丢弃连接：

避免 *SYN* 攻击方式二

在 **`TCP`** 服务器接收到 **`SYN`** 包并返回 **`SYN+ACK`** 包时，不分配资源，而是根据这个 **`SYN`** 包计算出一个 **`cookie`** 值。这个 **`cookie`**作为将要返回的 **`SYN ACK`** 包的初始序列号，当客户端返回一个 **`ACK`** 包时，根据包头信息计算 **`cookie`**，与返回的确认序列号（初始序列号 + 1）进行对比，如果相同，则是一个正常连接，然后，分配资源，建立连接。

1. 

## 为什么挥手需要四次

在 TCP三次握手的时候，服务端发送 `SYN+ACK` 包的时候，其实是将一个 `ACK` 包和一个 `SYN`包合并到一个包中，所以减少了一次包的发送，三次完成握手。

对于四次挥手，在主动关闭方发送 FIN 包后，接收端可能还要发送数据，不能立即关闭服务器端到客户端的数据通道，所以也就不能将服务器端的 `FIN` 包与对客户端的 `ACK` 包合并发送，只能先确认 `ACK`，然后服务器待无需发送数据时再发送 `FIN` 包，所以四次挥手时必须是四次数据包的交互。

## TIME_WAIT 经过 2MSL 进入 CLOSE 状态

`MSL` (Maximum Segment Lifetime)指的是报文在网络中最大生存时间。在客户端发送对服务器端的 `FIN` 的确认包 `ACK` 后，这个 `ACK` 包是有可能不可达的，服务器端如果收不到 `ACK` 的话需要重新发送 `FIN` 包。

所以客户端发送 `ACK` 后需要留出 `2MSL` 时间，等待确认服务器端确实收到了 ACK 包。

`2MSL` 的时间是从**客户端接收到** **FIN** **后发送** **ACK** **开始计时的**。如果在 TIME-WAIT 时间内，**因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文**，那么 **2MSL** **时间将重新计时**。

## 为什么需要 TIME_WAIT 状态？

1. 保证连接正确关闭 。TIME-WAIT 作用是**等待足够的时间以确保最后的** **ACK** **能让被动关闭方接收，从而帮助其正常关闭。**
2. 避免新旧连接混淆。在客户端发送完最后一个 `ACK` 报文段后，在经过 `2MSL` 时间，就可以使本连接持续的时间内所产生的所有报文都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文。

## TIME_WAIT 过多有什么危害？

1. 服务器端TIME_WAIT状态过多。理论上服务端可以建立很多连接，服务端只监听一个端口， 但是会把连接扔给处理线程。线程池可能处理不了那么多一直不断的连接了。系统资源被占满，会导致处理不过来新的连接。
2. 客户端TIME_WAIT 状态过多。客户端TIME_WAIT过多，就会导致端口资源被占用，因为端口就65536个，被占满就会导致无法创建新的连接。

怎么解决？

1. **复用处于** **TIME_WAIT** **的** **socket** **为新的连接所用**
2. 当系统中处于 TIME_WAIT 的连接**一旦超过某个值时，系统就会将所有的TIME_WAIT** **连接状态重置。**



## TCP和UDP的区别

1. 连接。TCP 是一个面向连接的协议，要求在发送方和接收方开始通信之前要进行三次握手建立连接，UDP 是一种无连接协议。
2. 可靠性。TCP是可靠的，UDP是不可靠的。（怎么体现可靠） 传输的数据无差错、不丢失、不重复、按序到达。
3. 传输方法。TCP是面向字节流的，UDP是面向报文的。
4. 首部长度。TCP首部有20个字节，UDP首部只有8个字节。
5. 应用协议。TCP应用于HTTP、HTTPS等协议，UDP用于DNS协议、视频电话等常见。

## UDP

UDP和TCP最大的区别在于它是无连接的。UDP其实只在IP的数据包服务之上增加了两个最基本的服务：复用/分用、差错检测

UDP优点：

1. 发送数据之前不需要建立连接
2. UDP的主机不需要维持复杂的连接状态表（TCP需要在端系统中维护连接状态。此连接状态包括接收和发送缓存、拥塞控制参数以及序号和确认号的参数）
3. UDP用户数据包只有8个字节的首部开销
4. 网络出现的拥塞不会使源主机的发送速率降低（没有拥塞控制）。这对于某些实时应用（视频会议）是很重要的
5. UDP支持一对一、一对多、多对一和多对多的交互通信

6. UDP是面向报文的，对应用层交下来的报文，不合并，不拆分，保留原报文的边界；

UDP如何实现可靠传输：**确认机制**  **超时重传**  **滑动窗口**

## TCP可靠性

1. 确认和重传：接收方收到报文就会确认，发送方发送一段时间后没有收到确认就重传。
2. 数据校验
3. 滑动窗口：发送方一边连续地发送数据包，一边等待接收方的确认
4. 流量控制：接收方控制的、调节发送方发送速度的机制
5. 拥塞控制：当网络拥塞时，减少发送端数据的发送



## 确认和重传

首先，由于丢包的可能性，要实现可靠通信：

1. 发送方要知道对方接收成功，因此需要接收方回复确认 即 ACK。
2. 如果丢包发生，发送方需要重传。

如何发送确认包和重传包？

1. 停止并等待。发送方等到接收方的确认包后，再发下一个数据包 。如果时限内收到对方确认，才发送下一个数据包。否则，重传当前数据包。
2. 流水线传输（连续ARQ协议）。 **发送方会连续发送一组数据包，同时等待这些数据包的确认 。** 发送方一边发送，一边等回复 。

### 重传机制

重传的触发方式有两种：超时重传和 快速重传。（一个是时间驱动，一个是数据驱动）

具体实施方式又有两种， 回退 N 重传和 选择重传。

1. 超时重传：在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据。TCP 会在以下两种情况发生超时重传：（1）数据包丢失 （2）确认应答丢失 

超时时间设置：

RTT  （Round-Trip Time 往返时延），就是**数据从网络一端传送到另一端所需的时间**，也就是包的往返时间。

RTO：超时重传时间是以 RTO （Retransmission Timeout 超时重传时间）表示。

当超时时间 **RTO** **较大**时，重发就慢，丢了老半天才重发，没有效率，性能差；

当超时时间 **RTO** **较小**时，会导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。

**超时重传时间** **RTO** **的值应该略大于报文往返** **RTT** **的值**。RTO是动态变化的。

2. 快速重传。如果收到同一个数据包的多次确认，立即发起重传 。一般取 `3` 次作为阈值，常叫做 3ACK 方法。快速重传机制的作用在于，将有可能在超时触发之前，提前发起重传。



重传具体实施方式

1. 回退N重传。发送方每发送一个数据包，都会发起一个定时器。一旦一个某个定时器触发，就会重传。发送指针回退到未拿到确认的数据包处，以实现重传。此方法下， 会重传这个指针后面所有的数据包。
2. 选择重传。SACK 。同样，每发送一个数据包，都会发起一个定时器。不同点在于， 只重传未拿到确认的数据包，不回退发送指针。

>  拓展   Duplicate SACK 又称 D-SACK     服务端会告诉「发送方」有哪些数据被重复接收了

## 滑动窗口机制

滑动窗口机制就是发送方一边连续地发送数据包，一边等待接收方的确认。

滑动窗口分为两种：发送窗口和接收窗口。由于 TCP 是全双工的， 所以通信的每一端都会同时维护两种窗口 。

窗口大小就是指**无需等待确认应答，而可以继续发送数据的最大值**。窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须**在缓冲区中保留已发送的数据**。如果按期收到确认应答，此时数据就可以从缓存区清除。

窗口大小由哪一方决定？

TCP 头里有一个字段叫 Window ，也就是窗口大小。

**这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。**所以，通常窗口的大小是由接收方的窗口大小来决定的。发送方发送的数据大小不能超过接收方的窗口大小，否则接收方就无法正常接收到数据。

## 流量控制机制

TCP三次握手，发送端和接收端进入到ESTABLISHED状态，就可以传输数据。但是发送端不能疯狂地向接收端发送数据，因为接收端接收不过来的话，接收方只能把处理不过来的数据存在缓存区里。如果缓存区都满了，发送方还在发送数据的话，接收方只能把收到的数据包丢掉，这样就浪费了网络资源。

**TCP** 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。其具体的实现方式，就是在回复时设置TCP报文首段中的窗口大小字段。

## 拥塞控制机制

**TCP协议拥塞控制的方法是 发送方主动减少发送量，避免「发送方」的数据填满整个网络。**拥塞控制是发送方主动减少数据传输，解决的宏观网络的超负荷问题的机制。

拥塞窗口：为了在「发送方」调节所要发送数据的量，定义了一个叫做「**拥塞窗口**」的概念。**拥塞窗口** **cwnd**是发送方维护的一个的状态变量，它会根据**网络的拥塞程度动态变化的**。 我们在前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。

拥塞窗口 cwnd 变化的规则：

- 只要网络中没有出现拥塞， cwnd 就会增大；
- 但网络中出现了拥塞， cwnd 就减少；

怎么知道当前网络是否出现了拥塞呢？（丢包？）

其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是**发生了超时重传，就会认为网络出现了拥塞。**

拥塞控制算法：

- 慢启动
- 拥塞避免
- 拥塞发生（有两种算法，超时重传、快速重传）
- 快恢复

**慢启动**：**发送方每收到一个ACK，拥塞窗口cwnd 的大小就会加1**。简而言之，如果没有遭遇超时 或 3ACK， 拥塞窗口就扩大一倍。（指数增长）

**拥塞避免**：有一个叫慢启动门限 ssthresh （slow start threshold）状态变量。拥塞窗口倍增到门限值后，就改为线性增长，比如每次增大一 。

**拥塞发生**：当网络出现拥塞，也就是发生数据包重传，重传机制主要有两种：（1）超时重传 （2）快速重传

（1）超时重传：  当发生了「超时重传」，则就会使用拥塞发生算法。这个时候，ssthresh 和 cwnd 的值会发生变化：

- 慢启动门限 ssthresh 设为 cwnd/2 ，
- 拥塞窗口大小 cwnd 重置为 1

（2）快速重传：当接收方收到三个重复的ACK，于是发送端就会进行快速重传，不必等待超时再重传。 ssthresh 和 cwnd 变化如下：

- cwnd = cwnd/2 ，也就是设置为原来的一半; 
- ssthresh = cwnd ;
- （其实就是，慢启动门限减半，拥塞窗口也减半）

进入**快速恢复**算法（直接跳过慢启动，直接进入拥塞避免的方式，就叫做快恢复）

快速恢复：直接跳过慢启动，直接进入拥塞避免的方式，就叫做快恢复



## TCP粘包

发送方产生粘包：当发送的数据包过小时，那么 TCP 协议默认启用 Nagle 算法，将这些较小的数据包进行合并发送（缓冲区数据发送是一个堆压的过程）；这个合并过程就是在发送缓冲区中进行的，也就是说数据发送出来它已经是粘包的状态了。

接收方产生粘包：数据放到接收缓冲区，没有及时取出来

如何解决粘包？

粘包的问题出现是因为不知道一个用户消息的边界在哪，如果知道了边界在哪，接收方就可以通过边界来划分出有效的用户消息。

- 固定长度的消息；比如规定一个消息的长度是 64 个字节
- 特殊字符作为边界；HTTP 是一个非常好的例子。
- 自定义消息结构。自定义一个消息结构，由包头和数据组成，包头里有一个字段来说明紧随其后的数据有多大。

如果使用 netty 的话，就有专门的编码器和解码器解决拆包和粘包问题了。

## Nagle算法

该算法的思路是**延时处理**，它满足以下两个条件中的一条才可以发送数据：

- 要等到窗口大小 >= MSS 或是 数据大小 >= MSS
- 收到之前发送数据的 ack 回包

只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件。



## 保活机制/心跳包

定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。如果开启了 TCP 保活，需要考虑以下几种情况：

- 第一种，对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。
- 第二种，是对端程序崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。
- 第三种，对端程序崩溃并重启。当 TCP 保活的探测报文发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，**会产生一个 RST 报文**，这样很快就会发现 TCP 连接已经被重置。

TCP 保活的这个机制检测的时间是有点长，我们可以自己在应用层实现一个心跳机制。比如，web 服务软件一般都会提供 `keepalive_timeout` 参数，用来指定 HTTP 长连接的超时时间。

## SOCKET编程

服务器端先初始化Socket，然后与端口绑定(bind)，对端口进行监听(listen)，调用accept阻塞，等待客户端连接。在这时如果有个客户端初始化一个Socket，然后连接服务器(connect)，如果连接成功，这时客户端与服务器端的连接就建立了。客户端发送数据请求，服务器端接收请求并处理请求，然后把回应数据发送给客户端，客户端读取数据，最后关闭连接，一次交互结束。



## ICMP应用

Ping：主要用来测试两台主机之间的连通性。Ping 的原理是通过向目的主机发送 ICMP Echo 请求报文，目的主机收到之后会发送 Echo 回答报文。Ping 会根据时间和成功响应的次数估算出数据包往返时间以及丢包率。

Traceroute：用来跟踪一个分组从源点到终点的路径

## IP地址和MAC地址

IP地址 为互联网上的每一个网络和每一台主机配置一个唯一的逻辑地址，用来与物理地址作区分。

MAC地址（Media Access Control Address）的全称叫做媒体访问控制地址，也称作局域网地址，以太网地址或者物理地址。MAC地址用于在网络中唯一标示一个网卡，一台设备若有一或多个网卡，则每个网卡都需要并会有一个唯一的MAC地址。

## IP地址有哪些分类？

一般可以这么认为，IP地址=网络号+主机号。

1. 网络号：它标志主机所连接的网络地址表示属于互联网的哪一个网络。
2. 主机号：它标志主机地址表示其属于该网络中的哪一台主机。

IP地址分为A，B，C，D，E五大类：

- A类地址(1~126)：以0开头，网络号占前8位，主机号占后面24位。
- B类地址(128~191)：以10开头，网络号占前16位，主机号占后面16位。
- C类地址(192~223)：以110开头，网络号占前24位，主机号占后面8位。
- D类地址(224~239)：以1110开头，保留位多播地址。
- E类地址(240~255)：以11110开头，保留位为将来使用

## NAT

Network Address Translation, 网络地址转换

用于解决内网中的主机要和因特网上的主机通信。由NAT路由器将主机的本地IP地址转换为全球IP地址，分为静态转换（转换得到的全球IP地址固定不变）和动态NAT转换。

## RIP

Routing Information Protocol, 距离矢量路由协议

每个路由器维护一张表，记录该路由器到其它网络的”跳数“，路由器到与其直接连接的网络的跳数是1，每多经过一个路由器跳数就加1；更新该表时和相邻路由器交换路由信息；路由器允许一个路径最多包含15个路由器，如果跳数为16，则不可达。交付数据报时优先选取距离最短的路径。

## DHCP

为了动态获取并使用一个合法的IP地址，需要经历以下几个阶段：

**(1) 发现阶段：即DHCP客户端寻找DHCP服务器的阶段。**

**(2) 提供阶段：即DHCP服务器提供IP地址的阶段。**

**(3) 选择阶段：即DHCP客户端选择某台DHCP服务器提供的IP地址的阶段。**

**(4) 确认阶段：即DHCP服务器确认所提供的IP地址的阶段**

1. 发现阶段

在发现阶段，DHCP客户端通过发送DHCP-DISCOVER报文来寻找DHCP服务器。

由于DHCP服务器的IP地址对于客户端来说是未知的，所以DHCP客户端以广播方式发送DHCP-DISCOVER报文。所有收到DHCP-DISCOVER报文的DHCP服务器都会发送回应报文，DHCP客户端据此可以知道网络中存在的DHCP服务器的位置。

2. 提供阶段

网络中接收到DHCP-DISCOVER报文的DHCP服务器，会选择一个合适的IP地址，连同IP地址租约期限和其他配置信息（如网关地址，域名服务器地址等）一同通过DHCP-OFFER报文发送给DHCP客户端。

DHCP服务器通过地址池保存可供分配的IP地址和其他配置信息。当DHCP服务器接收到DHCP请求报文后，将从IP地址池中取得空闲的IP地址及其他的参数，发送给DHCP客户端。

3. 选择阶段

如果有多台DHCP服务器向DHCP客户端回应DHCP-OFFER报文，则DHCP客户端只接受第一个收到的DHCP-OFFER报文。然后以广播方式发送DHCP-REQUEST请求报文，该报文中包含Option 54（服务器标识选项），即它选择的DHCP服务器的IP地址信息。

以广播方式发送DHCP-REQUEST请求报文，是为了通知所有的DHCP服务器，它将选择Option 54中标识的DHCP服务器提供的IP地址，其他DHCP服务器可以重新使用曾提供的IP地址。

4. 确认阶段

收到DHCP客户端发送的DHCP-REQUEST请求报文后，DHCP服务器根据DHCP-REQUEST报文中携带的MAC地址来查找有没有相应的租约记录。如果有，则发送DHCP-ACK报文作为应答，通知DHCP客户端可以使用分配的IP地址。

DHCP客户端收到DHCP服务器返回的DHCP-ACK确认报文后，会以广播的方式发送免费ARP报文，探测是否有主机使用服务器分配的IP地址，如果在规定的时间内没有收到回应，客户端才使用此地址。否则，客户端会发送DHCP-DECLINE报文给DHCP服务器，通知DHCP服务器该地址不可用，并重新申请IP地址。

如果DHCP服务器收到DHCP-REQUEST报文后，没有找到相应的租约记录，或者由于某些原因无法正常分配IP地址，则发送DHCP-NAK报文作为应答，通知DHCP客户端无法分配合适IP地址。DHCP客户端需要重新发送DHCP-DISCOVER报文来请求新的IP地址。

DHCP服务器提供三种IP分配方式：自动分配（永久使用这个地址） 动态分配（要释放） 手动分配


## Linux网络包

1. 数据包进入物理网卡，如果目的地址不是该网络设备，且该网络设备没有开启[混杂模式](https://unix.stackexchange.com/questions/14056/what-is-kernel-ip-forwarding)，该包会被该网络设备丢弃；
2. 物理网卡将数据包通过 DMA 的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化；
3. 物理网卡通过硬件中断（IRQ）通知 CPU，有新的数据包到达物理网卡需要处理；
4. 接下来 CPU 根据中断表，调用已经注册了的中断函数，这个中断函数会调到驱动程序（NIC Driver）中相应的函数；
5. 驱动先禁用网卡的中断，表示驱动程序已经知道内存中有数据了，告诉物理网卡下次再收到数据包直接写内存就可以了，不要再通知 CPU 了，这样可以提高效率，避免 CPU 不停地被中断；
6. 启动软中断继续处理数据包。这样做的原因是硬中断处理程序执行的过程中不能被中断，所以如果它执行时间过长，会导致 CPU 没法响应其它硬件的中断，于是内核引入软中断，这样可以将硬中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理；



内核和⽹络设备驱动是通过中断的⽅式来处理的。当设备上有数据到达的时候，会给 CPU 的
相关引脚上触发⼀个电压变化，以通知 CPU 来处理数据。对于⽹络模块来说，由于处理过程
⽐较复杂和耗时，如果在中断函数中完成所有的处理，将会导致中断处理函数（优先级过
⾼）将过度占据 CPU ，将导致 CPU ⽆法响应其它设备，例如⿏标和键盘的消息。因此Linux
中断处理函数是分上半部和下半部的。上半部是只进⾏最简单的⼯作，快速处理然后释放
CPU ，接着 CPU 就可以允许其它中断进来。剩下将绝⼤部分的⼯作都放到下半部中，可以
慢慢从容处理。2.4 以后的内核版本采⽤的下半部实现⽅式是软中断，由 ksoftirqd 内核线程
全权处理。和硬中断不同的是，硬中断是通过给 CPU 物理引脚施加电压变化，⽽软中断是通
过给内存中的⼀个变量的⼆进制值以通知软中断处理程序。



![image-20220417105302938](D:\mdImage\image-20220417105302938.png)

当⽹卡上收到数据以后，Linux 中第⼀个⼯作的模块是⽹络驱动。 ⽹络驱动会以 DMA 的⽅式 把⽹卡上收到的帧写到内存⾥。再向 CPU 发起⼀个中断，以通知 CPU 有数据到达。第⼆， 当 CPU 收到中断请求后，会去调⽤⽹络驱动注册的中断处理函数。 ⽹卡的中断处理函数并不 做过多⼯作，发出软中断请求，然后尽快释放 CPU。ksoftirqd 检测到有软中断请求到达，调 ⽤ poll 开始轮询收包，收到后交由各级协议栈处理。对于 udp 包来说，会被放到⽤户socket 的接收队列中。

（硬中断、软中断）

# Java基础

三大特性：封装 继承 多态



---

继承实现了  **IS-A**  关系，例如 Cat 和 Animal 就是一种 IS-A 关系，因此 Cat 可以继承自 Animal，从而获得 Animal 非 private 的属性和方法。

继承应该遵循里氏替换原则，子类对象必须能够替换掉所有父类对象。

Cat 可以当做 Animal 来使用，也就是说可以使用 Animal 引用 Cat 对象。父类引用指向子类对象称为  **向上转型** 。

---

多态分为编译时多态和运行时多态:

- 编译时多态主要指方法的重载
- 运行时多态指程序中定义的对象引用所指向的具体类型在运行期间才确定

运行时多态有三个条件:

- 继承
- 覆盖(重写)
- 向上转型

---

## 类图

### 泛化关系 (Generalization)

用来描述继承关系，在 Java 中使用 extends 关键字。

### 实现关系 (Realization)

用来实现一个接口，在 Java 中使用 implement 关键字。

……



## Java语言有哪些特点

面向对象

平台无关性（一次编译，到处运行）

## 标识符的命名规则

标识符的含义： 是指在程序中，我们自己定义的内容，譬如，类的名字，方法名称以及变量名称等 等，都是标识符。 

命名规则：

（硬性要求） 标识符可以包含英文字母，0-9的数字，$以及_ 

标识符不能以数字开头 

标识符不是关键字 

命名规范：

（非硬性要求） 

类名规范：首字符大写，后面每个单词首字母大写（大驼峰式）。 

变量名规范：首字母小写，后面每个单词首字母大写（小驼峰式）。 

方法名规范：同变量名。

## 八种基本数据类型的大小，以及他们的封装类

![image-20220412144518449](D:\mdImage\image-20220412144518449.png)

注： 1.int是基本数据类型，Integer是int的封装类，是引用类型。int默认值是0，而Integer默认值 是null，所以Integer能区分出0和null的情况。一旦java看到null，就知道这个引用还没有指向某个 对象，再任何引用使用前，必须为其指定一个对象，否则会报错。

 2.基本数据类型在声明时系统会自动给它分配空间，而引用类型声明时只是分配了引用空间， 必须通过实例化开辟数据空间之后才可以赋值。数组对象也是一个引用对象，将一个数组赋值给另 一个数组时只是复制了一个引用，所以通过某一个数组所做的修改在另一个数组中也看的见

虽然定义了boolean这种数据类型，但是只对它提供了非常有限的支持。在Java虚拟机中没有 任何供boolean值专用的字节码指令，Java语言表达式所操作的boolean值，在编译之后都使用Java 虚拟机中的int数据类型来代替，而boolean数组将会被编码成Java虚拟机的byte数组，每个元素 boolean元素占8位（一个字节）。这样我们可以得出boolean类型占了单独使用是4个字节，在数组中又是1个字 节。使用int的原因是，对于当下32位的处理器（CPU）来说，一次处理数据是32位（这里不是指的 是32/64位系统，而是指CPU硬件层面），具有高效存取的特点。

---

float 与 double

`1.1` 字面量属于 double 类型，不能直接将 `1.1` 直接赋值给 float 变量，因为这是向下转型。Java 不能隐式执行向下转型，因为这会使得精度降低。

```java
// float f = 1.1;
```

`1.1f` 字面量才是 float 类型。

```java
float f = 1.1f;
```

隐式类型转换：

因为字面量 1 是 int 类型，它比 short 类型精度要高，因此不能隐式地将 int 类型下转型为 short 类型。

```java
short s1 = 1;
// s1 = s1 + 1;

```

但是使用 += 运算符可以执行隐式类型转换。

```
s1 += 1;

```

上面的语句相当于将 s1 + 1 的计算结果进行了向下转型:

```
s1 = (short) (s1 + 1);
```

## switch

从 Java 7 开始，可以在 switch 条件判断语句中使用 String 对象。

```java
String s = "a";
switch (s) {
    case "a":
        System.out.println("aaa");
        break;
    case "b":
        System.out.println("bbb");
        break;
}

```

switch 不支持 long，是因为 switch 的设计初衷是对那些只有少数的几个值进行等值判断，如果值过于复杂，那么还是用 if 比较合适。

从 Java 7 开始，我们可以在 switch case 中使用字符串，但这仅仅是一个语法糖。内部实现在 switch 中使用字符串的 hash code。

## 抽象类

抽象类：

抽象类和抽象方法都使用 abstract 关键字进行声明。抽象类一般会包含抽象方法，抽象方法一定位于抽象类中。

抽象类和普通类最大的区别是，抽象类不能被实例化，需要继承抽象类才能实例化其子类。

---

1、抽象类不能被实例化。 2、抽象类应该至少有一个抽象方法，否则它没有任何意义。 3、抽象类中的抽象方法没有方法体。 4、抽象类的子类必须给出父类中的抽象方法的具体实现，除非该子类也是抽象类。

## 接口

---

http://www.itwanger.com/java/2019/11/06/java-abstract-interface.html

我们知道，有抽象方法的类被称为抽象类，**也就意味着抽象类中还能有不是抽象方法的方法**。这样的类就不能算作纯粹的接口，尽管它也可以提供接口的功能——只能说**抽象类是普通类与接口之间的一种中庸之道**。

**接口（英文：Interface），在 Java 中是一个抽象类型，是抽象方法的集合**；接口通过关键字 `interface` 来定义。接口与抽象类的不同之处在于：

1、抽象类可以有方法体的方法，但接口没有。 2、接口中的成员变量隐式为 `static final`，但抽象类不是的。 3、一个类可以实现多个接口，但只能继承一个抽象类。

以下示例展示了一个简单的接口：

```java
// 隐式的abstract
interface Coach {
	// 隐式的public
	void defend();
	void attack();
}
```

接口是隐式抽象的，所以声明时没有必要使用 `abstract` 关键字；接口的每个方法都是隐式抽象的，所以同样不需要使用 `abstract` 关键字；接口中的方法都是隐式 `public` 的。

和抽象类一样，接口也不能直接被实例化，它需要一个类来实现它。

实现多个接口：

---

接口：



接口是抽象类的延伸，在 Java 8 之前，它可以看成是一个完全抽象的类，也就是说它不能有任何的方法实现。

从 Java 8 开始，接口也可以拥有默认的方法实现，这是因为不支持默认方法的接口的维护成本太高了。在 Java 8 之前，如果一个接口想要添加新的方法，那么要修改所有实现了该接口的类。

接口的成员(字段 + 方法)默认都是 public 的，并且不允许定义为 private 或者 protected。

接口的字段**默认**都是 static 和 final 的。

```java
public interface InterfaceExample {
    void func1();

    default void func2(){
        System.out.println("func2");
    }

    int x = 123;
    // int y;               // Variable 'y' might not have been initialized
    public int z = 0;       // Modifier 'public' is redundant for interface fields
    // private int k = 0;   // Modifier 'private' not allowed here
    // protected int l = 0; // Modifier 'protected' not allowed here
    // private void fun3(); // Modifier 'private' not allowed here
}

```

```java
public class InterfaceImplementExample implements InterfaceExample {
    @Override
    public void func1() {
        System.out.println("func1");
    }
}

```

```java
// InterfaceExample ie1 = new InterfaceExample(); // 'InterfaceExample' is abstract; cannot be instantiated
InterfaceExample ie2 = new InterfaceImplementExample();
ie2.func1();
System.out.println(InterfaceExample.x);

```



**抽象类和接口的比较**

- 从设计层面上看，抽象类提供了一种 IS-A 关系，那么就必须满足里式替换原则，即子类对象必须能够替换掉所有父类对象。而接口更像是一种 LIKE-A 关系，它只是提供一种方法实现契约，并不要求接口和实现接口的类具有 IS-A 关系。
- 从使用上来看，一个类可以实现多个接口，但是不能继承多个抽象类。
- 接口的字段只能是 static 和 final 类型的，而抽象类的字段没有这种限制。
- 接口的成员只能是 public 的，而抽象类的成员可以有多种访问权限。

## super

访问父类的构造函数: 可以使用 super() 函数访问父类的构造函数，从而委托父类完成一些初始化的工作。

访问父类的成员: 如果子类重写了父类的中某个方法的实现，可以通过使用 super 关键字来引用父类的方法实现。

```java
public class SuperExample {
    protected int x;
    protected int y;

    public SuperExample(int x, int y) {
        this.x = x;
        this.y = y;
    }

    public void func() {
        System.out.println("SuperExample.func()");
    }
}

```

```java
public class SuperExtendExample extends SuperExample {
    private int z;

    public SuperExtendExample(int x, int y, int z) {
        super(x, y);
        this.z = z;
    }

    @Override
    public void func() {
        super.func();
        System.out.println("SuperExtendExample.func()");
    }
}

```

```java
SuperExample e = new SuperExtendExample(1, 2, 3);
e.func();

```

```java
SuperExample.func()
SuperExtendExample.func()

```



## instanceof 关键字的作用

instanceof 严格来说是Java中的一个双目运算符，用来测试一个对象是否为一个类的实例，用法为：

```java
boolean result = obj instanceof Class
```

其中 obj 为一个对象，Class 表示一个类或者一个接口，当 obj 为 Class 的对象，或者是其直接 或间接子类，或者是其接口的实现类，结果result 都返回 true，否则返回false。 注意：编译器会检查 obj 是否能转换成右边的class类型，如果不能转换则直接报错，如果不能确定类型，则通过编译，具体看运行时定。

```java
int i = 0;
System.out.println(i instanceof Integer);//编译不通过 i必须是引用类型，不能是基本类型
System.out.println(i instanceof Object);//编译不通过
```

```java
Integer integer = new Integer(1);
System.out.println(integer instanceof Integer);//true
```

```java
//false ,在 JavaSE规范 中对 instanceof 运算符的规定就是：如果 obj 为 null，那么将返回 false。
System.out.println(null instanceof Object);
```

## Java自动装箱与拆箱

装箱就是自动将基本数据类型转换为包装器类型（int-->Integer）；调用方法：Integer的 valueOf(int) 方法 

拆箱就是自动将包装器类型转换为基本数据类型（Integer-->int）。调用方法：Integer的 intValue方法

在Java SE5之前，如果要生成一个数值为10的Integer对象，必须这样进行：

```java
Integer i = new Integer(10)
```

而在从Java SE5开始就提供了自动装箱的特性，如果要生成一个数值为10的Integer对象，只需要 这样就可以了：

```java
Integer i = 10;
```

面试题1： 以下代码会输出什么？

```java
public class Main {
    public static void main(String[] args) {

        Integer i1 = 100;
        Integer i2 = 100;
        Integer i3 = 200;
        Integer i4 = 200;

        System.out.println(i1==i2);
        System.out.println(i3==i4);
    }
}

```

```
true
false
```

下面这段代码是Integer的valueOf方法的具体实现：

```java
public static Integer valueOf(int i) {
    if(i >= -128 && i <= IntegerCache.high)
        return IntegerCache.cache[i + 128];
    else
        return new Integer(i);
}

```

IntegerCache类的实现为：

```java
private static class IntegerCache {
    static final int high;
    static final Integer cache[];
    static {
        final int low = -128;
        // high value may be configured by property
        int h = 127;
        if (integerCacheHighPropValue != null) {
            // Use Long.decode here to avoid invoking methods that
            // require Integer's autoboxing cache to be initialized
            int i = Long.decode(integerCacheHighPropValue).intValue();
            i = Math.max(i, 127);
            // Maximum array size is Integer.MAX_VALUE
            h = Math.min(i, Integer.MAX_VALUE - -low);
        }
        high = h;
        cache = new Integer[(high - low) + 1];
        int j = low;
        for(int k = 0; k < cache.length; k++)
            cache[k] = new Integer(j++);
    }
    private IntegerCache() {}
}

```

从这2段代码可以看出，在通过valueOf方法创建Integer对象的时候，如果数值在[-128,127]之间， 便返回指向IntegerCache.cache中已经存在的对象的引用；否则创建一个新的Integer对象。 上面的代码中i1和i2的数值为100，因此会直接从cache中取已经存在的对象，所以i1和i2指向的是 同一个对象，而i3和i4则是分别指向不同的对象。

面试题2：以下代码输出什么

```java
public class Main {
    public static void main(String[] args) {

        Double i1 = 100.0;
        Double i2 = 100.0;
        Double i3 = 200.0;
        Double i4 = 200.0;

        System.out.println(i1==i2);
        System.out.println(i3==i4);
    }
}

```

```
false
false

```

原因： 在某个范围内的整型数值的个数是有限的，而浮点数却不是。



## 重载和重写的区别

重写(Override) 从字面上看，重写就是 重新写一遍的意思。其实就是在子类中把父类本身有的方法重新写一遍。子 类继承了父类原有的方法，但有时子类并不想原封不动的继承父类中的某个方法，所以在方法名， 参数列表，返回类型(除过子类中方法的返回值是父类中方法返回值的子类时)都相同的情况下， 对 方法体进行修改或重写，这就是重写。但要注意子类函数的访问修饰权限不能少于父类的。

```java
public class Father {
    public static void main(String[] args) {
        // TODO Auto-generated method stub
        Son s = new Son();
        s.sayHello();
    }
    public void sayHello() {
        System.out.println("Hello");
    }
}
class Son extends Father{
    @Override
    public void sayHello() {
        // TODO Auto-generated method stub
        System.out.println("hello by ");
    }
}

```

重写 总结：

1.发生在父类与子类之间 

2.方法名，参数列表，返回类型（除过子类中方法的返回类型 是父类中返回类型的子类）必须相同 

3.访问修饰符的限制一定要大于被重写方法的访问修饰符 （public>protected>default>private) 

4.重写方法一定不能抛出新的检查异常或者比被重写方法申 明更加宽泛的检查型异常



重载（Overload） 

在一个类中，同名的方法如果有不同的**参数列表**（参数类型不同、参数个数不同甚至是参数顺序不 同）则视为重载。**同时，重载对返回类型没有要求，可以相同也可以不同**，但不能通过返回类型是 否相同来判断重载。

```java
public class Father {
    public static void main(String[] args) {
        // TODO Auto-generated method stub
        Father s = new Father();
        s.sayHello();
        s.sayHello("wintershii");
    }
    public void sayHello() {
        System.out.println("Hello");
    }
    public void sayHello(String name) {
        System.out.println("Hello" + " " + name);
    }
}

```

重载 总结： 1.重载Overload是一个类中多态性的一种表现 2.重载要求同名方法的参数列表不同(参 数类型，参数个数甚至是参数顺序) 3.重载的时候，返回值类型可以相同也可以不相同。无法以返回 型别作为重载函数的区分标准

## JVM JRE JDK



## Hashcode的作用

（联系HashMap，计算出一个hash值，如果这个位置上没有元素，直接插入，如果hash值都不相等的话，那就不需要再进行后续比较，加速比较的速度）

java的集合有两类，一类是List，还有一类是Set。前者有序可重复，后者无序不重复。当我们在set 中插入的时候怎么判断是否已经存在该元素呢，可以通过equals方法。但是如果元素太多，用这样 的方法就会比较![image-20220412195024389](D:\mdImage\image-20220412195024389.png)满。 于是有人发明了哈希算法来提高集合中查找元素的效率。 这种方式将集合分成若干个存储区域，每 个对象可以计算出一个哈希码，可以将哈希码分组，每组分别对应某个存储区域，根据一个对象的 哈希码就可以确定该对象应该存储的那个区域。 

hashCode方法可以这样理解：它返回的就是根据对象的内存地址换算出的一个值。这样一来，当 集合要添加新的元素时，先调用这个元素的hashCode方法，就一下子能定位到它应该放置的物理 位置上。如果这个位置上没有元素，它就可以直接存储在这个位置上，不用再进行任何比较了；如 果这个位置上已经有元素了，就调用它的equals方法与新元素进行比较，相同的话就不存了，不相 同就散列其它的地址。这样一来实际调用equals方法的次数就大大降低了，几乎只需要一两次。

## hashCode()和equals()

**`hashCode()`与 `equals()` 的相关规定：**

1. 如果两个对象相等，则 `hashcode` 一定也是相同的
2. 两个对象相等,对两个 `equals()` 方法返回 true
3. 两个对象有相同的 `hashcode` 值，它们也不一定是相等的
4. 综上，`equals()` 方法被覆盖过，则 `hashCode()` 方法也必须被覆盖
5. `hashCode()`的默认行为是对堆上的对象产生独特值。如果没有重写 `hashCode()`，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。

**==与 equals 的区别**

对于基本类型来说，== 比较的是值是否相等；

对于引用类型来说，== 比较的是两个引用是否指向同一个对象地址（两者在内存中存放的地址（堆内存地址）是否指向同一个地方）；

对于引用类型（包括包装类型）来说，equals 如果没有被重写，对比它们的地址是否相等；如果 equals()方法被重写（例如 String），则比较的是地址里的内容。



## equals与==的区别

(基本类型：比较的是值是否相同；引用类型：比较的是引用是否相同。用来检测两个对象是否相等，即两个对象的内容是否相等；)

== 比较的是变量(栈)内存中存放的对象的(堆)内存地址，用来判断两个对象的地址是否相同，即是 否是指相同一个对象。比较的是真正意义上的指针操作。 1、比较的是操作符两端的操作数是否是同一个对象。 2、两边的操作数必须是同一类型的（可以是 父子类之间）才能编译通过。 3、比较的是地址，如果是具体的阿拉伯数字的比较，值相等则为 true，如： int a=10 与 long b=10L 与 double c=10.0都是相同的（为true），因为他们都指向地 址为10的堆。

equals： equals用来比较的是两个对象的内容是否相等，由于所有的类都是继承自java.lang.Object类的，所 以适用于所有对象，如果没有对该方法进行覆盖的话，调用的仍然是Object类中的方法，而Object 中的equals方法返回的却是==的判断。

总结： 

所有比较是否相等时，都是用equals 并且在对常量相比较时，把常量写在前面，因为使用object的 equals object可能为null 则空指针 

在阿里的代码规范中只使用equals ，阿里插件默认会识别，并可以快速修改，推荐安装阿里插件来 排查老代码使用“==”，替换成equals



## String、StringBuffer 和 StringBuilder 的区别是什么?

### String不可变的好处

**1. 可以缓存 hash 值**

因为 String 的 hash 值经常被使用，例如 String 用做 HashMap 的 key。不可变的特性可以使得 hash 值也不可变，因此只需要进行一次计算。

**2. String Pool 的需要**

如果一个 String 对象已经被创建过了，那么就会从 String Pool 中取得引用。只有 String 是不可变的，才可能使用 String Pool。

**3. 安全性**

String 经常作为参数，String 不可变性可以保证参数不可变。例如在作为网络连接参数的情况下如果 String 是可变的，那么在网络连接过程中，String 被改变，改变 String 对象的那一方以为现在连接的是其它主机，而实际情况却不一定是。

**4. 线程安全**

String 不可变性天生具备线程安全，可以在多个线程中安全地使用。

---



String是只读字符串，它并不是基本数据类型，而是一个对象。从底层源码来看是一个final类型的 字符数组，所引用的字符串不能被改变，一经定义，无法再增删改。每次对String的操作都会生成 新的String对象。

```java
private final char value[];//JDK8
private final byte value[];//JDK9及之后
```

每次+操作 ： 隐式在堆上new了一个跟原字符串相同的StringBuilder对象，再调用append方法 拼 接+后面的字符。

StringBuffer和StringBuilder他们两都继承了AbstractStringBuilder抽象类，从 AbstractStringBuilder抽象类中我们可以看到

```java
/**
* The value is used for character storage.
*/
char[] value;
```

他们的底层都是可变的字符数组，所以在进行频繁的字符串操作时，建议使用StringBuffer和 StringBuilder来进行操作。 另外StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所 以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。

---



**1. 可变性**

- String 不可变
- StringBuffer 和 StringBuilder 可变

**2. 线程安全**

- String 不可变，因此是线程安全的
- StringBuilder 不是线程安全的
- StringBuffer 是线程安全的，内部使用 synchronized 进行同步

---

## String.intern()

使用 String.intern() 可以保证相同内容的字符串变量引用同一的内存对象。

下面示例中，s1 和 s2 采用 new String() 的方式新建了两个不同对象，而 s3 是通过 s1.intern() 方法取得一个对象引用。intern() 首先把 s1 引用的对象放到 String Pool(字符串常量池)中，然后返回这个对象引用。因此 s3 和 s1 引用的是同一个字符串常量池的对象。

```java
String s1 = new String("aaa");
String s2 = new String("aaa");
System.out.println(s1 == s2);           // false
String s3 = s1.intern();
System.out.println(s1.intern() == s3);  // true

```

如果是采用 "bbb" 这种使用双引号的形式创建字符串实例，会自动地将新建的对象放入 String Pool 中。

```java
String s4 = "bbb";
String s5 = "bbb";
System.out.println(s4 == s5);  // true

```



在 Java 7 之前，字符串常量池被放在运行时常量池中，它属于永久代。而在 Java 7，字符串常量池被移到 Native Method 中（不是放堆里了吗）。这是因为永久代的空间有限，在大量使用字符串的场景下会导致 OutOfMemoryError 错误。

## Java创建对象方式

+ 用new语句创建对象。
+ 使用反射，使用Class.newInstance()创建对象。
+ 调用对象的clone()方法。
+ 运用反序列化手段，调用java.io.ObjectInputStream对象的`readObject()`方法。（破坏单例模式那里有这个）
+ 使用unsafe







---

1. 使用new关键字

如 User user=new User();

执行这条语句，jvm做了什么？

1. (类加载)首先在方法区的常量池中查看是否有new 后面参数（也就是类名）的符号引用，并检查是否有类的加载信息也就是是否被加载解析和初始化过。如果已经加载过了就不在加载，否则执行类的加载全过程
2. 加载完类后，大致做了如下三件事： a、给实例分配内存 b、调用构造函数，初始化成员字段 c、user对象指向分配的内存空间 注意：new操作不是原子操作，b和c的顺序可能会调换

2. 使用clone方法

当我们调用一个对象的clone方法，jvm就会创建一个新的对象，将前面对象的内容全部拷贝进去。用clone方法创建对象**并不会调用任何构造函数**。因为Object 类的 **clone 方法的 原理是从内存中（堆内存）以二进制流的方式进行拷贝，重新分配一个内存块**，那构造函数没有被执行也是非常正常的了.

```java
    protected native Object clone() throws CloneNotSupportedException;//Object类
```

注意：
1.clone是Object中的方法，Cloneable是一个标识接口，它表明这个类的对象是可以拷贝的。如果没有实现Cloneable接口却调用了clone()函数将抛出异常

2.Object.clone()未做同步处理，线程不安全

3.clone()有深拷贝和浅拷贝两种方式

使用clone方法创建对象的实例：

```java
public class Person implements Cloneable
{
    public String getName()
    {
        return name;
    }

    public void setName(String name)
    {
        this.name = name;
    }

    private String name;

    public Person(String name)
    {
        this.name = name;
    }

    public static void main(String[] args) {
        try {
            Person me = new Person("梁就");
            Person meclone = (Person)me.clone();
            System.out.println("我的名字:"+me.getName());
            System.out.println("我的克隆体名字:"+me.getName());
        } catch (CloneNotSupportedException e) {
            e.printStackTrace();
        }
    }

}

```

```
我的名字:梁就
我的克隆体名字:梁就
```



浅拷贝、深拷贝

https://segmentfault.com/a/1190000010648514

在 Java 中，除了**基本数据类型**（元类型）之外，还存在 **类的实例对象** 这个引用数据类型。而一般使用 『 **=** 』号做赋值操作的时候。对于基本数据类型，实际上是拷贝的它的值，但是对于对象而言，其实赋值的只是这个对象的引用，将原对象的引用传递过去，他们实际上还是指向的同一个对象。

而浅拷贝和深拷贝就是在这个基础之上做的区分，如果在拷贝这个对象的时候，只对基本数据类型进行了拷贝，而对引用数据类型只是进行了引用的传递，而没有真实的创建一个新的对象，则认为是浅拷贝。反之，在对引用数据类型进行拷贝的时候，创建了一个新的对象，并且复制其内的成员变量，则认为是深拷贝。

所以到现在，就应该了解了，所谓的浅拷贝和深拷贝，只是在拷贝对象的时候，对 **类的实例对象** 这种引用数据类型的不同操作而已。

总结来说：

1、浅拷贝：对基本数据类型进行值传递，对引用数据类型进行引用传递般的拷贝，此为浅拷贝。

![/clone-qian.png](D:\mdImage\1460000010648519)

（引用类型的，还是指向同一个地方）

2、深拷贝：对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容，此为深拷贝。

![/clone-深.png](D:\mdImage\1460000010648520)

Java 中的 clone()：

Object 上的 clone() 方法

在 Java 中，所有的 Class 都继承自 Object ，而在 Object 上，存在一个 clone() 方法，它被声明为了 `protected `，所以我们可以在其子类中，使用它。

而无论是浅拷贝还是深拷贝，都需要实现 clone() 方法，来完成操作。

可以看到，它的实现非常的简单，它限制所有调用 clone() 方法的对象，都必须实现 `Cloneable` 接口，否者将抛出 `CloneNotSupportedException` 这个异常。最终会调用 `internalClone()` 方法来完成具体的操作。而 `internalClone()` 方法，实则是一个 **native** 的方法。

![/clone-method.png](D:\mdImage\1460000010648521)

**Cloneable** 接口，可以看到它其实什么方法都不需要实现。对他可以简单的理解只是一个标记，是开发者允许这个对象被拷贝。

![/clone-cloneable.png](D:\mdImage\1460000010648522)



```java
class Home{
    public String getName()
    {
        return name;
    }

    public void setName(String name)
    {
        this.name = name;
    }

    public String name;

    public Home(String name)
    {
        this.name = name;
    }
}

public class Person implements Cloneable
{
    public String getName()
    {
        return name;
    }

    public void setName(String name)
    {
        this.name = name;
    }

    public Person(String name, Home home)
    {
        this.name = name;
        this.home = home;
    }

    private String name;
    private Home home;

    public Person(String name)
    {
        this.name = name;
    }

    public static void main(String[] args) {
        try {

            Home home = new Home("牛仔部落");
            Person me = new Person("梁就",home);
            Person meclone = (Person)me.clone();
            System.out.println("我的名字:"+me.getName());
            System.out.println("我的克隆体名字:"+meclone.getName());
            System.out.println("我和我的克隆体相等吗:"+(me==meclone));
            System.out.println("我的hashcode:"+me.hashCode());
            System.out.println("我的克隆体的hashcode:"+meclone.hashCode());
            System.out.println("我的快乐老家:"+me.home.hashCode());
            System.out.println("我的克隆体的快乐老家:"+meclone.home.hashCode());

        } catch (CloneNotSupportedException e) {
            e.printStackTrace();
        }
    }

}

```

```
我的名字:梁就
我的克隆体名字:梁就
我和我的克隆体相等吗:false
我的hashcode:1956725890
我的克隆体的hashcode:356573597
我的快乐老家:1735600054
我的克隆体的快乐老家:1735600054
```

可以看出，我和我的克隆体是不同的对象。但是我们的快乐老家都是一样的（也就是指向的home对象地址都一样）





基于Object#clone方法实现对象复制，但是Object#clone是protected方法，在覆写时，需要改成public修饰。我们需要在类的定义中实现Cloneable接口，标记对象是可以clone。

```java
class Home implements Cloneable //这里实现Cloneable接口
{
    @Override
//    protected Object clone() throws CloneNotSupportedException
    public Object clone() throws CloneNotSupportedException //这里改为Public 不然meclone.home = (Home)me.home.clone();会报错
    {
        //但我感觉应该在父类里面复写深拷贝子类对象的过程
        return super.clone();
    }

    public String getName()
    {
        return name;
    }

    public void setName(String name)
    {
        this.name = name;
    }

    public String name;

    public Home(String name)
    {
        this.name = name;
    }


}


public class Person implements Cloneable
{
    public String getName()
    {
        return name;
    }

    public void setName(String name)
    {
        this.name = name;
    }

    public Person(String name, Home home)
    {
        this.name = name;
        this.home = home;
    }

    private String name;
    private Home home;
    
    

    public Person(String name)
    {
        this.name = name;
    }

    public static void main(String[] args) {
        try {

            Home home = new Home("牛仔部落");
            Person me = new Person("梁就",home);
            Person meclone = (Person)me.clone();
            meclone.home = (Home)me.home.clone();
            System.out.println("我的名字:"+me.getName());
            System.out.println("我的克隆体名字:"+meclone.getName());
            System.out.println("我和我的克隆体相等吗:"+(me==meclone));
            System.out.println("我的hashcode:"+me.hashCode());
            System.out.println("我的克隆体的hashcode:"+meclone.hashCode());
            System.out.println("我的快乐老家:"+me.home.hashCode());
            System.out.println("我的克隆体的快乐老家:"+meclone.home.hashCode());

        } catch (CloneNotSupportedException e) {
            e.printStackTrace();
        }
    }

}

```

```
我的名字:梁就
我的克隆体名字:梁就
我和我的克隆体相等吗:false
我的hashcode:1956725890
我的克隆体的hashcode:356573597
我的快乐老家:1735600054
我的克隆体的快乐老家:21685669
```

我感觉应该在父类里面复写深拷贝子类对象的过程:

```java
class Home implements Cloneable //这里实现Cloneable接口
{
    @Override
//    protected Object clone() throws CloneNotSupportedException
    public Object clone() throws CloneNotSupportedException //这里改为Public 不然meclone.home = (Home)me.home.clone();会报错
    {
        //但我感觉应该在父类里面复写深拷贝子类对象的过程
        return super.clone();
    }

    public String getName()
    {
        return name;
    }

    public void setName(String name)
    {
        this.name = name;
    }

    public String name;

    public Home(String name)
    {
        this.name = name;
    }


}


public class Person implements Cloneable
{
    public String getName()
    {
        return name;
    }

    public void setName(String name)
    {
        this.name = name;
    }

    public Person(String name, Home home)
    {
        this.name = name;
        this.home = home;
    }

    private String name;
    private Home home;

    @Override
    protected Object clone() throws CloneNotSupportedException
    {
        Person person = (Person)super.clone();//重写这里
        person.home = (Home)this.home.clone();
        return person;
    }

    public Person(String name)
    {
        this.name = name;
    }

    public static void main(String[] args) {
        try {

            Home home = new Home("牛仔部落");
            Person me = new Person("梁就",home);
            Person meclone = (Person)me.clone();
//            meclone.home = (Home)me.home.clone();  去掉这里
            System.out.println("我的名字:"+me.getName());
            System.out.println("我的克隆体名字:"+meclone.getName());
            System.out.println("我和我的克隆体相等吗:"+(me==meclone));
            System.out.println("我的hashcode:"+me.hashCode());
            System.out.println("我的克隆体的hashcode:"+meclone.hashCode());
            System.out.println("我的快乐老家:"+me.home.hashCode());
            System.out.println("我的克隆体的快乐老家:"+meclone.home.hashCode());

        } catch (CloneNotSupportedException e) {
            e.printStackTrace();
        }
    }

}

```

```
我的名字:梁就
我的克隆体名字:梁就
我和我的克隆体相等吗:false
我的hashcode:1956725890
我的克隆体的hashcode:356573597
我的快乐老家:1735600054
我的克隆体的快乐老家:21685669
```



深拷贝的其他方式：1.复制构造函数 2.借助第三方库 https://developer.aliyun.com/article/609864#slide-6



---

反序列化

序列化是干什么的？

简单说就是为了保存在内存中的各种对象的状态(也就是实例变量，不是方法)，并且可以把保存的对象状态再读出来。虽然你可以用你自 己的各种各样的方法来保存object states，但是Java给你提供一种应该比你自己好的保存对象状态的机制，那就是序列化。一句话概括：序列化是指将对象的状态信息转换为可以存储或传输的形式的过程。

java中要序列化的类必要实现Serializable接口

##### 什么情况下需要序列化

a)当你想把的内存中的对象状态保存到一个文件中或者数据库中时候;

b)当你想用套接字在网络上传送对象的时候;

c)当你想通过RMI（远程方法调用）传输对象的时候;

使用反序列化创建对象实例：

1.对象要实现Serializable接口

```java
import java.io.Serializable; 
public class Person implements Serializable { 
    int age; 
    int height; 
    String name; 
    public Person(String name, int age, int height){ 
        this.name = name; 
        this.age = age; 
        this.height = height; 
    } 
}
```

2、序列化与反序列化

```java

import java.io.FileInputStream;

import java.io.FileOutputStream;

import java.io.IOException;

import java.io.ObjectInputStream;

import java.io.ObjectOutputStream;

public class STest {

    /**

     * Java对象的序列化与反序列化

     */
    public static void main(String[] args) {
        Student zhangsan = new Student("zhangsan", 11, 111);
        Student lisi = new Student("lisi", 12, 112);
        Student wangwu = new Student("wangwu", 13, 113);
        try {
            //需要一个文件输出流和对象输出流；文件输出流用于将字节输出到文件，对象输出流用于将对象输出为字节
            ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream("student.ser"));
            out.writeObject(zhangsan);
            out.writeObject(lisi);
            out.writeObject(wangwu);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}



```

![image-20220412163453001](D:\mdImage\image-20220412163453001.png)



```java

import java.io.FileInputStream;

import java.io.FileOutputStream;

import java.io.IOException;

import java.io.ObjectInputStream;

import java.io.ObjectOutputStream;

public class STest {

    /**

     * Java对象的序列化与反序列化

     */
    public static void main(String[] args) {
//        Student zhangsan = new Student("zhangsan", 11, 111);
//        Student lisi = new Student("lisi", 12, 112);
//        Student wangwu = new Student("wangwu", 13, 113);
//        try {
//            //需要一个文件输出流和对象输出流；文件输出流用于将字节输出到文件，对象输出流用于将对象输出为字节
//            ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream("student.ser"));
//            out.writeObject(zhangsan);
//            out.writeObject(lisi);
//            out.writeObject(wangwu);
//        } catch (IOException e) {
//            e.printStackTrace();
//        }

        try {
            ObjectInputStream in = new ObjectInputStream(new FileInputStream("student.ser"));
            Student one = (Student) in.readObject();
            Student two = (Student) in.readObject();
            Student three = (Student) in.readObject();
            System.out.println("name:"+one.name + " age:"+one.age + " height:"+one.height);
            System.out.println("name:"+two.name + " age:"+two.age + " height:"+two.height);
            System.out.println("name:"+three.name + " age:"+three.age + " height:"+three.height);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}



```

```
name:zhangsan age:11 height:111
name:lisi age:12 height:112
name:wangwu age:13 height:113
```



序列化破坏单例
问题演示：下面代码运行结果是false，表明序列化和反序列化破坏了单例设计模式。

```java
/**
 * 测试使用序列化破坏单例模式
 */
public class Client {
    public static void main(String[] args) throws Exception {
        Singleton s1 = readObjectFromFile();
        Singleton s2 = readObjectFromFile();
        System.out.println(s1 == s2); // false
    }
    // 从文件读取数据（对象）
    public static Singleton readObjectFromFile() throws Exception {
        // 1.创建对象输入流对象
        ObjectInputStream ois = new ObjectInputStream(new FileInputStream("C:\\Users\\Think\\Desktop\\a.txt"));
        // 2.读取对象
        Singleton instance = (Singleton) ois.readObject();
        // 3.释放资源
        ois.close();
        return instance;
    }
    // 向文件中写数据（对象）
    public static void writeObject2File() throws Exception {
        // 1.获取Singleton对象
        Singleton instance = Singleton.getInstance();
        // 2.创建对象输出流对象
        ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream("C:\\Users\\Think\\Desktop\\a.txt"));
        // 3.写对象
        oos.writeObject(instance);
        // 4.释放资源
        oos.close();
    }
}

```

解决方案：在 Singleton 类中添加readResolve()方法。

在反序列化时被反射调用，如果定义了这个方法，就返回这个方法的值，如果没有定义，则返回新 new 出来的对象。

```java
/**
 * 静态内部类方式(解决序列化破解单例模式)
 */
public class Singleton implements Serializable {
    // 私有构造方法
    private Singleton() {}
    // 提供公共的访问方式
    public static Singleton getInstance() {
        return SingletonHolder.INSTANCE;
    }
    /**
     * 下面是为了解决序列化反序列化破解单例模式
     * 当进行反序列化时，会自动调用该方法，将该方法的返回值直接返回
     */
    private Object readResolve() {
        return SingletonHolder.INSTANCE;
    }
    // 定义一个静态内部类
    private static class SingletonHolder {
        // 在内部类中声明并初始化外部类的对象
        private static final Singleton INSTANCE = new Singleton();
    }
}

```

---



反射

反射方法1：使用Class.newInstance()创建对象：

```java
import java.io.Serializable;
public class Student implements Serializable {
    int age;
    int height;
    String name;

    public int getAge()
    {
        return age;
    }

    /*
    注意 ：newInstance创建对象实例的时候会调用无参的构造函数，所以必需确保类中有无参数的可见的构造函数，否则将会抛出异常。
     */
    public Student()
    {
    }

    @Override
    public String toString()
    {
        return "Student{" +
                "age=" + age +
                ", height=" + height +
                ", name='" + name + '\'' +
                '}';
    }

    public void setAge(int age)
    {
        this.age = age;
    }

    public int getHeight()
    {
        return height;
    }

    public void setHeight(int height)
    {
        this.height = height;
    }

    public String getName()
    {
        return name;
    }

    public void setName(String name)
    {
        this.name = name;
    }

    public Student(String name, int age, int height){
        this.name = name;
        this.age = age;
        this.height = height;
    }
}

```

```java
public class reflectionExample
{
    public static void main(String[] args) throws IllegalAccessException, InstantiationException
    {
        Student student = Student.class.newInstance();

        student.setName("梁就");
        student.setAge(8);
        student.setHeight(80);

        System.out.println(student);


    }
}

```

```
Student{age=8, height=80, name='梁就'}
```

反射方法2：调用类对象的构造方法——Constructor

---

第五种：使用Unsafe



## 获取一个类Class对象的方式有哪些？

搞清楚类对象和实例对象，但都是对象。 

第一种：通过类对象的 getClass() 方法获取，细心点的都知道，这个 getClass 是 Object 类里面的 方法。

```java
User user=new User();
//clazz就是一个User的类对象
Class<?> clazz=user.getClass();

```

第二种：通过类的静态成员表示，每个类都有隐含的静态成员 class。

```java
//clazz就是一个User的类对象
Class<?> clazz=User.class;
```

第三种：通过 Class 类的静态方法 forName() 方法获取。

```java
Class<?> clazz = Class.forName("com.tian.User"); 
```



## IO流

Java 中 IO 流分为几种? 按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。

Java Io 流共涉及 40 多个类，这些类看上去很杂乱，但实际上很有规则，而且彼此之间存在非常紧 密的联系， Java I0 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。

## BIO/NIO/AIO

Java 共支持 3 种网络编程的/IO 模型：**BIO、NIO、AIO**

BIO：同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。

---

BIO：同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销 

NIO：同步非阻塞，（通过选择器监听多个通道，非阻塞，处理完成之后就返回），服务器实现模式为一个线程处理多个请求(连接)，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有 I/O 请求就进行处理 

AIO(NIO.2) ： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。

异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。

## **接口与抽象类区别？**

### 接口和抽象类有什么共同点和区别？

**共同点** ：

- 都不能被实例化。
- 都可以包含抽象方法。
- 都可以有默认实现的方法（Java 8 可以用 `default` 关键在接口中定义默认方法）。

**区别** ：

- 接口主要用于对类的行为进行约束，你实现了某个接口就具有了对应的行为。抽象类主要用于代码复用，强调的是所属关系（比如说我们抽象了一个发送短信的抽象类，）。
- **一个类只能继承一个抽象类，但是可以实现多个接口**。
- 接口中的成员变量只能是 `public static final` 类型的，不能被修改且必须有初始值，而抽象类的成员变量默认 default，可在子类中被重新定义，也可被重新赋值。



1. 抽象类可以有方法实现，而接口的方法中只能是抽象方法；
2. 一个类只能继承一个抽象类，而一个类却可以实现多个接口
3. 接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法；
4. 抽象类中的成员变量可以是各种类型的，接口中的成员变量只能是public static final类型；

## 反射

通过反射你可以获取任意一个类的所有属性和方法，你还可以调用这些方法和属性。

反射之所以被称为框架的灵魂，主要是因为它赋予了我们在运行时分析类以及执行类中方法的能力。

反射机制是在运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意个对象， 都能够调用它的任意一个方法。在java中，只要给定类的名字，就可以通过反射机制来获得类的所 有信息。这种动态获取的信息以及动态调用对象的方法的功能称为Java语言的反射机制。

哪里会用到反射机制？

jdbc就是典型的反射

```java
Class.forName('com.mysql.jdbc.Driver.class');//加载MySQL的驱动类
```

这就是反射。如hibernate，struts等框架使用反射实现的。

3、反射的实现方式： 第一步：获取Class对象，有4中方法： 1）Class.forName(“类的路径”)； 2）类名.class 3）对象 名.getClass() 4）基本类型的包装类，可以调用包装类的Type属性来获得该包装类的Class对象

4、实现Java反射的类： 1）Class：表示正在运行的Java应用程序中的类和接口 注意： 所有获取对象的信息都需要Class类 来实现。 2）Field：提供有关类和接口的属性信息，以及对它的动态访问权限。 3）Constructor： 提供关于类的单个构造方法的信息以及它的访问权限 4）Method：提供类或接口中某个方法的信息

5、反射机制的优缺点： 优点： 1）能够运行时动态获取类的实例，提高灵活性； 2）与动态编译结合 缺点： 1）使用反射 性能较低，需要解析字节码，将内存中的对象进行解析。 解决方案： 1、通过setAccessible(true) 关闭JDK的安全检查来提升反射速度； 2、多次创建一个类的实例时，有缓存会快很多 3、 ReflectASM工具类，通过字节码生成的方式加快反射速度 2）相对不安全，破坏了封装性（因为通 过反射可以获得私有方法和属性）

---



每个类都有一个  **Class**  对象，包含了与类有关的信息。当编译一个新类时，会产生一个同名的 .class 文件，该文件内容保存着 Class 对象。

类加载相当于 Class 对象的加载。类在第一次使用时才动态加载到 JVM 中，可以使用 `Class.forName("com.mysql.jdbc.Driver")` 这种方式来控制类的加载，该方法会返回一个 Class 对象。

反射可以提供运行时的类信息，并且这个类可以在运行时才加载进来，甚至在编译时期该类的 .class 不存在也可以加载进来。

Class 和 java.lang.reflect 一起对反射提供了支持，java.lang.reflect 类库主要包含了以下三个类:

- **Field** : 可以使用 get() 和 set() 方法读取和修改 Field 对象关联的字段；
- **Method** : 可以使用 invoke() 方法调用与 Method 对象关联的方法；
- **Constructor** : 可以用 Constructor 创建新的对象。

---

## 异常

**Checked Exception** 即受检查异常，Java 代码在编译过程中，如果受检查异常没有被 `catch`/`throw` 处理的话，就没办法通过编译 。

**Unchecked Exception** 即 **不受检查异常** ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。`RuntimeException` 及其子类都统称为非受检查异常，例如：`NullPointerException`、`NumberFormatException`（字符串转换为数字）、`ArrayIndexOutOfBoundsException`（数组越界）、`ClassCastException`（类型转换错误）、`ArithmeticException`（算术错误）等。

---



Throwable 可以用来表示任何可以作为异常抛出的类，分为两种:  **Error**  和 **Exception**。其中 Error 用来表示 JVM 无法处理的错误，Exception 分为两种:

- **受检异常** : 需要用 try...catch... 语句捕获并进行处理，并且可以从异常中恢复；
- **非受检异常** : 是程序运行时错误，例如除 0 会引发 Arithmetic Exception，此时程序崩溃并且无法恢复。

![image-20220412204035221](D:\mdImage\image-20220412204035221.png)

## 泛型

泛型提供了编译时类型安全检测机制，该机制允许程序员在编译时检测到非法的类型。泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。

Java 的泛型是伪泛型，这是因为 Java 在运行期间，所有的泛型信息都会被擦掉，这也就是通常所说类型擦除 。

**常用的通配符为： T，E，K，V，？**

- ？ 表示不确定的 Java 类型
- T (type) 表示具体的一个 Java 类型
- K V (key value) 分别代表 Java 键值中的 Key Value
- E (element) 代表 Element

---

使用泛型的好处？ 

以集合来举例，使用泛型的好处是我们不必因为添加元素类型的不同而定义不同类型的集合，如整 型集合类，浮点型集合类，字符串集合类，我们可以定义一个集合来存放整型、浮点型，字符串型 数据，而这并不是最重要的，因为我们只要把底层存储设置了Object即可，添加的数据全部都可向 上转型为Object。 更重要的是我们可以通过规则按照自己的想法控制存储的数据类型。



## 注解

## 序列化

对于不想进行序列化的变量，使用 transient 关键字修饰。 transient 关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化 时，被 transient 修饰的变量值不会被持久化和恢复。transient 只能修饰变量，不能修饰类和方 法。

## Java的四种引用，强软弱虚

强引用 

强引用是平常中使用最多的引用，强引用在程序内存不足（OOM）的时候也不会被回收，使用 方式：（宁愿抛出OOM错误）

```java
String str = new String("str");
System.out.println(str);
```

软引用 

软引用在程序内存不足时，会被回收，使用方式：

```java
// 注意：wrf这个引用也是强引用，它是指向SoftReference这个对象的，
// 这里的软引用指的是指向new String("str")的引用，也就是SoftReference类中T
SoftReference<String> wrf = new SoftReference<String>(new String("str"));
```

可用场景： 创建缓存的时候，创建的对象放进缓存中，当内存不足时，JVM就会回收早先创建 的对象。

---

弱引用 

弱引用就是只要JVM垃圾回收器发现了它，就会将之回收，使用方式：

```java
WeakReference<String> wrf = new WeakReference<String>(str)
```

可用场景： Java源码中的 java.util.WeakHashMap 中的 key 就是使用弱引用，我的理解就是， 一旦我不需要某个引用，JVM会自动帮我处理它，这样我就不需要做其它操作。

---

虚引用 

虚引用的回收机制跟弱引用差不多，但是它被回收之前，会被放入 ReferenceQueue 中。注意 哦，其它引用是被JVM回收后才被传入 ReferenceQueue 中的。由于这个机制，所以虚引用大多 被用于引用销毁前的处理工作。还有就是，虚引用创建的时候，必须带有 ReferenceQueue ， 使用例子：

```java
PhantomReference<String> prf = new PhantomReference<String>(new String("str"),
new ReferenceQueue<>());
```

可用场景： 对象销毁前的一些操作，比如说资源释放等。 Object.finalize() 虽然也可以做这 类动作，但是这个方式即不安全又低效 上诉所说的几类引用，都是指对象本身的引用，而不是指Reference的四个子类的引用 (SoftReference等)。



## final有哪些用法

- 被final修饰的类不可以被继承 

- 被final修饰的方法不可以被重写 

- 被final修饰的变量不可以被改变.如果修饰引用,那么表示引用不可变,引用指向的内容可变. 

- 被final修饰的方法,JVM会尝试将其内联,以提高运行效率 

- 被final修饰的常量,在编译阶段会存入常量池中.

---



**1. 数据**

声明数据为常量，可以是编译时常量，也可以是在运行时被初始化后不能被改变的常量。

- 对于基本类型，final 使数值不变；
- 对于引用类型，final 使引用不变，也就不能引用其它对象，但是被引用的对象本身是可以修改的。

```
final int x = 1;
// x = 2;  // cannot assign value to final variable 'x'
final A y = new A();
y.a = 1;

```



**2. 方法**

声明方法不能被子类重写。

private 方法隐式地被指定为 final，如果在子类中定义的方法和基类中的一个 private 方法签名相同，此时子类的方法不是重写基类方法，而是在子类中定义了一个新的方法。



**3. 类**

声明类不允许被继承。

## static都有哪些用法?

所有的人都知道static关键字这两个基本的用法:静态变量和静态方法.也就是被static所修饰的变量/ 方法都属于类的静态资源,类实例所共享. 除了静态变量和静态方法之外,static也用于静态块,多用于初始化操作:

```java
public calss PreCache{
    static{
        //执行相关操作
    }
}
```

此外static也多用于修饰内部类,此时称之为静态内部类

---



**1. 静态变量**

- 静态变量: 又称为类变量，也就是说这个变量属于类的，类所有的实例都共享静态变量，可以直接通过类名来访问它；静态变量在内存中只存在一份。
- 实例变量: 每创建一个实例就会产生一个实例变量，它与该实例同生共死。

```java
public class A {
    private int x;         // 实例变量
    private static int y;  // 静态变量

    public static void main(String[] args) {
        // int x = A.x;  // Non-static field 'x' cannot be referenced from a static context
        A a = new A();
        int x = a.x;
        int y = A.y;
    }
}

```

**2. 静态方法**

静态方法在类加载的时候就存在了，它不依赖于任何实例。所以静态方法必须有实现，也就是说它不能是抽象方法(abstract)。

```java
public abstract class A {
    public static void func1(){
    }
    // public abstract static void func2();  // Illegal combination of modifiers: 'abstract' and 'static'
}

```

只能访问所属类的静态字段和静态方法，方法中不能有 this 和 super 关键字。

```java
public class A {
    private static int x;
    private int y;

    public static void func1(){
        int a = x;
        // int b = y;  // Non-static field 'y' cannot be referenced from a static context
        // int b = this.y;     // 'A.this' cannot be referenced from a static context
    }
}

```

**3. 静态语句块**

静态语句块在类初始化时运行一次。

```java
public class A {
    static {
        System.out.println("123");
    }

    public static void main(String[] args) {
        A a1 = new A();
        A a2 = new A();
    }
}

```

```
123

```

**4. 静态内部类**

非静态内部类依赖于外部类的实例，而静态内部类不需要。

```java
public class OuterClass {
    class InnerClass {
    }

    static class StaticInnerClass {
    }

    public static void main(String[] args) {
        // InnerClass innerClass = new InnerClass(); // 'OuterClass.this' cannot be referenced from a static context
        OuterClass outerClass = new OuterClass();
        InnerClass innerClass = outerClass.new InnerClass();
        StaticInnerClass staticInnerClass = new StaticInnerClass();
    }
}

```

静态内部类不能访问外部类的非静态的变量和方法。

**5. 静态导包**

在使用静态变量和方法时不用再指明 ClassName，从而简化代码，但可读性大大降低。

```
import static com.xxx.ClassName.*

```

**6. 初始化顺序**

静态变量和静态语句块优先于实例变量和普通语句块，静态变量和静态语句块的初始化顺序取决于它们在代码中的顺序。

```
public static String staticField = "静态变量";

```

```
static {
    System.out.println("静态语句块");
}

```

```
public String field = "实例变量";

```

```
{
    System.out.println("普通语句块");
}

```

最后才是构造函数的初始化。

```java
public InitialOrderTest() {
    System.out.println("构造函数");
}
```

存在继承的情况下，初始化顺序为:

- 父类(静态变量、静态语句块)
- 子类(静态变量、静态语句块)
- 父类(实例变量、普通语句块)
- 父类(构造函数)
- 子类(实例变量、普通语句块)
- 子类(构造函数)

## a=a+b与a+=b有什么区别吗?

+= 操作符会进行隐式自动类型转换,此处a+=b隐式的将加操作的结果类型强制转换为持有结果的类 型,而a=a+b则不会自动进行类型转换.如：

```
byte a = 127;
byte b = 127;
b = a + b; // 报编译错误:cannot convert from int to byte
b += a;

```

以下代码是否有错,有的话怎么改？

```
short s1= 1;
s1 = s1 + 1;

```

有错误.short类型在进行运算时会自动提升为int类型,也就是说 s1+1 的运算结果是int类型,而s1是 short类型,此时编译器会报错.

正确写法：

```
short s1= 1;
s1 += 1;
```

## try catch finally，try里有return，finally还执行么？

执行，**并且finally的执行早于try里面的return**

结论： 

1、不管有没有出现异常，finally块中代码都会执行； 

2、当try和catch中有return时，finally仍然会执行； 

3、finally是在return后面的表达式运算后执行的（此时并没有返回运算后的值，而是先把要返回的 值保存起来，管finally中的代码怎么样，返回的值都不会改变，任然是之前保存的值），所以函数 返回值是在finally执行前确定的； 

4、finally中最好不要包含return，否则程序会提前退出，返回值不是try或catch中保存的返回值。



## 说说你平时是怎么处理 Java 异常的

try-catch-finally try 块负责监控可能出现异常的代码 catch 块负责捕获可能出现的异常，并进行处理 finally 块负责清理各种资源，不管是否出现异常都会执行 其中 try 块是必须的，catch 和 finally 至少存在一个标准异常处理流程

![image-20220412192348224](D:\mdImage\image-20220412192348224.png)

抛出异常→捕获异常→捕获成功（当 catch 的异常类型与抛出的异常类型匹配时，捕获成功） →异常被处理，程序继续运行 抛出异常→捕获异常→捕获失败（当 catch 的异常类型与抛出异 常类型不匹配时，捕获失败）→异常未被处理，程序中断运行

在开发过程中会使用到自定义异常，在通常情况下，程序很少会自己抛出异常，因为异常的类名通 常也包含了该异常的有用信息，所以在选择抛出异常的时候，应该选择合适的异常类，从而可以明 确地描述该异常情况，所以这时候往往都是自定义异常。

自定义异常通常是通过继承 java.lang.Exception 类，如果想自定义 Runtime 异常的话，可以继承 java.lang.RuntimeException 类，实现一个无参构造和一个带字符串参数的有参构造方法。

在业务代码里，可以针对性的使用自定义异常。比如说：该用户不具备某某权限、余额不足等。

## Excption与Error包结构



Java可抛出(Throwable)的结构分为三种类型：被检查的异常(CheckedException)，运行时异常 (RuntimeException)，错误(Error)。 

1、运行时异常 

定义:RuntimeException及其子类都被称为运行时异常。

特点:Java编译器不会检查它。也就是说，当程序中可能出现这类异常时，倘若既"没有通过throws 声明抛出它"，也"没有用try-catch语句捕获它"，还是会编译通过。例如，除数为零时产生的 ArithmeticException异常，数组越界时产生的IndexOutOfBoundsException异常，fail-fast机制产 生的ConcurrentModificationException异常（java.util包下面的所有的集合类都是快速失败 的，“快速失败”也就是fail-fast，它是Java集合的一种错误检测机制。当多个线程对集合进行结构上 的改变的操作时，有可能会产生fail-fast机制。记住是有可能，而不是一定。例如：假设存在两个线 程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的 结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生fail-fast机制，这个错叫并发修改异常。Failsafe，java.util.concurrent包下面的所有的类都是安全失败的，在遍历过程中，如果已经遍历的数 组上的内容变化了，迭代器不会抛出ConcurrentModificationException异常。如果未遍历的数组 上的内容发生了变化，则有可能反映到迭代过程中。这就是ConcurrentHashMap迭代器弱一致的 表现。ConcurrentHashMap的弱一致性主要是为了提升效率，是一致性与效率之间的一种权衡。 要成为强一致性，就得到处使用锁，甚至是全局锁，这就与Hashtable和同步的HashMap一样 了。）等，都属于运行时异常。

常见的五种运行时异常： ClassCastException（类转换异常） IndexOutOfBoundsException（数组越界） NullPointerException（空指针异常） ArrayStoreException（数据存储异常，操作数组是类型不一致）、BufferOverflowException

2、被检查异常 

定义:Exception类本身，以及Exception的子类中除了"运行时异常"之外的其它子类都属于被检查异常。

特点 : Java编译器会检查它。 此类异常，要么通过throws进行声明抛出，要么通过try-catch进行捕 获处理，否则不能通过编译。例如，CloneNotSupportedException就属于被检查异常。当通过 clone()接口去克隆一个对象，而该对象对应的类没有实现Cloneable接口，就会抛出 CloneNotSupportedException异常。被检查异常通常都是可以恢复的。 如：

IOException 

FileNotFoundException 

SQLException

被检查的异常适用于那些不是因程序引起的错误情况，比如：读取文件时文件不存在引发的 FileNotFoundException 。然而，不被检查的异常通常都是由于糟糕的编程引起的，比如：在对象 引用时没有确保对象非空而引起的 NullPointerException 。

3、错误 

定义 : Error类及其子类。 

特点 : 和运行时异常一样，编译器也不会对错误进行检查。 

当资源不足、约束失败、或是其它程序无法继续运行的条件发生时，就产生错误。程序本身无法修 复这些错误的。例如，VirtualMachineError就属于错误。出现这种错误会导致程序终止运行。 OutOfMemoryError、ThreadDeath。 Java虚拟机规范规定JVM的内存分为了好几块，比如堆，栈，程序计数器，方法区等



## Object 有哪些常用方法？大致说一下每个方法的含义

clone 方法 

保护(protected)方法，实现对象的浅复制，只有实现了 Cloneable 接口才可以调用该方法，否则抛出 CloneNotSupportedException 异常，深拷贝也需要实现 Cloneable，同时其成员变量为引用类型 的也需要实现 Cloneable，然后重写 clone 方法。

finalize 方法 

该方法和垃圾收集器有关系，判断一个对象是否可以被回收的最后一步就是判断是否重写了此方法。

equals 方法 

该方法使用频率非常高。一般 equals 和 == 是不一样的，但是在 Object 中两者是一样的。子类一 般都要重写这个方法。

hashCode 方法 

该方法用于哈希查找，重写了 equals 方法一般都要重写 hashCode 方法，这个方法在一些具有哈 希功能的 Collection 中用到。

wait 方法

配合 synchronized 使用，wait 方法就是使当前线程等待该对象的锁，当前线程必须是该对象的拥 有者，也就是具有该对象的锁。wait() 方法一直等待，直到获得锁或者被中断。wait(long timeout) 设定一个超时间隔，如果在规定时间内没有获得锁就返回。

调用该方法后当前线程进入睡眠状态，直到以下事件发生。 1. 其他线程调用了该对象的 notify 方法； 2. 其他线程调用了该对象的 notifyAll 方法； 3. 其他线程调用了 interrupt 中断该线程； 4. 时间间隔到了。 

此时该线程就可以被调度了，如果是被中断的话就抛出一个 InterruptedException 异常。

notify 方法 

配合 synchronized 使用，该方法唤醒在该对象上等待队列中的某个线程（同步队列中的线程是给 抢占 CPU 的线程，等待队列中的线程指的是等待唤醒的线程）。

notifyAll 方法

 配合 synchronized 使用，该方法唤醒在该对象上等待队列中的所有线程。

# JVM

## java内存区域

**线程私有的：**

- 程序计数器
- 虚拟机栈
- 本地方法栈

**线程共享的：**

- 堆
- 方法区
- 直接内存 (非运行时数据区的一部分)

### 程序计数器

当前线程所执行的字节码的行号指示器。记录字节码指令的地址。没有规定任何OutOfMemoryError 情况。

程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。**字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。**

另外，**为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。**

**从上面的介绍中我们知道程序计数器主要有两个作用：**

1. 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。
2. 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。

**注意：程序计数器是唯一一个不会出现 `OutOfMemoryError` 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡**

### Java 虚拟机栈

Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息

**局部变量表主要存放了编译期可知的各种数据类型**（boolean、byte、char、short、int、float、long、double）、**对象引用**（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。

**Java 虚拟机栈会出现两种错误：`StackOverFlowError` 和 `OutOfMemoryError`。**

### 本地方法栈

和虚拟机栈所发挥的作用非常相似，区别是： **虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。** 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。

本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。

方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 `StackOverFlowError` 和 `OutOfMemoryError` 两种错误

### 堆

Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。**此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。**

Java 世界中“几乎”所有的对象都在堆中分配，但是，随着 JIT 编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从 JDK 1.7 开始已经默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。

Java 堆是垃圾收集器管理的主要区域，因此也被称作**GC 堆（Garbage Collected Heap）**。从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代；再细致一点有：Eden、Survivor、Old 等空间。**进一步划分的目的是更好地回收内存，或者更快地分配内存。**

在 JDK 7 版本及 JDK 7 版本之前，堆内存被通常分为下面三部分：

1. 新生代内存(Young Generation)
2. 老生代(Old Generation)
3. 永久代(Permanent Generation)

Eden 区、两个 Survivor 区 S0 和 S1 都属于新生代。

**JDK 8 版本之后 PermGen 已被 Metaspace(元空间) 取代，元空间使用的是直接内存。**

大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 S0 或者 S1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。

堆这里最容易出现的就是 `OutOfMemoryError` 错误，并且出现这种错误之后的表现形式还会有几种，比如：

1. **`java.lang.OutOfMemoryError: GC Overhead Limit Exceeded`** ： 当 JVM 花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。
2. **`java.lang.OutOfMemoryError: Java heap space`** :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发此错误。

### 方法区

方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 **Java 虚拟机规范把方法区描述为堆的一个逻辑部分**，但是它却有一个别名叫做 **Non-Heap（非堆）**，目的应该是与 Java 堆区分开来。

#### 常用参数

JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小

```
-XX:PermSize=N //方法区 (永久代) 初始大小
-XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen
```

JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。

```
-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小）
-XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小

```

### 运行时常量池

运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池表（用于存放编译期生成的各种字面量和符号引用）

既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。

### 直接内存

JDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel）与缓存区（Buffer）的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。

### 对象的创建

![Java创建对象的过程](D:\系统学习笔记\a-面试背诵\img\Java创建对象的过程.91a83a4b.png)

#### Step1:类加载检查

虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。

#### Step2:分配内存

在**类加载检查**通过后，接下来虚拟机将为新生对象**分配内存**。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。**分配方式**有 **“指针碰撞”** 和 **“空闲列表”** 两种，**选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定**。

#### Step3:初始化零值

内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。

#### Step4:设置对象头

初始化零值完成之后，**虚拟机要对对象进行必要的设置**，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 **这些信息存放在对象头中。** 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。

#### Step5:执行 init 方法

在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，`<init>` 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 `<init>` 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来

### 对象的内存布局

在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：**对象头**、**实例数据**和**对齐填充**。

**Hotspot 虚拟机的对象头包括两部分信息**，**第一部分用于存储对象自身的运行时数据**（哈希码、GC 分代年龄、锁状态标志等等），**另一部分是类型指针**，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。

**实例数据部分是对象真正存储的有效信息**，也是在程序中所定义的各种类型的字段内容。

**对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。** 

### 字符串常量池常见问题

+ 字符串常量拼接得到的字符串常量在编译阶段就已经被存放字符串常量池，这个得益于编译器的优化。

**只要使用 new 的方式创建对象，便需要创建新的对象** 。

使用 new 的方式创建对象的方式如下，可以简单概括为 3 步：

1. 在堆中创建一个字符串对象
2. 检查字符串常量池中是否有和 new 的字符串值相等的字符串常量
3. 如果没有的话需要在字符串常量池中也创建一个值相等的字符串常量，如果有的话，就直接返回堆中的字符串实例对象地址。

String s1 = new String("abc");这句话创建了几个字符串对象？

会创建 1 或 2 个字符串：

- 如果字符串常量池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。
- 如果字符串常量池中没有字符串常量“abc”，那么它将首先在字符串常量池中创建，然后在堆空间中创建，因此将创建总共 2 个字符串对象。

### 8 种基本类型的包装类和常量池

Java 基本类型的包装类的大部分都实现了常量池技术。

`Byte`,`Short`,`Integer`,`Long` 这 4 种包装类默认创建了数值 **[-128，127]** 的相应类型的缓存数据，`Character` 创建了数值在 **[0,127]** 范围的缓存数据，`Boolean` 直接返回 `True` Or `False`。

两种浮点数类型的包装类 `Float`,`Double` 并没有实现常量池技术。

## JVM垃圾回收

Java 的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java 自动内存管理最核心的功能是 **堆** 内存中对象的分配与回收。

Java 堆是垃圾收集器管理的主要区域，因此也被称作**GC 堆（Garbage Collected Heap）**.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。**进一步划分的目的是更好地回收内存，或者更快地分配内存。**

**堆空间的基本结构：**

![img](D:\系统学习笔记\a-面试背诵\img\01d330d8-2710-4fad-a91c-7bbbfaaefc0e.c5bf5d75.png)

上图所示的 Eden 区、From Survivor0("From") 区、To Survivor1("To") 区都属于新生代，Old Memory 区属于老年代。

大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为大于 15 岁），就会被晋升到老年代中。

经过这次 GC 后，Eden 区和"From"区已经被清空。这个时候，"From"和"To"会交换他们的角色，也就是新的"To"就是上次 GC 前的“From”，新的"From"就是上次 GC 前的"To"。不管怎样，都会保证名为 To 的 Survivor 区域是空的。Minor GC 会一直重复这样的过程，在这个过程中，有可能当次 Minor GC 后，Survivor 的"From"区域空间不够用，有一些还达不到进入老年代条件的实例放不下，则放不下的部分会提前进入老年代。

##  对象分配

### 1.1 对象优先在 eden 区分配

目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。

大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC

> **简单解释一下为什么会出现这种情况：** 因为给 allocation2 分配内存的时候 eden 区内存几乎已经被分配完了，我们刚刚讲了当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.GC 期间虚拟机又发现 allocation1 无法存入 Survivor 空间，所以只好通过 **分配担保机制** 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放 allocation1，所以不会出现 Full GC。执行 Minor GC 后，后面分配的对象如果能够存在 eden 区的话，还是会在 eden 区分配内存。

### 1.2 大对象直接进入老年代

大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。

**为什么要这样呢？**

为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。

### 1.3 长期存活的对象将进入老年代

既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。

如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为 1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 `-XX:MaxTenuringThreshold` 来设置。

### 1.4 动态对象年龄判定

大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 `-XX:MaxTenuringThreshold` 来设置。



### 1.5 主要进行 gc 的区域

针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种：

部分收集 (Partial GC)：

- 新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集；
- 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集；
- 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。

整堆收集 (Full GC)：收集整个 Java 堆和方法区。

### 1.6 空间分配担保

空间分配担保是为了确保在 Minor GC 之前老年代本身还有容纳新生代所有对象的剩余空间。

## 引用计数法、可达性分析、引用

堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。

### 2.1 引用计数法

**目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。** 

### 2.2 可达性分析算法

可达性分析算法是以GC Roots为起始点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的，需要被回收。

**哪些对象可以作为 GC Roots 呢？**

- 虚拟机栈(栈帧中的本地变量表)中引用的对象 比如：各个线程被调用的方法中使用到的参数、局部变量等。
- 本地方法栈(Native 方法)中引用的对象
- 方法区中类静态属性引用的对象 比如：Java类的引用类型静态变量
- 方法区中常量引用的对象 字符串常量池（StringTable）里的引用
- 所有被同步锁synchronized持有的对象
- Java虚拟机内部的引用。
  - 基本数据类型对应的Class对象，一些常驻的异常对象（如：NullPointerException、OutofMemoryError），系统类加载器。

**对象可以被回收，就代表一定会被回收吗？**

即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 `finalize` 方法。当对象没有覆盖 `finalize` 方法，或 `finalize` 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。

被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。

### 2.3 再谈引用

**强引用（StrongReference）**

如果一个对象具有强引用，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。

**软引用（SoftReference）**

如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。

软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。

**弱引用（WeakReference）**

在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。

弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。

**虚引用（PhantomReference）**

"虚引用"顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。





### 如何判断一个类是无用的类

方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？

判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 **“无用的类”** ：

- 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。
- 加载该类的 `ClassLoader` 已经被回收。
- 该类对应的 `java.lang.Class` 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。



## 垃圾收集算法

### 标记-清除算法

该算法分为“标记”和“清除”阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题：

1. **效率问题**
2. **空间问题（标记清除后会产生大量不连续的碎片）**



### 标记-复制算法

为了解决效率问题，“标记-复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。

不会有内存碎片

需要占用双倍内存空间

### 标记-整理算法

根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。

速度慢

没有内存碎片

### 分代收集算法

当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。

**比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。**

## :exclamation:垃圾收集器

### Serial 收集器

这个收集器是一个单线程收集器。它的 **“单线程”** 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ **"Stop The World"** ），直到它收集结束。

**新生代采用标记-复制算法，老年代采用标记-整理算法。**

### ParNew 收集器

**ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。**

**新生代采用标记-复制算法，老年代采用标记-整理算法。**

### Parallel Scavenge 收集器

**Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。** Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解，手工优化存在困难的时候，使用 Parallel Scavenge 收集器配合自适应调节策略，把内存管理优化交给虚拟机去完成也是一个不错的选择。

**新生代采用标记-复制算法，老年代采用标记-整理算法。**

**这是 JDK1.8 默认收集器**

### Serial Old 收集器

**Serial 收集器的老年代版本**，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。

### Parallel Old 收集器

**Parallel Scavenge 收集器的老年代版本**。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。

### CMS 收集器

**CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。**

**CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。**

从名字中的**Mark Sweep**这两个词可以看出，CMS 收集器是一种 **“标记-清除”算法**实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：

- **初始标记：** 暂停所有用户线程，并记录下直接与Gc root 相连的对象，速度很快 ；
- **并发标记：** 从GC Roots的直接关联对象开始遍历整个对象图，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。但在这个阶段结束，这因为用户线程可能会不断的更新引用域，所以需要跟踪记录这些发生引用更新的地方。
- **重新标记：暂停所有用户线程，修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录** 。这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短
- **并发清除：** 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。

从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：**并发收集、低停顿**。但是它有下面三个明显的缺点：

- **对 CPU 资源敏感；**
- **无法处理浮动垃圾；**
- **它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。**



### G1 收集器

**G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征.**

- **可预测的停顿**：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。
- **空间整合**：与 CMS 的“标记-清理”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。
- **并行与并发**：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。
- **分代收集**：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。

**G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)** 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）

> G1 收集器对各个分区回收所获得的空间大小和回收所需时间的经验值进行排序，得到一个优先级列表，每次根据用户设置的最大回收停顿时间，优先回收价值最大的分区。
>
> **特点**：可以由用户**指定**期望的垃圾收集停顿时间。

G1 收集器的回收过程分为以下几个步骤：

**初始标记**。暂停所有其他线程，记录直接与 GC Roots 直接相连的对象，耗时较短 。

**并发标记**。从 GC Roots 开始对堆中对象进行可达性分析，找出要回收的对象，耗时较长，不过可以和用户程序并发执行。

**最终标记**。需对其他线程做短暂的暂停，用于处理并发标记阶段对象引用出现变动的区域。

**筛选回收**。对各个分区的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，然后把决定回收的分区的存活对象复制到空的分区中，再清理掉整个旧的分区的全部空间。这里的操作涉及存活对象的移动，会暂停用户线程，由多条收集器线程并行完成。



分区：

**分区 Region：化整为零**

1.  使用G1收集器时，它将整个Java堆划分成约2048个大小相同的独立Region块，每个Region块大小根据堆空间的实际大小而定，整体被控制在1MB到32MB之间，且为2的N次幂，即1MB，2MB，4MB，8MB，16MB，32MB。可以通过

2.  XX:G1HeapRegionSize设定。**所有的Region大小相同，且在JVM生命周期内不会被改变。**



**Region的细节**

<img src="https://unpkg.zhimg.com/youthlql@1.0.8/JVM/chapter_012/0019.png">

1.  每个Region都是通过指针碰撞来分配空间
2.  G1为每一个Region设 计了两个名为TAMS（Top at Mark Start）的指针，把Region中的一部分空间划分出来用于并发回收过程中的新对象分配，并发回收时新分配的对象地址都必须要在这两个指针位置以上。
3.  TLAB还是用来保证并发性

#### G1 垃圾回收流程（尚硅谷）



G1 GC的垃圾回收过程主要包括如下三个环节：

*   年轻代GC（Young GC）
*   老年代并发标记过程（Concurrent Marking）
*   混合回收（Mixed GC）
*   （如果需要，单线程、独占式、高强度的Full GC还是继续存在的。它针对GC的评估失败提供了一种失败保护机制，即强力回收。）

**回收流程**

1.  应用程序分配内存，当年轻代的Eden区用尽时开始年轻代回收过程；G1的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1 GC暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到Survivor区间或者老年区间，也有可能是两个区间都会涉及。
2.  当堆内存使用达到一定值（默认45%）时，开始老年代并发标记过程。
3.  标记完成马上开始混合回收过程。对于一个混合回收期，G1 GC从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的G1回收器和其他GC不同，**G1的老年代回收器不需要整个老年代被回收，一次只需要扫描/回收一小部分老年代的Region就可以了**。同时，这个老年代Region是和年轻代一起被回收的。

Remembered Set（记忆集）

1.  一个对象被不同区域引用的问题

2.  一个Region不可能是孤立的，一个Region中的对象可能被其他任意Region中对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确？

3.  在其他的分代收集器，也存在这样的问题（而G1更突出，因为G1主要针对大堆）

4.  回收新生代也不得不同时扫描老年代？这样的话会降低Minor GC的效率


**解决方法：**

1.  无论G1还是其他分代收集器，JVM都是使用Remembered Set来避免全堆扫描；
2.  每个Region都有一个对应的Remembered Set
3.  每次Reference类型数据写操作时，都会产生一个Write Barrier暂时中断操作；
4.  然后检查将要写入的引用指向的对象是否和该Reference类型数据在不同的Region（其他收集器：检查老年代对象是否引用了新生代对象）；
5.  如果不同，通过CardTable把相关引用信息记录到引用指向对象的所在Region对应的Remembered Set中；
6.  当进行垃圾收集时，在GC根节点的枚举范围加入Remembered Set；就可以保证不进行全局扫描，也不会有遗漏。

---

1.  在回收 Region 时，为了不进行全堆的扫描，引入了 Remembered Set
2.  Remembered Set 记录了当前 Region 中的对象被哪个对象引用了
3.  这样在进行 Region 复制时，就不要扫描整个堆，只需要去 Remembered Set 里面找到引用了当前 Region 的对象
4.  Region 复制完毕后，修改 Remembered Set 中对象的引用即可

##### G1回收过程一：年轻代 GC

1.  JVM启动时，G1先准备好Eden区，程序在运行过程中不断创建对象到Eden区，当Eden空间耗尽时，G1会启动一次年轻代垃圾回收过程。
2.  年轻代回收只回收Eden区和Survivor区
3.  YGC时，首先G1停止应用程序的执行（Stop-The-World），G1创建回收集（Collection Set），回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代Eden区和Survivor区所有的内存分段。

图的大致意思就是：

1、回收完E和S区，剩余存活的对象会复制到新的S区

2、S区达到一定的阈值可以晋升为O区

**细致过程：**

**然后开始如下回收过程：**

1. 第一阶段，扫描根

   根是指GC Roots，根引用连同RSet记录的外部引用作为扫描存活对象的入口。

2. 第二阶段，更新RSet

3. 第三阶段，处理RSet

   识别被老年代对象指向的Eden中的对象，这些被指向的Eden中的对象被认为是存活的对象。

4. 第四阶段，复制对象。

   *   此阶段，对象树被遍历，Eden区内存段中存活的对象会被复制到Survivor区中空的内存分段，Survivor区内存段中存活的对象
   *   如果年龄未达阈值，年龄会加1，达到阀值会被会被复制到Old区中空的内存分段。
   *   如果Survivor空间不够，Eden空间的部分数据会直接晋升到老年代空间。

5. 第五阶段，处理引用

   处理Soft，Weak，Phantom，Final，JNI Weak 等引用。最终Eden空间的数据为空，GC停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。

##### G1回收过程二：并发标记过程

1.  初始标记阶段：标记从根节点直接可达的对象。这个阶段是STW的，并且会触发一次年轻代GC。正是由于该阶段时STW的，所以我们只扫描根节点可达的对象，以节省时间。
2.  根区域扫描（Root Region Scanning）：G1 GC扫描Survivor区直接可达的老年代区域对象，并标记被引用的对象。这一过程必须在Young GC之前完成，因为Young GC会使用复制算法对Survivor区进行GC。
3.  并发标记（Concurrent Marking）：
    1.  在整个堆中进行并发标记（和应用程序并发执行），此过程可能被Young GC中断。
    2.  **在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那这个区域会被立即回收。**
    3.  同时，并发标记过程中，会计算每个区域的对象活性（区域中存活对象的比例）。
4.  再次标记（Remark）：由于应用程序持续进行，需要修正上一次的标记结果。是STW的。G1中采用了比CMS更快的原始快照算法：Snapshot-At-The-Beginning（SATB）。
5.  独占清理（cleanup，STW）：计算各个区域的存活对象和GC回收比例，并进行排序，识别可以混合回收的区域。为下阶段做铺垫。是STW的。这个阶段并不会实际上去做垃圾的收集
6.  并发清理阶段：识别并清理完全空闲的区域。

##### G1回收过程三：混合回收过程

当越来越多的对象晋升到老年代Old Region时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即Mixed GC，该算法并不是一个Old GC，除了回收整个Young Region，还会回收一部分的Old Region。这里需要注意：是一部分老年代，而不是全部老年代。可以选择哪些Old Region进行收集，从而可以对垃圾回收的耗时时间进行控制。也要注意的是Mixed GC并不是Full GC。



## 类文件结构详解

### 魔数（Magic Number）

```
u4             magic; //Class 文件的标志
```

每个 Class 文件的头 4 个字节称为魔数（Magic Number）,它的唯一作用是**确定这个文件是否为一个能被虚拟机接收的 Class 文件**。

### Class 文件版本号（Minor&Major Version）

```
 u2             minor_version;//Class 的小版本号
 u2             major_version;//Class 的大版本号
```

紧接着魔数的四个字节存储的是 Class 文件的版本号：第 5 和第 6 位是**次版本号**，第 7 和第 8 位是**主版本号**。

### 常量池（Constant Pool）

```
 u2             constant_pool_count;//常量池的数量
 cp_info        constant_pool[constant_pool_count-1];//常量池
```

紧接着主次版本号之后的是常量池，常量池的数量是 `constant_pool_count-1`（**常量池计数器是从 1 开始计数的，将第 0 项常量空出来是有特殊考虑的，索引值为 0 代表“不引用任何一个常量池项”**）。

常量池主要存放两大常量：字面量和符号引用。字面量比较接近于 Java 语言层面的的常量概念，如文本字符串、声明为 final 的常量值等。而符号引用则属于编译原理方面的概念。包括下面三类常量：

- 类和接口的全限定名
- 字段的名称和描述符
- 方法的名称和描述符

常量池中每一项常量都是一个表，这 14 种表有一个共同的特点：**开始的第一位是一个 u1 类型的标志位 -tag 来标识常量的类型，代表当前这个常量属于哪种常量类型．**

### 访问标志(Access Flags)

在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口，是否为 `public` 或者 `abstract` 类型，如果是类的话是否声明为 `final` 等等。

### 当前类（This Class）、父类（Super Class）、接口（Interfaces）索引集合

```java
u2             this_class;//当前类
u2             super_class;//父类
u2             interfaces_count;//接口
u2             interfaces[interfaces_count];//一个类可以实现多个接口
```

类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，由于 Java 语言的单继承，所以父类索引只有一个，除了 `java.lang.Object` 之外，所有的 java 类都有父类，因此除了 `java.lang.Object` 外，所有 Java 类的父类索引都不为 0。

接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按 `implements` (如果这个类本身是接口的话则是`extends`) 后的接口顺序从左到右排列在接口索引集合中。

### 字段表集合（Fields）

```java
 u2             fields_count;//Class 文件的字段的个数
 field_info     fields[fields_count];//一个类会可以有个字段
```

字段表（field info）用于描述接口或类中声明的变量。字段包括类级变量以及实例变量，但不包括在方法内部声明的局部变量。

- **access_flags:** 字段的作用域（`public` ,`private`,`protected`修饰符），是实例变量还是类变量（`static`修饰符）,可否被序列化（transient 修饰符）,可变性（final）,可见性（volatile 修饰符，是否强制从主内存读写）。
- **name_index:** 对常量池的引用，表示的字段的名称；
- **descriptor_index:** 对常量池的引用，表示字段和方法的描述符；
- **attributes_count:** 一个字段还会拥有一些额外的属性，attributes_count 存放属性的个数；
- **attributes[attributes_count]:** 存放具体属性具体内容。

### 方法表集合（Methods）

```
u2             methods_count;//Class 文件的方法的数量
method_info    methods[methods_count];//一个类可以有个多个方法
```

methods_count 表示方法的数量，而 method_info 表示方法表。

Class 文件存储格式中对方法的描述与对字段的描述几乎采用了完全一致的方式。方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项。

### 属性表集合（Attributes）

```
u2             attributes_count;//此类的属性表中的属性数
attribute_info attributes[attributes_count];//属性表集合
```

在 Class 文件，字段表，方法表中都可以携带自己的属性表集合，以用于描述某些场景专有的信息。与 Class 文件中其它的数据项目要求的顺序、长度和内容不同，属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写 入自己定义的属性信息，Java 虚拟机运行时会忽略掉它不认识的属性。

## 类加载过程详解

Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？

系统加载 Class 类型的文件主要三步：**加载->连接->初始化**。连接过程又可分为三步：**验证->准备->解析**。

[Loading, Linking, and Initializing](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-5.html)

### 加载

类加载过程的第一步，主要完成下面 3 件事情：

1. 通过全类名获取定义此类的二进制字节流
2. 将字节流所代表的静态存储结构转换为方法区的运行时数据结构
3. 在内存中生成一个代表该类的 `Class` 对象，作为方法区这些数据的访问入口

虚拟机规范上面这 3 点并不具体，因此是非常灵活的。比如："通过全类名获取定义此类的二进制字节流" 并没有指明具体从哪里获取、怎样获取。比如：比较常见的就是从 `ZIP` 包中读取（日后出现的 `JAR`、`EAR`、`WAR` 格式的基础）、其他文件生成（典型应用就是 `JSP`）等等。

### 连接

#### 验证

![验证阶段示意图](D:\系统学习笔记\a-面试背诵\img\验证阶段.png)

#### 准备

**准备阶段是正式为类变量分配内存并设置类变量初始值的阶段**，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意：

1. 这时候进行内存分配的仅包括类变量（ Class Variables ，即静态变量，被 `static` 关键字修饰的变量，只与类相关，因此被称为类变量），而不包括实例变量。实例变量会在对象实例化时随着对象一块分配在 Java 堆中。
2. 这里所设置的初始值"通常情况"下是数据类型默认的零值（如 0、0L、null、false 等）。特殊情况：比如给 value 变量加上了 final 关键字`public static final int value=111` ，那么准备阶段 value 的值就被赋值为 111。

#### 解析

解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符 7 类符号引用进行

### 初始化

初始化阶段是执行初始化方法 `<clinit> ()`方法的过程，是类加载的最后一步，这一步 JVM 才开始真正执行类中定义的 Java 程序代码(字节码)。

## 类加载器详解

所有的类都由类加载器加载，加载的作用就是将 `.class`文件加载到内存。（数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。）

JVM 提供了 

### 3 种类加载器



1. 启动类加载器(Bootstrap ClassLoader)

   负责加载`%JAVA_HOME%/lib`目录中的jar 包和类，或通过`-Xbootclasspath` 参数指定路径中的，且被虚拟机认可（按文件名识别，如 rt.jar）的类。

2. 扩展类加载器(Extension ClassLoader)

   负责加载 ``%JAVA_HOME%E/lib/ext` 目录中的，或通过 `java.ext.dirs` 系统变量指定路径中的jar 包和类。

3. 应用程序类加载器(Application ClassLoader)：

   负责加载用户路径（classpath）上的jar 包和类。

###  双亲委派模型

每一个类都有一个对应它的类加载器。即在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派给父类加载器的 `loadClass()` 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 `BootstrapClassLoader` 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为 null 时，会使用启动类加载器 `BootstrapClassLoader` 作为父类加载器。

双亲委派模型的好处：

双亲委派模型保证了 Java 程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 `java.lang.Object` 类的话，那么程序运行的时候，系统就会出现多个不同的 `Object` 类。

如果我们不想用双亲委派模型怎么办？

自定义加载器的话，需要继承 `ClassLoader` 。如果我们不想打破双亲委派模型，就重写 `ClassLoader` 类中的 `findClass()` 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写 `loadClass()` 方法





**双亲委派机制过程？**

1. 当AppClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtClassLoader去完成。
2. 当ExtClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。
3. 如果BootStrapClassLoader加载失败(例如在$JAVA_HOME/jre/lib里未查找到该class)，会使用ExtClassLoader来尝试加载；
4. 若ExtClassLoader也加载失败，则会使用AppClassLoader来加载，如果AppClassLoader也加载失败，则会报出异常ClassNotFoundException。

https://www.pdai.tech/md/java/jvm/java-jvm-classload.html





# Java集合

## 集合种类

Java 集合， 也叫作容器，主要是由两大接口派生而来：一个是 `Collection`接口，主要用于存放单一元素；另一个是 `Map` 接口，主要用于存放键值对。对于`Collection` 接口，下面又有三个主要的子接口：`List`、`Set` 和 `Queue`。

![img](D:\系统学习笔记\a-面试背诵\img\java-collection-hierarchy.1727461b.png)

## :exclamation:HashMap

https://mp.weixin.qq.com/s/P8ZnwkoxZZaL6vKkKkNFJQ HashMap追问

以jdk8为例，简要流程如下：

1. 首先根据key的值计算hash值，找到该元素在数组中存储的下标
2. 如果数组是空的，则调用resize进行初始化；
3. 如果没有哈希冲突直接放在对应的数组下标里
4. 如果冲突了，且key已经存在，就覆盖掉value
5. 如果冲突后是链表结构，就判断该链表是否大于8，如果大于8并且数组容量小于64，就进行扩容；如果链表节点数量大于8并且数组的容量大于64，则将这个结构转换成红黑树；否则，链表插入键值对，若key存在，就覆盖掉value
6. 如果冲突后，发现该节点是红黑树，就将这个节点挂在树上

扩容之后原位置的节点只有两种调整

- 保持原位置不动（新bit位为0时）
- 散列原索引+扩容大小的位置去（新bit位为1时）

这样的扩容方式不仅节省了重新计算hash的时间，而且保证了当前桶中的元素总数一定小于等于原来桶中的元素数量，避免了更严重的hash冲突，均匀的把之前冲突的节点分散到新的桶中去



简介：

HashMap 主要用来存放键值对，它基于哈希表的 Map 接口实现，是常用的 Java 集合之一，是非线程安全的。

`HashMap` 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个

JDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。 JDK1.8 以后的 `HashMap` 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。

`HashMap` 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。并且， `HashMap` 总是使用 2 的幂作为哈希表的大小。

底层实现：

JDK1.8 之前 ：

`HashMap` 底层是 **数组和链表** 结合在一起使用也就是 **链表散列**。**HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。**

**所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。**

JDK1.8 之后：

相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。

## :exclamation:HashMap的put和get

JDK1.8  put

1. 判断table数组是不是为空或者长度为0，如果是的话，就进行resize()扩容
2. 如果数组不为空的话， 就根据key的hashcode计算这个元素要插入的下标i。
3. 如果table[i]==null，直接新建节点添加。
4. 如果table[i]不为null，就需要判断这个table[i]的首节点的key是不是相等，如果相同直接覆盖value
5. 如果不相等，就判断这个结点table[i] 是不是红黑树，如果是红黑树，则直接在树中插入键值对
6. 如果不是红黑树，就说明它是链表结构，那就遍历table[i]，如果链表长度不大于8的话，就插入节点，大于8的话就把链表转换为红黑树
7. 插入成功后，判断实际存在的键值对数量size是否超过了最大容量threshold，如果超过，进行扩容（resize）。

> **JDK1.7 put 方法的代码**
>
> **对于 put 方法的分析如下：**
>
> - ① 如果定位到的数组位置没有元素 就直接插入。
> - ② 如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的 key 比较，如果 key 相同就直接覆盖，不同就采用头插法插入元素。

get 

1. 首先也是计算key的hash值，再根据hash值去计算table数组的下标i
2. 判断首节点是否为空, 为空则直接返回空;
3. 再判断首节点的key 是否和目标值相同, 相同则直接返回(首节点不用区分链表还是红黑树);
4. 如果没命中的话，接下来再判断首节点的下一个节点。看是树结构还是链表结构，根据结构的不同，进入不同的逻辑里面去查找。



## :exclamation:ConcurrentHashMap

JDK1.7：

首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。

**JDK1.7版本的`ConcurrentHashMap` 是由 `Segment` 数组和 `HashEntry` 数组组成。**

Segment 实现了 `ReentrantLock`,所以 `Segment` 是一种可重入锁，扮演锁的角色。`HashEntry` 用于存储键值对数据。

一个 `ConcurrentHashMap` 里包含一个 `Segment` 数组。一个 `Segment` 包含一个 `HashEntry` 数组，每个 `HashEntry` 是一个链表结构的元素，每个 `Segment` 守护着一个 `HashEntry` 数组里的元素，当对 `HashEntry` 数组的数据进行修改时，必须首先获得对应的 `Segment` 的锁。

JDK1.8：

JDK1.8的`ConcurrentHashMap` 取消了 `Segment` 分段锁，采用 CAS 和 `synchronized` 来保证并发安全。数据结构跟 HashMap1.8 的结构类似，数组+链表/红黑二叉树。Java 8 在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为 O(N)）转换为红黑树（寻址时间复杂度为 O(log(N))）

`synchronized` 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发

---

JDK1.7 的 `ConcurrentHashMap` 底层采用 **分段的数组+链表** 实现，JDK1.8 采用的数据结构跟 `HashMap1.8` 的结构一样，数组+链表/红黑二叉树。

**实现线程安全的方式（重要）：** ① **在 JDK1.7 的时候，`ConcurrentHashMap`（分段锁）** 对整个桶数组进行了分割分段(`Segment`)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 **到了 JDK1.8 的时候已经摒弃了 `Segment` 的概念，而是直接用 `Node` 数组+链表+红黑树的数据结构来实现，并发控制使用 `synchronized` 和 CAS 来操作。（JDK1.6 以后 对 `synchronized` 锁做了很多优化）** 整个看起来就像是优化过且线程安全的 `HashMap`，虽然在 JDK1.8 中还能看到 `Segment` 的数据结构，但是已经简化了属性，只是为了兼容旧版本；



## :exclamation:ConcurrentHashMap的put和get

put

1. 判断key和value是不是为空，为空则抛出异常
2. 如果当前table数组还未初始化，先将table数组进行初始化操作； 
3. 通过key的hashcode计算table数组下标索引i
4. 如果table数组中索引为i的位置的元素为null,则直接使用 CAS 将值插入即可
5. 如果这个位置不为空的话，说明发生了hash碰撞，首先判断数组是不是处于扩容状态，如果处于扩容状态的话，这个线程就要去协助扩容
6. 如果这个节点的类型是链表节点，那就依次遍历确定这个新加入的值所在位置。
7. 如果是树节点的话，那就调用红黑树的插入方法进行插入新的节点
8. 插入完节点之后再次检查链表长度，如果长度大于8，就把这个链表转换成红黑树；
9. 对当前容量大小进行检查，如果超过了临界值就需要扩容。

get：

1. 通过key的hashcode计算table数组下标索引i
2. 看table[i]是否为查找的节点，若是则直接返回；
3. 若不是，则继续再看当前是不是树节点？通过看节点的hash值是否为小于0，如果小于0则为树节点。如果是树节点在红黑树中查找节点；
4. 如果不是树节点，那就只剩下为链表的形式的一种可能性了，就向后遍历查找节点，若查找到则返回节点的value即可，若没有找到就返回null。

> 这个 get 请求，我们需要 cas 来保证变量的原子性。如果 tab[i] 正被锁住，那么 CAS 就会失败，失败之后就会不断的重试。这也保证了在高并发情况下不会出错。





---





## HashMap 和 Hashtable 的区别

1. **线程是否安全：** `HashMap` 是非线程安全的，`Hashtable` 是线程安全的,因为 `Hashtable` 内部的方法基本都经过`synchronized` 修饰。（如果你要保证线程安全的话就使用 `ConcurrentHashMap` 吧！）；
2. **效率：** 因为线程安全的问题，`HashMap` 要比 `Hashtable` 效率高一点。另外，`Hashtable` 基本被淘汰，不要在代码中使用它；
3. **对 Null key 和 Null value 的支持：** `HashMap` 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；Hashtable 不允许有 null 键和 null 值，否则会抛出 `NullPointerException`。
4. **初始容量大小和每次扩充容量大小的不同 ：** ① 创建时如果不指定容量初始值，`Hashtable` 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。**`HashMap` 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 `HashMap` 会将其扩充为 2 的幂次方大小（`HashMap` 中的`tableSizeFor()`方法保证，下面给出了源代码）。也就是说 `HashMap` 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方**。
5. **底层数据结构：** JDK1.8 以后的 `HashMap` 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。**Hashtable 没有这样的机制。

## HashMap 和 HashSet 区别

如果你看过 `HashSet` 源码的话就应该知道：`HashSet` 底层就是基于 `HashMap` 实现的。（`HashSet` 的源码非常非常少，因为除了 `clone()`、`writeObject()`、`readObject()`是 `HashSet` 自己不得不实现之外，其他方法都是直接调用 `HashMap` 中的方法

## HashMap 和 TreeMap 区别

`TreeMap` 和`HashMap` 都继承自`AbstractMap` ，但是需要注意的是`TreeMap`它还实现了`NavigableMap`接口和`SortedMap` 接口。

## HashMap 中的 key 我们可以使用任何类作为 key 吗？

平时可能大家使用的最多的就是使用 String 作为 HashMap 的 key，但是现在我们想使用某个自定 义类作为 HashMap 的 key，那就需要注意以下几点： 

如果类重写了 equals 方法，它也应该重写 hashCode 方法。 

类的所有实例需要遵循与 equals 和 hashCode 相关的规则。 

如果一个类没有使用 equals，你不应该在 hashCode 中使用它。 

咱们自定义 key 类的最佳实践是使之为不可变的，这样，**hashCode 值可以被缓存起来（？）**，拥有 更好的性能。不可变的类也可以确保 hashCode 和 equals 在未来不会改变，这样就会解决与 可变相关的问题了。

## HashMap 的长度为什么是 2 的幂次方

为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ `(n - 1) & hash`”。（n 代表数组长度）。这也就解释了 HashMap 的长度为什么是 2 的幂次方。

## HashMap 多线程操作导致死循环问题

主要原因在于并发下的 Rehash 会造成元素之间会形成一个循环链表。不过，jdk 1.8 后解决了这个问题，但是还是不建议在多线程下使用 HashMap,因为多线程下使用 HashMap 还是会存在其他问题比如数据丢失。并发环境下推荐使用 ConcurrentHashMap 。

## HashMap遍历方式

    for (Map.Entry<Integer, String> entry : map.entrySet()) {
      System.out.println(entry.getKey());
      System.out.println(entry.getValue());
    }
## HashSet 如何检查重复

当你把对象加入`HashSet`时，`HashSet` 会先计算对象的`hashcode`值来判断对象加入的位置，同时也会与其他加入的对象的 `hashcode` 值作比较，如果没有相符的 `hashcode`，`HashSet` 会假设对象没有重复出现。但是如果发现有相同 `hashcode` 值的对象，这时会调用`equals()`方法来检查 `hashcode` 相等的对象是否真的相同。如果两者相同，`HashSet` 就不会让加入操作成功。

在openjdk8中，`HashSet`的`add()`方法只是简单的调用了`HashMap`的`put()`方法，并且判断了一下返回值以确保是否有重复元素。

## ConcurrentHashMap 和 Hashtable 的区别

`ConcurrentHashMap` 和 `Hashtable` 的区别主要体现在实现线程安全的方式上不同。

- **底层数据结构：** JDK1.7 的 `ConcurrentHashMap` 底层采用 **分段的数组+链表** 实现，JDK1.8 采用的数据结构跟 `HashMap1.8` 的结构一样，数组+链表/红黑二叉树。`Hashtable` 和 JDK1.8 之前的 `HashMap` 的底层数据结构类似都是采用 **数组+链表** 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的；
- **实现线程安全的方式（重要）：** ① **在 JDK1.7 的时候，`ConcurrentHashMap`（分段锁）** 对整个桶数组进行了分割分段(`Segment`)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 **到了 JDK1.8 的时候已经摒弃了 `Segment` 的概念，而是直接用 `Node` 数组+链表+红黑树的数据结构来实现，并发控制使用 `synchronized` 和 CAS 来操作。（JDK1.6 以后 对 `synchronized` 锁做了很多优化）** 整个看起来就像是优化过且线程安全的 `HashMap`，虽然在 JDK1.8 中还能看到 `Segment` 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② **`Hashtable`(同一把锁)** :使用 `synchronized` 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。



## List, Set, Queue, Map 四者的区别？

- `List`(对付顺序的好帮手): 存储的元素是有序的、可重复的。
- `Set`(注重独一无二的性质): 存储的元素是无序的、不可重复的。
- `Queue`(实现排队功能的叫号机): 按特定的排队规则来确定先后顺序，存储的元素是有序的、可重复的。
- `Map`(用 key 来搜索的专家): 使用键值对（key-value）存储，类似于数学上的函数 y=f(x)，"x" 代表 key，"y" 代表 value，key 是无序的、不可重复的，value 是无序的、可重复的，每个键最多映射到一个值。

### List

- `Arraylist`： `Object[]` 数组
- `Vector`：`Object[]` 数组
- `LinkedList`： 双向链表(JDK1.6 之前为循环链表，JDK1.7 取消了循环

Arraylist 和 Vector 的区别?

- `ArrayList` 是 `List` 的主要实现类，底层使用 `Object[ ]`存储，适用于频繁的查找工作，线程不安全 ；
- `Vector` 是 `List` 的古老实现类，底层使用`Object[ ]` 存储，线程安全的。(Vector 的所有方法加上了synchronized )





### Set

- `HashSet`(无序，唯一): 基于 `HashMap` 实现的，底层采用 `HashMap` 来保存元素
- `LinkedHashSet`: `LinkedHashSet` 是 `HashSet` 的子类，并且其内部是通过 `LinkedHashMap` 来实现的。有点类似于我们之前说的 `LinkedHashMap` 其内部是基于 `HashMap` 实现一样，不过还是有一点点区别的
- `TreeSet`(有序，唯一): 红黑树(自平衡的排序二叉树

### Queue

- `PriorityQueue`: `Object[]` 数组来实现二叉堆
- `ArrayQueue`: `Object[]` 数组 + 双指针

### Map

- `HashMap`： JDK1.8 之前 `HashMap` 由数组+链表组成的，数组是 `HashMap` 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间
- `LinkedHashMap`： `LinkedHashMap` 继承自 `HashMap`，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，`LinkedHashMap` 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。
- `Hashtable`： 数组+链表组成的，数组是 `Hashtable` 的主体，链表则是主要为了解决哈希冲突而存在的
- `TreeMap`： 红黑树（自平衡的排序二叉树）



## Arraylist 与 LinkedList 区别?

1. **是否保证线程安全：** `ArrayList` 和 `LinkedList` 都是不同步的，也就是不保证线程安全；
2. **底层数据结构：** `Arraylist` 底层使用的是 **`Object` 数组**；`LinkedList` 底层使用的是 **双向链表** 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！）
3. 插入和删除是否受元素位置的影响：
   - `ArrayList` 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行`add(E e)`方法的时候， `ArrayList` 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（`add(int index, E element)`）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。
   - `LinkedList` 采用链表存储，所以，如果是在头尾插入或者删除元素不受元素位置的影响（`add(E e)`、`addFirst(E e)`、`addLast(E e)`、`removeFirst()` 、 `removeLast()`），近似 O(1)，如果是要在指定位置 `i` 插入和删除元素的话（`add(int index, E element)`，`remove(Object o)`） 时间复杂度近似为 O(n) ，因为需要先移动到指定位置再插入。
4. **是否支持快速随机访问：** `LinkedList` 不支持高效的随机元素访问，而 `ArrayList` 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于`get(int index)`方法)。
5. **内存空间占用：** ArrayList 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）



ArrayList 实现了 Cloneable 接口，标识着它可以被复制。注意：ArrayList 里面的 clone() 复制其实 是浅复制。



## ArrayList 与 Vector 区别？

- Vector是线程安全的，ArrayList不是线程安全的。其中，Vector在关键性的方法前面都加了synchronized关键字，来保证线程的安全性。如果有多个线程会访问到集合，那最好是使用 Vector，因为不需要我们自己再去考虑和编写线程安全的代码。
- ArrayList在底层数组不够用时在原来的基础上扩展0.5倍(人话：扩展1.5倍)，Vector是扩展1倍，这样ArrayList就有利于节约内存空间。

## :exclamation:ArrayList 扩容机制

JDK8里面的ArrayList 有三个构造函数，**以无参构造方法创建 `ArrayList` 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。**

> JDK6 new 无参构造的 `ArrayList` 对象时，直接创建了长度是 10 的 `Object[]` 数组 elementData 。

在这之后，如果数组的大小超过了数组容量之后，就需要扩容。每次扩容之后容量变为原来的1.5倍。

>  底层其实是调用了`Arrays.copyOf`方法来进行扩充数组容量的。

---

ArrayList扩容的本质就是计算出新的扩容数组的size后实例化，并将原有数组内容复制到新数组中去。**默认情况下，新的容量会是原容量的1.5倍**。



## Array 和 ArrayList 有什么区别？什么时候该应 Array 而不是 ArrayList 呢？

- Array 可以包含基本类型和对象类型，ArrayList 只能包含对象类型。
- Array 大小是固定的，ArrayList 的大小是动态变化的。
- ArrayList 提供了更多的方法和特性，比如：addAll()，removeAll()，iterator() 等等。

## 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同

- `HashSet`、`LinkedHashSet` 和 `TreeSet` 都是 `Set` 接口的实现类，都能保证元素唯一，并且都不是线程安全的。
- `HashSet`、`LinkedHashSet` 和 `TreeSet` 的主要区别在于底层数据结构不同。**`HashSet` 的底层数据结构是哈希表（基于 `HashMap` 实现）。`LinkedHashSet` 的底层数据结构是链表和哈希表，元素的插入和取出顺序满足 FIFO。`TreeSet` 底层数据结构是红黑树，元素是有序的，排序的方式有自然排序和定制排序**。
- 底层数据结构不同又导致这三者的应用场景不同。`HashSet` 用于不需要保证元素插入和取出顺序的场景，`LinkedHashSet` 用于保证元素的插入和取出顺序满足 FIFO 的场景，`TreeSet` 用于支持对元素自定义排序规则的场景。

## Queue 与 Deque 的区别

`Queue` 是单端队列，只能从一端插入元素，另一端删除元素，实现上一般遵循 **先进先出（FIFO）** 规则。

`Queue` 扩展了 `Collection` 的接口，根据 **因为容量问题而导致操作失败后处理方式的不同** 可以分为两类方法: 一种在操作失败后会抛出异常，另一种则会返回特殊值。

| `Queue` 接口 | 抛出异常  | 返回特殊值 |
| ------------ | --------- | ---------- |
| 插入队尾     | add(E e)  | offer(E e) |
| 删除队首     | remove()  | poll()     |
| 查询队首元素 | element() | peek()     |

`Deque` 是双端队列，在队列的两端均可以插入或删除元素。

`Deque` 扩展了 `Queue` 的接口, 增加了在队首和队尾进行插入和删除的方法，同样根据失败后处理方式的不同分为两类：

| `Deque` 接口 | 抛出异常      | 返回特殊值      |
| ------------ | ------------- | --------------- |
| 插入队首     | addFirst(E e) | offerFirst(E e) |
| 插入队尾     | addLast(E e)  | offerLast(E e)  |
| 删除队首     | removeFirst() | pollFirst()     |
| 删除队尾     | removeLast()  | pollLast()      |
| 查询队首元素 | getFirst()    | peekFirst()     |
| 查询队尾元素 | getLast()     | peekLast()      |

事实上，`Deque` 还提供有 `push()` 和 `pop()` 等其他方法，可用于模拟栈

## ArrayDeque 与 LinkedList 的区别

`ArrayDeque` 和 `LinkedList` 都实现了 `Deque` 接口，两者都具有队列的功能，但两者有什么区别呢？

- `ArrayDeque` 是基于可变长的数组和双指针来实现，而 `LinkedList` 则通过链表来实现。
- `ArrayDeque` 不支持存储 `NULL` 数据，但 `LinkedList` 支持。
- `ArrayDeque` 是在 JDK1.6 才被引入的，而`LinkedList` 早在 JDK1.2 时就已经存在。
- `ArrayDeque` 插入时可能存在扩容过程, 不过均摊后的插入操作依然为 O(1)。虽然 `LinkedList` 不需要扩容，但是每次插入数据时均需要申请新的堆空间，均摊性能相比更慢。

从性能的角度上，选用 `ArrayDeque` 来实现队列要比 `LinkedList` 更好。此外，`ArrayDeque` 也可以用于实现栈。

## PriorityQueue

`PriorityQueue` 是在 JDK1.5 中被引入的, 其与 `Queue` 的区别在于元素出队顺序是与优先级相关的，即总是优先级最高的元素先出队。

这里列举其相关的一些要点：

- `PriorityQueue` 利用了二叉堆的数据结构来实现的，底层使用可变长的数组来存储数据
- `PriorityQueue` 通过堆元素的上浮和下沉，实现了在 O(logn) 的时间复杂度内插入元素和删除堆顶元素。
- `PriorityQueue` 是非线程安全的，且不支持存储 `NULL` 和 `non-comparable` 的对象。
- `PriorityQueue` 默认是小顶堆，但可以接收一个 `Comparator` 作为构造参数，从而来自定义元素优先级的先后。

1. 





## 如何选用集合?

主要根据集合的特点来选用，比如我们需要根据键值获取到元素值时就选用 `Map` 接口下的集合，需要排序时选择 `TreeMap`,不需要排序时就选择 `HashMap`,需要保证线程安全就选用 `ConcurrentHashMap`。

当我们只需要存放元素值时，就选择实现`Collection` 接口的集合，需要保证元素唯一时选择实现 `Set` 接口的集合比如 `TreeSet` 或 `HashSet`，不需要就选择实现 `List` 接口的比如 `ArrayList` 或 `LinkedList`，然后再根据实现这些接口的集合的特点来选用。

## 为什么要使用集合？

当我们需要保存一组类型相同的数据的时候，我们应该是用一个容器来保存，这个容器就是数组，但是，使用数组存储对象具有一定的弊端， 因为我们在实际开发中，存储的数据的类型是多种多样的，于是，就出现了“集合”，集合同样也是用来存储多个数据的。

数组的缺点是一旦声明之后，长度就不可变了；同时，声明数组时的数据类型也决定了该数组存储的数据的类型；而且，数组存储的数据是有序的、可重复的，特点单一。 但是集合提高了数据存储的灵活性，Java 集合不仅可以用来存储不同类型不同数量的对象，还可以保存具有映射关系的数据。



## 什么是 fail-fast

fail-fast 机制是 Java 集合（Collection）中的一种错误机制。

当多个线程对同一个集合的内容进行 操作时，就可能会产生 fail-fast 事件。

 例如：当某一个线程 A 通过 iterator 去遍历某集合的过程中，若该集合的内容被其他线程所改变 了，那么线程 A 访问集合时，就会抛出 ConcurrentModificationException 异常，产生 fail-fast 事 件。这里的操作主要是指 add、remove 和 clear，对集合元素个数进行修改。 

解决办法：建议使用“java.util.concurrent 包下的类”去取代“java.util 包下的类”。 可以这么理解：在遍历之前，把 modCount 记下来 expectModCount，后面 expectModCount 去 和 modCount 进行比较，如果不相等了，证明已并发了，被修改了，于是抛出 ConcurrentModificationException 异常。

# 多线程并发

## :exclamation:线程池

**使用线程池的好处**：

- **降低资源消耗**。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。
- **提高响应速度**。当任务到达时，任务可以不需要等到线程创建就能立即执行。
- **提高线程的可管理性**。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。

### 如何创建线程池

1. **通过 Executor 框架的工具类 Executors 来实现**
   + **newFixedThreadPool**：该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。
     + 核心线程数 == 最大线程数（没有救急线程被创建），因此也无需超时时间。
     + 阻塞队列是无界的，可以放任意数量的任务
   + **newSingleThreadExecutor**：方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。**线程数固定为 1，任务数多于 1 时，会放入无界队列排队**。
   +  **newCachedThreadPool**：核心线程数是 0， 最大线程数是 Integer.MAX_VALUE，救急线程的空闲生存时间是 60s，意味着它里面全部都是救急线程（60s 后可以回收），救急线程可以无限创建。队列采用了 SynchronousQueue 实现特点是，它没有容量，没有线程来取是放不进去的（一手交钱、一手交货）

《阿里巴巴 Java 开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险

> Executors 返回线程池对象的弊端如下：
>
> - **FixedThreadPool 和 SingleThreadExecutor** ： 允许请求的队列长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致 OOM。
> - **CachedThreadPool 和 ScheduledThreadPool** ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。



2. **通过构造方法实现**

参数：

- corePoolSize 核心线程数目
- maximumPoolSize 最大线程数目
- keepAliveTime 生存时间 - 针对救急线程
- unit 时间单位 - 针对救急线程
- workQueue 阻塞队列
- threadFactory 线程工厂 - 可以为线程创建时起个好名字
- handler 拒绝策略

1. 线程池中刚开始没有线程，当一个任务提交给线程池后，线程池会创建一个新线程来执行任务。

2. 当线程数达到 corePoolSize 并没有线程空闲，这时再加入任务，新加的任务会被加入workQueue 队列排队，直到有空闲的线程。

3. 如果队列选择了有界队列，那么任务超过了队列大小时，会创建 maximumPoolSize - corePoolSize 数目的线程来救急。
4. 如果线程到达 maximumPoolSize ，并且队列也已经被放满了任务时，仍然有新任务这时会执行拒绝策略。拒绝策略 jdk 提供了 4 种实现

AbortPolicy 让调用者抛出 RejectedExecutionException 异常，这是默认策略

+ AbortPolicy 让调用者抛出 RejectedExecutionException 异常，这是默认策略
+ CallerRunsPolicy 让调用者运行任务
+ DiscardPolicy不处理新任务，直接丢弃掉
+ DiscardOldestPolicy 放弃队列中最早的任务，本任务取而代之（丢弃最早的未处理的任务请求）

### 实现 Runnable 接口和 Callable 接口的区别

**`Runnable` 接口** 不会返回结果或抛出检查异常，但是 **`Callable` 接口** 可以

1. **`execute()`方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否；**
2. **`submit()`方法用于提交需要返回值的任务。线程池会返回一个 `Future` 类型的对象，通过这个 `Future` 对象可以判断任务是否执行成功**，并且可以通过 `Future` 的 `get()`方法来获取返回值，`get()`方法会阻塞当前线程直到任务完成，而使用 `get(long timeout，TimeUnit unit)`方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。

### 执行 execute()方法和 submit()方法的区别

## :exclamation:synchronized 

**`synchronized` 关键字解决的是多个线程之间访问资源的同步性，`synchronized`关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。**

在 Java 早期版本中，`synchronized` 属于 **重量级锁**，效率低下。之后，官方也是对synchronized进行了优化，比如说偏向锁、轻量级锁。

**synchronized 关键字最主要的三种使用方式：**

**1.修饰实例方法:** 作用于当前对象实例加锁，进入同步代码前要获得 **当前对象实例的锁**

**2.修饰静态方法:** 也就是给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 **当前 class对象 的锁**。

**3.修饰代码块** ：指定加锁对象，对给定对象/类加锁。`synchronized(this|object)` 表示进入同步代码库前要获得**给定对象的锁**。

 **synchronized 关键字的底层原理**：

1. synchronized 同步语句块的情况：在字节码层面，**`synchronized` 同步语句块的实现使用的是 `monitorenter` 和 `monitorexit` 指令，其中 `monitorenter` 指令指向同步代码块的开始位置，`monitorexit` 指令则指明同步代码块的结束位置。**当执行 `monitorenter` 指令时，线程试图获取锁也就是获取 **对象监视器 `monitor`** 的持有权。

> 在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由[ObjectMonitoropen in new window](https://github.com/openjdk-mirror/jdk7u-hotspot/blob/50bdefc3afe944ca74c3093e7448d6b889cd20d1/src/share/vm/runtime/objectMonitor.cpp)实现的。每个对象中都内置了一个 `ObjectMonitor`对象。

2. synchronized 修饰方法的的情况：`synchronized` 修饰的方法并没有 `monitorenter` 指令和 `monitorexit` 指令，取得代之的确实是 `ACC_SYNCHRONIZED` 标识，该标识指明了该方法是一个同步方法。JVM 通过该 `ACC_SYNCHRONIZED` 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。如果是实例方法，JVM 会尝试获取实例对象的锁。如果是静态方法，JVM 会尝试获取当前 class 的锁
3. **两者的本质都是对对象监视器 monitor 的获取**



### 锁升级

轻量级锁：假设有两个方法同步块，利用同一个对象加锁

+ 线程会创建一个锁记录（Lock Record）对象，每个线程的栈帧都会包含一个锁记录的结构，内部可以存储锁定对象的Mark Word
+ 让锁记录中 Object reference 指向锁对象，并尝试用 cas 替换 Object 的 Mark Word，将 Mark Word 的值存入锁记录
+ 如果 cas 替换成功，对象头中存储了 锁记录地址和状态 00 ，表示由该线程给对象加锁
+ 如果 cas 失败，有两种情况: (1)如果是其它线程已经持有了该 Object 的轻量级锁，这时表明有竞争，进入锁膨胀过程 (2)如果是自己执行了 synchronized 锁重入，那么再添加一条 Lock Record 作为重入的计数
+ 当退出 synchronized 代码块（解锁时）如果有取值为 null 的锁记录，表示有重入，这时重置锁记录，表示重入计数减一
+ 当退出 synchronized 代码块（解锁时）锁记录的值不为 null，这时使用 cas 将 Mark Word 的值恢复给对象头 (1)成功，则解锁成功 (2)失败，说明轻量级锁进行了锁膨胀或已经升级为重量级锁，进入重量级锁解锁流程

**锁膨胀**

如果在尝试加轻量级锁的过程中，CAS 操作无法成功，这时一种情况就是有其它线程为此对象加上了轻量级锁（有竞争），这时需要进行锁膨胀，将轻量级锁变为重量级锁。

+ 这时 Thread-1 加轻量级锁失败，进入锁膨胀流程
  +  即为 Object 对象申请 Monitor 锁，让 Object 指向重量级锁地址	
  + 然后自己进入 Monitor 的 EntryList BLOCKED
+ 当 Thread-0 退出同步块解锁时，使用 cas 将 Mark Word 的值恢复给对象头，失败。这时会进入重量级解锁流程，即按照 Monitor 地址找到 Monitor 对象，设置 Owner 为 null，唤醒 EntryList 中 BLOCKED 线程

**自旋优化**

重量级锁竞争的时候，还可以使用自旋来进行优化，如果当前线程自旋成功（即这时候持锁线程已经退出了同步块，释放了锁），这时当前线程就可以避免阻塞。

**偏向锁**

轻量级锁在没有竞争时（就自己这个线程），每次重入仍然需要执行 CAS 操作。Java 6 中引入了偏向锁来做进一步优化：只有第一次使用 CAS 将线程 ID 设置到对象的 Mark Word 头，之后发现这个线程 ID 是自己的就表示没有竞争，不用重新 CAS。以后只要不发生竞争，这个对象就归该线程所有

重入的时候检查对象头里面的线程ID是不是自己的。



## :exclamation:volatile

volatile是轻量级的同步机制，volatile保证变量对所有线程的可见性

1. 当对volatile变量进行写操作的时候，JVM会向处理器发送一条LOCK前缀的指令，将该变量所在缓
   存行的数据写回系统内存。

2. 由于缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己的缓存是不是过期了，
当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行置为无效状态，当处
理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存中。

volatile关键字的两个作用：

1. 保证了不同线程对共享变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他

线程来说是立即可见的。

2. 禁止进行指令重排序。

volatile禁止指令重排序的原理是什么：在每个volatile读操作后插入LoadLoad屏障，在读操作后插入LoadStore屏障。在每个volatile写操作的前面插入一个StoreStore屏障，后面插入一个SotreLoad屏障。

volatile 的底层实现原理是内存屏障，Memory Barrier（Memory Fence） 

- 对 volatile 变量的写指令后会加入写屏障
- 对 volatile 变量的读指令前会加入读屏障

写屏障（sfence）保证在该屏障之前的，对共享变量的改动，都同步到主存当中

而读屏障（lfence）保证在该屏障之后，对共享变量的读取，加载的是主存中最新数据



## synchronized 关键字和 volatile 关键字的区别

`synchronized` 关键字和 `volatile` 关键字是两个互补的存在，而不是对立的存在！

- **`volatile` 关键字**是线程同步的**轻量级实现**，所以 **`volatile `性能肯定比`synchronized`关键字要好** 。但是 **`volatile` 关键字只能用于变量而 `synchronized` 关键字可以修饰方法以及代码块** 。
- **`volatile` 关键字能保证数据的可见性，但不能保证数据的原子性。`synchronized` 关键字两者都能保证。**
- **`volatile`关键字主要用于解决变量在多个线程之间的可见性，而 `synchronized` 关键字解决的是多个线程之间访问资源的同步性。**

## **ReentrantLock**

ReentrantLock 意思为可重入锁，指的是一个线程能够对一个临界资源重复加锁。



**ReentrantLock** **是如何实现可重入性的**：ReentrantLock 内部自定义了同步器 Sync，在加锁的时候通过 CAS 算法，将线程对象放到一个双向链表中，每次获取锁的时候，检查当前维护的那个线程 ID 和当前请求的线程 ID 是否 一致，如果一致，同步状态加1，表示锁被当前线程获取了多次。



AQS 重要方法与 ReentrantLock 的关联https://javaguide.cn/java/concurrent/reentrantlock.html#_2-1-%E5%8E%9F%E7%90%86%E6%A6%82%E8%A7%88

加锁：

- 通过 ReentrantLock 的加锁方法 Lock 进行加锁操作。
- 会调用到内部类 Sync 的 Lock 方法，由于 Sync#lock 是抽象方法，根据 ReentrantLock 初始化选择的公平锁和非公平锁，执行相关内部类的 Lock 方法，本质上都会执行 AQS 的 Acquire 方法。
- AQS 的 Acquire 方法会执行 `tryAcquire` 方法，但是由于 tryAcquire 需要自定义同步器实现，因此执行了 ReentrantLock 中的 tryAcquire 方法，由于 ReentrantLock 是通过公平锁和非公平锁内部类实现的 tryAcquire 方法，因此会根据锁类型不同，执行不同的 tryAcquire。
- tryAcquire 是获取锁逻辑，获取失败后，会执行框架 AQS 的后续逻辑，跟 ReentrantLock 自定义同步器无关。

解锁：

- 通过 ReentrantLock 的解锁方法 Unlock 进行解锁。
- Unlock 会调用内部类 Sync 的 Release 方法，该方法继承于 AQS。
- Release 中会调用 tryRelease 方法，tryRelease 需要自定义同步器实现，tryRelease 只在 ReentrantLock 中的 Sync 实现，因此可以看出，释放锁的过程，并不区分是否为公平锁。
- 释放成功后，所有处理由 AQS 框架完成，与自定义同步器无关。(之后就是`unparkSuccessor(head)`)





总的来说，一个线程获取锁失败了，被放入等待队列，acquireQueued 会把放入队列中的线程不断去获取锁，直到获取成功或者不再需要获取（中断）。



## ReentrantLock和synchronized区别

1. 使用synchronized关键字实现同步，线程执行完同步代码块会自动释放锁，而**ReentrantLock需要**
**手动释放锁**。
2. synchronized是非公平锁，ReentrantLock可以设置为**公平锁**。
3. ReentrantLock上等待获取锁的线程是可中断的，**线程可以放弃等待锁**。而synchonized会无限期
等待下去。
4. ReentrantLock 可以设置超时获取锁。在指定的截止时间之前获取锁，如果截止时间到了还没有获
取到锁，则返回。
5. ReentrantLock 的 tryLock() 方法可以尝试非阻塞的获取锁，调用该方法后立刻返回，如果能够获
取则返回true，否则返回false。





## :exclamation:Java的锁

**乐观锁与悲观锁**：

1. 乐观锁不会锁定资源，所有的线程都能访问并修改同一个资源，如果没有冲突就修改成功并退出，否则就会继续循环尝试。乐观锁最常见的实现就是CAS。（使用数据版本记录机制实现，添加version字段。或使用时间戳）
2. 每次访问资源都会加锁，执行完同步代码释放锁，synchronized 和 ReentrantLock 属于悲观锁。
3. 悲观锁适合写操作多的场景。乐观锁适合读操作多的场景，不加锁可以提升读操作的性能。

**公平锁与非公平锁**：按照线程访问顺序获取对象锁。synchronized 是非公平锁，ReentrantLock 默认是非公平锁，但是可以设置为公平锁。

**共享式与独占式锁**：共享式与独占式的最主要区别在于：同一时刻独占式只能有一个线程获取同步状态，而共享式在同一时刻可以有多个线程获取同步状态。例如读操作可以有多个线程同时进行，而写操作同一时刻只能有一个线程进行写操作，其他操作都会被阻塞。

## :exclamation:JMM

JMM就是Java内存模型(java memory model)。因为在不同的硬件生产商和不同的操作系统下，内存的访问有一定的差异，所以会造成相同的代码运行在不同的系统上会出现各种问题。所以**java内存模型(JMM)屏蔽掉各种硬件和操作系统的内存访问差异，以实现让java程序在各种平台下都能达到一致的并发效果。**

Java内存模型规定**所有的变量都存储在主内存**中，包括实例变量，静态变量，但是不包括局部变量和方法参数。每个线程都有自己的工作内存，**线程的工作内存保存了该线程用到的变量和主内存的副本拷贝，线程对变量的操作都在工作内存中进行**。**线程不能直接读写主内存中的变量**。

不同的线程之间也无法访问对方工作内存中的变量。线程之间变量值的传递均需要通过主内存来完成。

每个线程的工作内存都是独立的，线程操作数据只能在工作内存中进行，然后刷回到主存。这是 Java 内存模型定义的线程基本工作方式。

原子性、有序性、可见性



## AQS

AQS，AbstractQueuedSynchronizer，抽象队列同步器，定义了一套多线程访问共享资源的同步器框架，许多并发工具的实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch。

AQS使用一个volatile的int类型的成员变量state来表示同步状态，通过CAS修改同步状态的值。当线程调用 lock 方法时 ，如果 state=0，说明没有任何线程占有共享资源的锁，可以获得锁并将 state=1。如果 state=1，则说明有线程目前正在使用共享变量，其他线程必须加入同步队列进行等待。

```
private volatile int state;//共享变量，使用volatile修饰保证线程可见性
```

同步器依赖内部的同步队列（一个FIFO双向队列）（CLH队列）来完成同步状态的管理，当前线程获取同步状态失败时，同步器会将当前线程以及等待状态（独占或共享 ）构造成为一个节点（Node）并将其加入同步队列并进行自旋，当同步状态释放时，会把首节中的后继节点对应的线程唤醒，使其再次尝试获取同步状态。

## CAS

CAS全称 Compare And Swap，比较与交换，是乐观锁的主要实现方式。CAS 在不使用锁的情况下实现多线程之间的变量同步。ReentrantLock 内部的 AQS 和原子类内部都使用了 CAS。

CAS算法涉及到三个操作数：需要读写的内存值 V。进行比较的值 A。要写入的新值 B。

只有当 V 的值等于 A 时，才会使用原子方式用新值B来更新V的值，否则会继续重试直到成功更新值。

CAS 三大问题：
1. ABA问题。CAS需要在操作值的时候检查内存值是否发生变化，没有发生变化才会更新内存值。但
是如果内存值原来是A，后来变成了B，然后又变成了A，那么CAS进行检查时会发现值没有发生变
化，但是实际上是有变化的。ABA问题的解决思路就是在变量前面添加版本号，每次变量更新的时
候都把版本号加一，这样变化过程就从 A－B－A 变成了 1A－2B－3A 。JDK从1.5开始提供了AtomicStampedReference类来解决ABA问题，原子更新带有版本号的引用类型。
2. 循环时间长开销大。CAS操作如果长时间不成功，会导致其一直自旋，给CPU带来非常大的开销。
3. 只能保证一个共享变量的原子操作。对一个共享变量执行操作时，CAS能够保证原子操作，但是对
多个共享变量操作时，CAS是无法保证操作的原子性的。Java从1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，可以把多个变量放在一个对象里来进行CAS操作。

## **CountDownLatch**

CountDownLatch用于某个线程等待其他线程**执行完任务**再执行，与thread.join()功能类似。常见的应用场景是开启多个线程同时执行某个任务，等到所有任务执行完再执行特定操作，如汇总统计结果。

## **CyclicBarrier**

CyclicBarrier(同步屏障)，用于一组线程互相等待到某个状态，然后这组线程再**同时**执行。参数parties指让多少个线程或者任务等待至某个状态；参数barrierAction为当这些线程都达到某个状态时会执行的内容。

## CyclicBarrier和CountDownLatch区别

CyclicBarrier 和 CountDownLatch 都能够实现线程之间的等待。

CountDownLatch用于某个线程等待其他线程**执行完任务**再执行。CyclicBarrier用于一组线程互相等待到某个状态，然后这组线程再**同时**执行。 CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()方法重置，可用于处理更为复杂的业务场景。

## **Semaphore**

Semaphore类似于锁，它用于控制同时访问特定资源的线程数量，控制并发线程数。

## **JUC原子类**

1. **基本类型原子类**： 使用原子的方式更新基本类型
   + AtomicInteger：整型原子类 
   + AtomicLong：长整型原子类  
   + AtomicBoolean ：布尔型原子类
2. **数组类型原子类**：使用原子的方式更新数组里的某个元素
   + AtomicIntegerArray：整形数组原子类
   + AtomicLongArray：长整形数组原子类
   + AtomicReferenceArray ：引用类型数组原子类
3. **引用类型原子类**：
   + AtomicReference：引用类型原子类
   + AtomicStampedReference：带有版本号的引用类型原子类。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。
   + AtomicMarkableReference ：原子更新带有标记的引用类型。该类将 boolean 标记与引用关联起来
4. **对象的属性修改类型**
   + AtomicIntegerFieldUpdater ：原⼦更新整形字段的更新器
   + AtomicLongFieldUpdater ：原⼦更新⻓整形字段的更新器
   + AtomicReferenceFieldUpdater ：原⼦更新引⽤类型字段的更新器

## ThreadLocal

**如果创建了一个`ThreadLocal`变量，那么访问这个变量的每个线程都会有这个变量的本地副本。每一个线程都可以独立地改变自己的副本，不会影响其他线程。**

每个线程都有一个ThreadLocalMap(ThreadLocal内部类)，Map中元素的键为ThreadLocal，而值对应线程的变量副本。ThreadLocal 并不是用来解决共享资源的多线程访问的问题，因为每个线程中的资源只是副本，并不共享。因此ThreadLocal适合作为线程上下文变量，简化线程内传参。

ThreadLocal内存泄漏的原因：`ThreadLocalMap` 中使用的 key 为 `ThreadLocal`，是弱引用,而 value 是强引用。所以，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，`ThreadLocalMap` 中就会出现 key 为 null 的 键值对（Entry）。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap 实现中已经考虑了这种情况，在调用 `set()`、`get()`、`remove()` 方法的时候，会清理掉 key 为 null 的记录。使用完 `ThreadLocal`方法后 最好手动调用`remove()`方法。

ThreadLocal 适用场景：每个线程需要有自己单独的实例，且需要在多个方法中共享实例，即同时满足实例在线程间的隔离与方法间的共享。比如Java web应用中，每个线程有自己单独的 Session 实例，就可以使用ThreadLocal来实现。

## :exclamation:进程和线程

一个比较通俗的说法是，进程是资源分配的基本单位，线程是独立调度的基本单位。一个进程中可以有多个线程，它们共享进程资源。从 JVM的角度来看，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。

同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。由于创建或撤销进程时，系统都要为之分配或回收资源，比如说要分配内存。但是线程切换的时候，开销会比较小。



## 并发编程的三个重要特性

1. **原子性** : 一次操作或者多次操作，要么所有的操作全部都得到执行并且不会受到任何因素的干扰而中断，要么都不执行。`synchronized` 可以保证代码片段的原子性。
2. **可见性** ：当一个线程对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。`volatile` 关键字可以保证共享变量的可见性。
3. **有序性** ：代码在执行的过程中的先后顺序，Java 在编译器以及运行期间的优化，代码的执行顺序未必就是编写代码时候的顺序。`volatile` 关键字可以禁止指令进行重排序优化。

## 使用多线程可能带来什么问题?

内存泄漏、死锁、线程不安全等等

## 线程的生命周期和状态

![Java 线程的状态 ](D:\系统学习笔记\a-面试背诵\img\Java线程的状态.png)

线程创建之后它将处于 **NEW（新建）** 状态，调用 `start()` 方法后开始运行。获得了 CPU 时间片（timeslice）后就处于 **RUNNING（运行）** 状态。当线程执行 `wait()`方法之后，线程进入 **WAITING（等待）** 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 **TIMED_WAITING(超时等待)** 状态相当于在等待状态的基础上增加了超时限制，比如通过 `sleep（long millis）`方法或 `wait（long millis）`方法可以将 Java 线程置于 TIMED_WAITING 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 **BLOCKED（阻塞）** 状态。线程在执行 Runnable 的`run()`方法之后将会进入到 **TERMINATED（终止）** 状态（？）。

![Java 线程状态变迁 ](D:\系统学习笔记\a-面试背诵\img\Java+线程状态变迁.png)

## 什么是上下文切换?

线程在执行过程中会有自己的运行条件和状态（也称上下文），比如上文所说到过的程序计数器，栈信息等。当出现如下情况的时候，线程会从占用 CPU 状态中退出。

- 主动让出 CPU，比如调用了 `sleep()`, `wait()` 等。
- 时间片用完，因为操作系统要防止一个线程或者进程长时间占用CPU导致其他线程或者进程饿死。
- 调用了阻塞类型的系统中断，比如请求 IO，线程被阻塞。
- 被终止或结束运行

这其中前三种都会发生线程切换，线程切换意味着需要保存当前线程的上下文，留待线程下次占用 CPU 的时候恢复现场。并加载下一个将要占用 CPU 的线程上下文。这就是所谓的 **上下文切换**。

## 线程死锁

1. 互斥条件：该资源任意一个时刻只由一个线程占用。
2. 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
3. 不剥夺条件:线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。
4. 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。

## 如何预防和避免线程死锁?

**如何预防死锁？** 破坏死锁的产生的必要条件即可：

1. **破坏请求与保持条件** ：一次性申请所有的资源。
2. **破坏不剥夺条件** ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。
3. **破坏循环等待条件** ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。

## sleep() 方法和 wait() 方法区别和共同点

- 两者最主要的区别在于：**`sleep()` 方法没有释放锁，而 `wait()` 方法释放了锁** 。
- 两者都可以暂停线程的执行。
- `wait()` 通常被用于线程间交互/通信，`sleep() `通常被用于暂停执行。
- `wait()` 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 `notify() `或者 `notifyAll()` 方法。`sleep() `方法执行完成后，线程会自动苏醒。或者可以使用 `wait(long timeout)` 超时后线程会自动苏醒。

## Java线程间通信方式讲解

线程间通信方式有两种：**共享内存**和**消息传递。**

不同进程间线程通信等同于进程间通信，同一进程间可用共享内存实现。

在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信，典型的共享内存通信方式就是通过共享对象进行通信。

在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信，在java中典型的消息传递方式就是wait()和notify()。

http://www.codebaoku.com/it-java/it-java-227064.html



## 不能直接调用 run() 方法

new 一个 Thread，线程进入了新建状态。调用 `start()`方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 `start()` 会执行线程的相应准备工作，然后自动执行 `run()` 方法的内容，这是真正的多线程工作。 但是，直接执行 `run()` 方法，会把 `run()` 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。



# 数据库

## 索引优化

05.深入浅出索引（下）.pdf

覆盖索引

如果执行的语句是select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。

**由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。**



## 索引

**索引是一种用于快速查询和检索数据的数据结构。**使用索引可以大大加快 数据的检索速度（大大减少检索的数据量）

https://javaguide.cn/database/mysql/mysql-index.html#hash%E8%A1%A8-b-%E6%A0%91

既然哈希表这么快，**为什么MySQL 没有使用其作为索引的数据结构呢？**

**1.Hash 冲突问题** ：我们上面也提到过Hash 冲突了，不过对于数据库来说这还不算最大的缺点。

**2.Hash 索引不支持顺序和范围查询(Hash 索引不支持顺序和范围查询是它最大的缺点：** 假如我们要对表中的数据进行排序或者进行范围查询，那 Hash 索引可就不行了。

## :exclamation:B+树

`B+树`是一种数据结构，是一个N叉排序树，通常用于数据库和操作系统的`文件系统`中。

数据库是基于页的形式进行管理索引，如 查询id=4的 直接先比较页 先去页目录中找,再去 数据目录中找

B树的每个节点中，不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B树的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率。在B+树中，所有数据记录节点，都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+树的高度。

**B+树更加适合在区间查询**：由于B+树的数据都存储在叶子结点中，叶子结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以**B+树更加适合在区间查询的情况**，而在数据库中基于范围的查询是非常频繁的，所以通常B+树用于数据库索引。

B+Tree相对于B-Tree有几点不同：

1. 非叶子节点只存储键值信息。
2. 所有叶子节点之间都有一个链指针。
3. 数据记录都存放在叶子节点中。

## 聚簇索引/聚集索引 

InnoDB使用表的主键构造主键索引树，同时叶子节点中存放的即为整张表的记录数据。聚集索引叶子节点的存储是逻辑上连续的，使用双向链表连接，叶子节点按照主键的顺序排序，因此对于主键的排序查找和范围查找速度比较快。

聚集索引的叶子节点就是整张表的行记录。InnoDB 主键使用的是聚簇索引。聚集索引要比非聚集索引查询效率高很多。

对于InnoDB来说，聚集索引一般是表中的主键索引，如果表中没有显示指定主键，则会选择表中的第一个不允许为NULL的唯一索引。如果没有主键也没有合适的唯一索引，那么innodb内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键长度为6个字节，它的值会随着数据的插入自增。

- 聚簇索引： 将数据存储与索引放到了一块，索引结构的叶子节点保存了行数据
- 非聚簇索引：将数据与索引分开存储，索引结构的叶子节点指向了数据对应的位置

`注意`:**在innodb中，在聚簇索引之上创建的索引称之为辅助索引，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引。辅助索引叶子节点存储的不再是行的物理位置，而是主键值，辅助索引访问数据总是需要二次查找**。

使用聚簇索引的优势
- 问题: 每次使用辅助索引检索都要经过两次B+树查找，看上去聚簇索引的效率明显要低于非聚簇索引，这不是多此一举吗？聚簇索引的优势在哪？
- 1.**访问速度加快，同一页已经加载到内存**。由于**行数据和聚簇索引的叶子节点存储在一起**，**同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中（缓存器）**，**再次访问时，会在内存中完成访问，不必访问磁盘**。这样主键和行数据是一起被载入内存的，找到叶子节点就可以立刻将行数据返回了，如果按照主键Id来组织数据，获得数据更快。
- 2.减少索引维护，减少存储空间。辅助索引的叶子节点，存储主键值，而不是数据的存放地址。好处是当行数据放生变化时，索引树的节点也需要分裂变化；或者是我们需要查找的数据，在上一次IO读写的缓存中没有，需要发生一次新的IO操作时，可以避免对辅助索引的维护工作，只需要维护聚簇索引树就好了。另一个好处是，因为**辅助索引存放的是主键值**，减少了辅助索引占用的存储空间大小。



## 索引分类

- 主键索引
- 单列索引（普通索引）
- 唯一索引   
- 复合索引
- 全文索引 

:exclamation:



## 数据库范式

第一范式，确保数据库表字段的**原子性**。

第二范式，首先要满足第一范式，另外包含两部分内容：

- 一是表必须有**一个主键**；
- 二是非主键列必须**完全依赖**于主键，而不能只依赖于主键的一部分。

第三范式，首先要满足第二范式。另外非主键列必须**直接依赖**于主键，不能存在传递依赖。即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。

## 事务的四大特性

1. **原子性**（`Atomicity`） ： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；
2. **一致性**（`Consistency`）： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；
3. **隔离性**（`Isolation`）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；
4. **持久性**（`Durability`）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。

## **数据事务的实现原理**

MySQL InnoDB 引擎使用 **redo log(重做日志)** 保证事务的**持久性**，使用 **undo log(回滚日志)** 来保证事务的**原子性**。MySQL InnoDB 引擎通过 **锁机制**、**MVCC** 等手段来保证事务的隔离性（ 默认支持的隔离级别是 **`REPEATABLE-READ`** ）。保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。

小林：InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？

- 原子性和持久性是通过 redo log （重做日志）来保证的；
- 一致性是通过 undo log（回滚日志） 来保证的；
- 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的；



MySQL事务的ACID，一致性是最终目的。

保证一致性的措施有：

- A原子性：靠undo log来保证（异常或执行失败后进行回滚）。
- D持久性：靠redo log来保证（保证当MySQL宕机或停电后，可以通过redo log最终将数据保存至磁盘中）。
- I隔离性：事务间的读写靠MySQL的锁机制来保证隔离，事务间的写操作靠MVCC机制（快照读、当前读）来保证隔离性。
- C一致性：事务的最终目的，即需要数据库层面保证，又需要应用层面进行保证。

https://www.cnblogs.com/bearbrick0/p/15982567.html

## 并发事务带来哪些问题?

在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。

- **脏读（Dirty read）:** 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。
- **不可重复读（Unrepeatable read）:** 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
- **幻读（Phantom read）:** 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。
- **丢失修改（Lost to modify）:** 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 修改 A=A-1，事务 2 也修改 A=A-1，最终结果 A=19，事务 1 的修改被丢失。（修改丢失、更改丢失）



## 事务隔离级别有哪些?

SQL 标准定义了四个隔离级别：

- **READ-UNCOMMITTED(读未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。
- **READ-COMMITTED(读已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。
- **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。
- **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务`依次逐个执行`，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。



## :exclamation:索引

### 1.什么是索引

- 官方定义: 一种帮助mysql提高查询效率的数据结构
- 索引的优点:
  1、大大加快数据查询速度
- 索引的缺点:
  1、维护索引需要耗费数据库资源
  	2、索引需要占用磁盘空间
  	3、当对表的数据进行增删改的时候，因为要维护索引，速度会受到影响

### 2.索引分类

- a.主键索引
  设定为主键后数据库会自动建立索引，innodb为聚簇索引

- b.单值索引
  即一个索引只包含单个列，一个表可以有多个单列索引 

- c.唯一索引   

  索引列的值必须唯一，但允许有空值(只能有一个空值)

- d.复合索引
  即一个索引包含多个列

- e.Full Text 全文索引 (My5.7版本之前 只能由于MYISAM引擎)
  全文索引类型为FULLTEXT，在定义索引的列上支持值的全文查找，允许在这些索引列中插入重复值和空值。全文索引可以在CHAR、VARCHAR、TEXT类型列上创建。MYSQL只有MYISAM存储引擎支持全文索引

### 3.索引的基本操作

1.主键索引     自动创建

2.单列索引(普通索引|单值索引)  --建表时创建      即一个索引只包含单个列，一个表可以有多个单列索引

3.唯一索引     索引列的值必须唯一，但允许有空值

4.复合索引     即一个索引包含多个列

5.全文索引

### 4.索引原理

基于页的形式进行管理索引

如 查询id=4的 直接先比较页 先去页目录中找,再去 数据目录中找

###  5. 什么情况下无法利用索引呢?

1. 查询语句中使用LIKE关键字
   在查询语句中使用 LIKE 关键字进行查询时，如果匹配字符串的第一个字符为“%”，索引不会被使用。如果“%”不是在第一个位置，索引就会被使用。

2. 查询语句中使用多列索引
   多列索引是在表的多个字段上创建一个索引，只有查询条件中使用了这些字段中的第一个字段，索引才会被使用。

3. 查询语句中使用OR关键字
   查询语句只有OR关键字时，如果OR前后的两个条件的列都是索引，那么查询中将使用索引。如果OR前后有一个条件的列不是索引，那么查询中将不使用索引。

## MyISAM 和 InnoDB 的区别

1. 是否支持行级锁。`MyISAM 只有表级锁(table-level locking)，而 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁`。也就说，MyISAM 一锁就是锁住了整张表，并发性能会比较差。
2. 支持事务。`MyISAM 不提供事务支持`。InnoDB 提供事务支持，具有提交(commit)和回滚(rollback)事务的能力。
3. 是否支持外键。MyISAM 不支持，而 InnoDB 支持
4. 是否支持数据库异常崩溃后的安全恢复。MyISAM 不支持，而 InnoDB 支持。使用 InnoDB 的数据库在异常崩溃后，数据库重新启动的时候会保证数据库恢复到崩溃前的状态。这个恢复的过程依赖于 `redo log` 。

> MySQL InnoDB 引擎使用 **redo log(重做日志)** 保证事务的**持久性**，使用 **undo log(回滚日志)** 来保证事务的**原子性**。

5. 是否支持 MVCC。MyISAM 不支持，而 InnoDB 支持。

## InnoDB 存储引擎的锁

- Record lock：记录锁，单个行记录上的锁
- Gap lock：间隙锁，锁定一个范围，不包括记录本身
- Next-key lock：record+gap 临键锁，锁定一个范围，包含记录本身



# 操作系统

## 进程间通信机制

管道、命名管道、消息队列、共享内存、信号量、信号、Socket

管道： `ps -ef | grep java`  竖线就是⼀个**管道**，它的功能是将前⼀个命令（ ps -ef ）的输出，作为后⼀个命令（ grep java ）的输⼊，从这功能描述，可以看出**管道传输数据是单向的**

管道还有另外⼀个类型是**命名管道**。**管道这种通信⽅式效率低，不适合进程间频繁地交换数据**。当然，它的好处，⾃然就是简单，同时也我们很容易得知管道⾥的数据已经被另⼀个进程读取了。

---

消息队列

**A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。同理，B 进程要给 A 进程发送消息也是如此**。

**消息队列通信过程中，存在⽤户态与内核态之间的数据拷⻉开销**

---

共享内存

**共享内存的机制，就是拿出⼀块虚拟地址空间来，映射到相同的物理内存中**。这样这个进程写⼊的东⻄，另外⼀个进程⻢上就能看到了，都不需要拷⻉来拷⻉去，传来传去，⼤⼤提⾼了进程间通信的速度。

信号量

**信号量其实是⼀个整型的计数器，主要⽤于实现进程间的互斥与同步，⽽不是⽤于缓存进程间通信的数据**。

信号

运⾏在 shell 终端的进程，我们可以通过键盘输⼊某些组合键的时候，给进程发送信号。例如

- Ctrl+C 产⽣ SIGINT 信号，表示终⽌该进程；
- Ctrl+Z 产⽣ SIGTSTP 信号，表示停⽌该进程，但还未结束；

如果进程在后台运⾏，可以通过 kill 命令的⽅式给进程发送信号，但前提需要知道运⾏中的进程 PID号，例如：

- kill -9 1050 ，表示给 PID 为 1050 的进程发送 SIGKILL 信号，⽤来⽴即结束该进程；

所以，信号事件的来源主要有硬件来源（如键盘 Cltr+C ）和软件来源（如 kill 命令）。

## 线程间的通信方式

锁机制：包括互斥锁、条件变量、读写锁

信号量机制(Semaphore)



1. **互斥量(Mutex)**：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。
2. **信号量(Semphares)** ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量。
3. **事件(Event)** :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作。



### 线程间通信的方式

1. 等待通知机制 wait()、notify()、join()、interrupted()
2. 并发工具synchronized、lock、CountDownLatch、CyclicBarrier、Semaphore



## 进程有哪几种状态?

- **创建状态(new)** ：进程正在被创建，尚未到就绪状态。
- **就绪状态(ready)** ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。
- **运行状态(running)** ：进程正在处理器上上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。
- **阻塞状态(waiting)** ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。
- **结束状态(terminated)** ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。





## IO多路复用

https://juejin.cn/post/6882984260672847879写的不错





---

https://www.zhihu.com/question/28594409

首先，要从你常用的IO操作谈起，比如read和write，通常IO操作都是**阻塞I/O**的，也就是说当你调用read时，如果没有数据收到，那么线程或者进程就会被挂起，直到收到数据。

![img](D:\mdImage\16ef4bcfbd8319535edeb45f597dfc61_1440w.jpg)



这样，当服务器需要处理1000个连接的的时候，而且只有很少连接忙碌的，那么会需要1000个线程或进程来处理1000个连接，而1000个线程**大部分是被阻塞起来的**。由于CPU的核数或超线程数一般都不大，比如4,8,16,32,64,128，比如4个核要跑1000个线程，那么每个线程的时间槽非常短，而线程切换非常频繁。这样是有问题的：

1. 线程是有内存开销的，1个线程可能需要512K（或2M）存放栈，那么1000个线程就要512M（或2G）内存。
2. 线程的切换，或者说上下文切换是有CPU开销的，当大量时间花在上下文切换的时候，分配给真正的操作的CPU就要少很多。

那么，我们就要引入**非阻塞I/O**的概念，非阻塞IO很简单，通过fcntl（POSIX）或ioctl（Unix）设为非阻塞模式，这时，当你调用read时，如果有数据收到，就返回数据，如果没有数据收到，就立刻返回一个错误，如EWOULDBLOCK。这样是不会阻塞线程了，但是你还是要不断的轮询来读取或写入。

![img](D:\mdImage\2cb0550b87ca28336d0411e58b45b013_1440w.jpg)



于是，我们需要引入**IO[多路复用](https://www.zhihu.com/search?q=多路复用&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A74003996})**的概念。多路复用是指使用一个线程来检查多个文件描述符（Socket）的就绪状态，比如调用select和poll函数，传入多个文件描述符，如果有一个文件描述符就绪，则返回，否则阻塞直到超时。得到就绪状态后进行真正的操作可以在同一个线程里执行，也可以启动线程执行（比如使用线程池）。

![img](D:\mdImage\9155e2307879cd7ce515e7a997b9d532_1440w.jpg)

这样在处理1000个连接时，只需要1个线程监控就绪状态，对就绪的每个连接开一个线程处理就可以了，这样需要的线程数大大减少，减少了内存开销和上下文切换的CPU开销。

使用select函数的方式如下图所示：

![img](D:\mdImage\bf52854bd1dc678de998b77aebaa2311_1440w.jpg)

---

I/O Multiplexing 的命名中 multiplexing 来自于通讯领域：在一个信道上传输多路信号或者数据流的技术[1]。

Linux 下的事件驱动的 I/O 编程之所以需要 multiplexing，是因为对 **I/O ready 事件的通知是以一个监听集合为单位完成的**。大家在用 select, poll, 和 epoll 时都是先创建这么一个集合，然后统一的 wait，一次 wait 可以获知多个 fd 的 I/O 事件。所以 **multiplex 的是监听集合，并非 I/O 本身**。实际发生操作的 I/O  fd 都是可以并发读写互不干扰的，不存在 multiplexing 的问题。



![img](D:\mdImage\v2-0348e9e9f90b1f451f6a70846726d6c1_1440w.jpg)



---

**select/epoll函数就是多路复用器(multiplexing)**

1) 早期的socket服务器是这样的：

```text
ClientSocket s = accept(ServerSocket);  // 阻塞
   new Thread(s,function(){
         while( have data) {
            s.read(); // 阻塞
            s.send('xxxx');
         }
         s.close();
   });
```

  核心是一个线程处理一个socket。

2) 现在的event-driven/proactor/reactor模式：

```
ClientSocket s = accept(ServerSocket);  // 阻塞
  waitting_set.add(s); 
  while(1) {
     set_have_data  ds = select(waitting_set); // 多路复用器，
                                               // 多路复用是说多个socket数据通路
                                               // 一个select就可以侦听
                                               // 就好像他们是一个数据通路一样。
     foreach(ds as item) {
          item.read();  // reactor这里不会阻塞，采用异步io，详情查linux aio
          item.send('xxx');
     }
      item.close();
  }


```

可以看到，第一，一个线程就完成了n个socket的读写，第二，多了select语句。

event-driven/proactor/reactor好处：

1. 避免大量的线程创建工作，每个线程的创建工作都要消耗系统资源，比如每个线程至少要1M内存来维持其存在，32位的机器，不用thread pool，并发4千就挂了。
2. 线程多了难免会更多的发生线程切换（context switch），线程切换要保存线程的stack，register，如果是进程切换，要清cpu的L1 cache，MMU的TLB，使得系统不必要的开销过大。

---

当我们要编写一个echo服务器程序的时候，需要对用户从标准输入键入的交互命令做出响应。在这种情况下，服务器必须响应两个相互独立的I/O事件：1）网络客户端发起网络连接请求，2）用户在键盘上键入命令行。

我们先等待哪个事件呢？没有哪个选择是理想的。如果在acceptor中等待一个连接请求，我们就不能响应输入的命令。类似地，如果在read中等待一个输入命令，我们就不能响应任何连接请求。针对这种困境的一个解决办法就是I/O多路复用技术。基本思路就是使用select函数，要求内核挂起进程，只有在一个或多个I/O事件发生后，才将控制返回给应用程序。--《UNIX网络编程》



---

https://www.bilibili.com/video/BV11Z4y157RY?p=36

![image-20220416104348465](D:\mdImage\image-20220416104348465.png)

![image-20220416105217513](D:\mdImage\image-20220416105217513.png)

```
cd /home/code/cprogram
```

```
g++ -g -o tcpselect tcpselect.cpp
```

```
g++ -g -o client client.cpp
```

```
./tcpselect 5005
```

```
./client 127.0.0.1 5005
```



client：客户端

```java
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <netinet/in.h>
#include <sys/socket.h>
#include <arpa/inet.h>

int main(int argc, char *argv[])
{
  if (argc != 3)
  {
    printf("usage:./tcpclient ip port\n"); return -1;
  }

  int sockfd;
  struct sockaddr_in servaddr;
  char buf[1024];
 
  if ((sockfd=socket(AF_INET,SOCK_STREAM,0))<0) { printf("socket() failed.\n"); return -1; }
	
  memset(&servaddr,0,sizeof(servaddr));
  servaddr.sin_family=AF_INET;
  servaddr.sin_port=htons(atoi(argv[2]));
  servaddr.sin_addr.s_addr=inet_addr(argv[1]);

  if (connect(sockfd, (struct sockaddr *)&servaddr,sizeof(servaddr)) != 0)
  {
    printf("connect(%s:%s) failed.\n",argv[1],argv[2]); close(sockfd);  return -1;
  }

  printf("connect ok.\n");

  for (int ii=0;ii<10000;ii++)
  {
    // 从命令行输入内容。
    memset(buf,0,sizeof(buf));
    printf("please input:"); scanf("%s",buf);
    // sprintf(buf,"1111111111111111111111ii=%08d",ii);

    if (write(sockfd,buf,strlen(buf)) <=0)
    { 
      printf("write() failed.\n");  close(sockfd);  return -1;
    }
		
    memset(buf,0,sizeof(buf));
    if (read(sockfd,buf,sizeof(buf)) <=0) 
    { 
      printf("read() failed.\n");  close(sockfd);  return -1;
    }

    printf("recv:%s\n",buf);

    // close(sockfd); break;
  }
} 

```

tcpselect：

```java
#include <stdio.h>
#include <unistd.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <arpa/inet.h>
#include <sys/fcntl.h>

// 初始化服务端的监听端口。
int initserver(int port);

int main(int argc,char *argv[])
{
  if (argc != 2)
  {
    printf("usage: ./tcpselect port\n"); return -1;
  }

  // 初始化服务端用于监听的socket。
  int listensock = initserver(atoi(argv[1]));
  printf("listensock=%d\n",listensock);

  if (listensock < 0)
  {
    printf("initserver() failed.\n"); return -1;
  }

  fd_set readfdset;  // 读事件的集合，包括监听socket和客户端连接上来的socket。
  int maxfd;  // readfdset中socket的最大值。

  // 初始化结构体，把listensock添加到集合中。
  FD_ZERO(&readfdset);

  FD_SET(listensock,&readfdset);
  maxfd = listensock;

  while (1)
  {
    // 调用select函数时，会改变socket集合的内容，所以要把socket集合保存下来，传一个临时的给select。
    fd_set tmpfdset = readfdset;

    int infds = select(maxfd+1,&tmpfdset,NULL,NULL,NULL);
    // printf("select infds=%d\n",infds);

    // 返回失败。
    if (infds < 0)
    {
      printf("select() failed.\n"); perror("select()"); break;
    }

    // 超时，在本程序中，select函数最后一个参数为空，不存在超时的情况，但以下代码还是留着。
    if (infds == 0)
    {
      printf("select() timeout.\n"); continue;
    }

    // 检查有事情发生的socket，包括监听和客户端连接的socket。
    // 这里是客户端的socket事件，每次都要遍历整个集合，因为可能有多个socket有事件。
    for (int eventfd=0; eventfd <= maxfd; eventfd++)
    {
      if (FD_ISSET(eventfd,&tmpfdset)<=0) continue;

      if (eventfd==listensock)
      { 
        // 如果发生事件的是listensock，表示有新的客户端连上来。
        struct sockaddr_in client;
        socklen_t len = sizeof(client);
        int clientsock = accept(listensock,(struct sockaddr*)&client,&len);
        if (clientsock < 0)
        {
          printf("accept() failed.\n"); continue;
        }

        printf ("client(socket=%d) connected ok.\n",clientsock);

        // 把新的客户端socket加入集合。
        FD_SET(clientsock,&readfdset);

        if (maxfd < clientsock) maxfd = clientsock;

        continue;
      }
      else
      {
        // 客户端有数据过来或客户端的socket连接被断开。
        char buffer[1024];
        memset(buffer,0,sizeof(buffer));

        // 读取客户端的数据。
        ssize_t isize=read(eventfd,buffer,sizeof(buffer));

        // 发生了错误或socket被对方关闭。
        if (isize <=0)
        {
          printf("client(eventfd=%d) disconnected.\n",eventfd);

          close(eventfd);  // 关闭客户端的socket。

          FD_CLR(eventfd,&readfdset);  // 从集合中移去客户端的socket。

          // 重新计算maxfd的值，注意，只有当eventfd==maxfd时才需要计算。
          if (eventfd == maxfd)
          {
            for (int ii=maxfd;ii>0;ii--)
            {
              if (FD_ISSET(ii,&readfdset))
              {
                maxfd = ii; break;
              }
            }

            printf("maxfd=%d\n",maxfd);
          }

          continue;
        }

        printf("recv(eventfd=%d,size=%d):%s\n",eventfd,isize,buffer);

        // 把收到的报文发回给客户端。
        write(eventfd,buffer,strlen(buffer));
      }
    }
  }

  return 0;
}

// 初始化服务端的监听端口。
int initserver(int port)
{
  int sock = socket(AF_INET,SOCK_STREAM,0);
  if (sock < 0)
  {
    printf("socket() failed.\n"); return -1;
  }

  // Linux如下
  int opt = 1; unsigned int len = sizeof(opt);
  setsockopt(sock,SOL_SOCKET,SO_REUSEADDR,&opt,len);
  setsockopt(sock,SOL_SOCKET,SO_KEEPALIVE,&opt,len);

  struct sockaddr_in servaddr;
  servaddr.sin_family = AF_INET;
  servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
  servaddr.sin_port = htons(port);

  if (bind(sock,(struct sockaddr *)&servaddr,sizeof(servaddr)) < 0 )
  {
    printf("bind() failed.\n"); close(sock); return -1;
  }

  if (listen(sock,5) != 0 )
  {
    printf("listen() failed.\n"); close(sock); return -1;
  }

  return sock;
}

```

![image-20220416173733464](D:\mdImage\image-20220416173733464.png)

![image-20220416173745462](D:\mdImage\image-20220416173745462.png)





![image-20220416111405995](D:\mdImage\image-20220416111405995.png)

![image-20220416173817607](D:\mdImage\image-20220416173817607.png)

(一般是立即报告)

### BIO、NIO、AIO

https://mp.weixin.qq.com/s?__biz=MzA4NjI1MTkyNw==&mid=2449993957&idx=1&sn=12c3911a5a2e2683d6b2263d027dffda&scene=21#wechat_redirect



## Unix网络编程中的五种IO模型

https://mp.weixin.qq.com/s/T-hP3wt4whtvVh1H1LBU3w



## select、poll、epoll

https://segmentfault.com/a/1190000003063859

**select，poll，epoll都是IO多路复用的机制**。I/O多路复用就是通过一种机制，一个**进程**(?)可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。（这里啰嗦下）

https://m.haicoder.net/note/linux-interview/linux-interview-linux-select-poll-epoll.html  （看这个完事）

select，poll，epoll 都是 IO 多路复用的机制。I/O 多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。

但 select，poll，epoll 本质上都是同步 I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步 I/O 则无需自己负责进行读写，异步 I/O 的实现会负责把数据从内核拷贝到用户空间。

select的几大缺点

1. 每次调用 select，都需要把 fd 集合从用户态拷贝到内核态，这个开销在 fd 很多时会很大。
2. 同时每次调用 select 都需要在内核遍历传递进来的所有 fd，这个开销在 fd 很多时也很大。
3. select 支持的文件描述符数量太小了，默认是 1024。

poll实现

poll 的实现和 select 非常相似，只是描述 fd 集合的方式不同，poll 使用 pollfd 结构而不是 select 的 fd_set 结构，其他的都差不多。

epoll

epoll 既然是对 select 和 poll 的改进，就应该能避免上述的三个缺点。那 epoll 都是怎么解决的呢？在此之前，我们先看一下 epoll 和 select 和 poll 的调用接口上的不同，select 和 poll 都只提供了一个函数—— select 或者 poll 函数。而 epoll 提供了三个函数，epoll_create,epoll_ctl 和 epoll_wait，epoll_create 是创建一个 epoll 句柄；epoll_ctl 是注册要监听的事件类型；epoll_wait 则是等待事件的产生。

对于第一个缺点，epoll 的解决方案在 epoll_ctl 函数中。每次注册新的事件到 epoll 句柄中时（在 epoll_ctl 中指定 EPOLL_CTL_ADD），会把所有的 fd 拷贝进内核，而不是在 epoll_wait 的时候重复拷贝。epoll 保证了每个 fd 在整个过程中只会拷贝一次。

对于第二个缺点，epoll 的解决方案不像 select 或 poll 一样每次都把 current 轮流加入 fd 对应的设备等待队列中，而只在 epoll_ctl 时把 current 挂一遍（这一遍必不可少）并为每个 fd 指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的 fd 加入一个就绪链表）。epoll_wait 的工作实际上就是在这个就绪链表中查看有没有就绪的 fd（利用 schedule_timeout()实 现睡一会，判断一会的效果，和 select 实现中的第 7 步是类似的）。

对于第三个缺点，epoll 没有这个限制，它所支持的 FD 上限是最大可以打开文件的数目，这个数字一般远大于 2048,举个例子,在 1GB 内存的机器上大约是 10 万左右，具体数目可以 cat /proc/sys/fs/file-max 察看,一般来说这个数目和系统内存关系很大。

---

select：
用直白的话来介绍 select：select 机制会监听它所负责的所有 socket，当其中一个 socket 或者多个 socket 可读或者可写的时候，它就会返回，而如果所有的 socket 都是不可读或者不可写的时候，这个进程就会被阻塞，直到超时或者 socket 可读写，当 select 函数返回后，可以通过遍历 fdset，来找到就绪的描述符。

1. 用户进程调用 select() 函数，如果当前没有可读写的 socket，则用户进程进入阻塞状态。
2. 对于内核空间来说，它会从用户空间拷贝 fd_set 到内核空间，然后在内核中遍历一遍所有的 socket 描述符，如果没有满足条件的 socket 描述符，内核将进行休眠，当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的内核进程，即在 socket 可读写时唤醒，或者在超时后唤醒。
3. 返回 select() 函数的调用结果给用户进程，返回就绪 socket 描述符的数目，超时返回 0，出错返回 -1。
4. 注意，在 select() 函数返回后还是需要轮询去找到就绪的 socket 描述符的，此时用户进程才可以去操作 socket 。

select 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。当然 select 也有很多缺点：

1. **单个进程**能够监视的文件描述符的数量存在最大限制，在 Linux 上一般为 1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但是这样也会造成效率的降低。
2. 需要维护一个用来存放大量 fd 的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。
3. 每次在有 socket 描述符活跃的时候，都需要遍历一遍所有的 fd 找到该描述符，这会带来大量的时间消耗（时间复杂度是O(n)，并且伴随着描述符越多，这开销呈线性增长）

**select函数原型：**

```
int select(int maxfdp1,fd_set *readset,fd_set *writeset,fd_set *exceptset,const struct timeval *timeout)
```

参数说明：

- maxfdp1 指定感兴趣的 socket 描述符个数，它的值是套接字最大 socket 描述符加 1，socket 描述符 0、1、2 … maxfdp1-1 均将被设置为感兴趣（即会查看他们是否可读、可写）。
- readset：指定这个 socket 描述符是可读的时候才返回。
- writeset：指定这个 socket 描述符是可写的时候才返回。
- exceptset：指定这个 socket 描述符是异常条件时候才返回。
- timeout：指定了超时的时间，当超时了也会返回。

**如果对某一个的条件不感兴趣，就可以把它设为空指针**。

返回值：就绪 socket 描述符的数目，超时返回 0，出错返回 -1。



---

https://losophy.github.io/post/c352656a.html

# Redis



## 什么是Redis

**Redis(Remote Dictionary Server) 就是一个使用 C 语言开发的数据库**，不过与传统数据库不同的是 **Redis 的数据是存在内存中的** ，也就是它是内存数据库，所以读写速度非常快，因此 Redis 被广泛应用于缓存方向。



Redis是一款内存高速缓存数据库。Redis全称为：**Remote Dictionary Server**（远程数据服务），使用C语言编写，Redis是一个key-value存储系统（键值存储系统），支持丰富的数据类型，如：String、list、set、zset、hash。

Redis是一种支持key-value等多种数据结构的存储系统。可用于缓存，事件发布或订阅，高速队列等场景。支持网络，提供字符串，哈希，列表，队列，集合结构直接存取，基于内存，可持久化。

## 为什么要使用Redis



**读写性能优异**

- Redis能读的速度是110000次/s,写的速度是81000次/s （测试条件见下一节）。

**数据类型丰富**

- Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。

**原子性**

- Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。

**丰富的特性**

- Redis支持 publish/subscribe, 通知, key 过期等特性。

**持久化**

- Redis支持RDB, AOF等持久化方式

**发布订阅**

- Redis支持发布/订阅模式

**分布式**

- Redis Cluste

## Redis的使用场景



### 热点数据的缓存

缓存是Redis最常见的应用场景，之所有这么使用，主要是因为Redis读写性能优异。而且逐渐有取代memcached，成为首选服务端缓存的组件。而且，Redis内部是支持事务的，在使用时候能有效保证数据的一致性。

作为缓存使用时，一般有两种方式保存数据：

- 读取前，先去读Redis，如果没有数据，读取数据库，将数据拉入Redis。
- 插入数据时，同时写入Redis。

方案一：实施起来简单，但是有两个需要注意的地方：

- 避免缓存击穿。（数据库没有就需要命中的数据，导致Redis一直没有数据，而一直命中数据库。）
- 数据的实时性相对会差一点。

方案二：数据实时性强，但是开发时不便于统一处理。

当然，两种方式根据实际情况来适用。如：方案一适用于对于数据实时性要求不是特别高的场景。方案二适用于字典表、数据量不大的数据存储。



### 限时业务的运用

redis中可以使用expire命令设置一个键的生存时间，到时间后redis会删除它。利用这一特性可以运用在限时的优惠活动信息、手机验证码等业务场景。

### 计数器相关问题

redis由于incrby命令可以实现原子性的递增，所以可以运用于高并发的秒杀活动、分布式序列号的生成、具体业务还体现在比如限制一个手机号发多少条短信、一个接口一分钟限制多少请求、一个接口一天限制调用多少次等等。

### 分布式锁

这个主要利用redis的setnx命令进行，setnx："set if not exists"就是如果不存在则成功设置缓存同时返回1，否则返回0 ，这个特性在很多后台中都有所运用，因为我们服务器是集群的，定时任务可能在两台机器上都会运行，所以在定时任务中首先 通过setnx设置一个lock， 如果成功设置则执行，如果没有成功设置，则表明该定时任务已执行。 当然结合具体业务，我们可以给这个lock加一个过期时间，比如说30分钟执行一次的定时任务，那么这个过期时间设置为小于30分钟的一个时间就可以，这个与定时任务的周期以及定时任务执行消耗时间相关。

在分布式锁的场景中，主要用在比如秒杀系统等。

### 延时操作

比如在订单生产后我们占用了库存，10分钟后去检验用户是否真正购买，如果没有购买将该单据设置无效，同时还原库存。 由于redis自2.8.0之后版本提供Keyspace Notifications功能，允许客户订阅Pub/Sub频道，以便以某种方式接收影响Redis数据集的事件。 所以我们对于上面的需求就可以用以下解决方案，我们在订单生产时，设置一个key，同时设置10分钟后过期， 我们在后台实现一个监听器，监听key的实效，监听到key失效时将后续逻辑加上。

当然我们也可以利用rabbitmq、activemq等消息中间件的延迟队列服务实现该需求。

### 排行榜相关问题

关系型数据库在排行榜方面查询速度普遍偏慢，所以可以借助redis的SortedSet进行热点数据的排序。

比如点赞排行榜，做一个SortedSet, 然后以用户的openid作为上面的username, 以用户的点赞数作为上面的score, 然后针对每个用户做一个hash, 通过zrangebyscore就可以按照点赞数获取排行榜，然后再根据username获取用户的hash信息，这个当时在实际运用中性能体验也蛮不错的。

### 点赞、好友等相互关系的存储

Redis 利用集合的一些命令，比如求交集、并集、差集等。

在微博应用中，每个用户关注的人存在一个集合中，就很容易实现求两个人的共同好友功能。

### 简单队列

由于Redis有list push和list pop这样的命令，所以能够很方便的执行队列操作。



## Redis 5种基础数据类型

首先对redis来说，**所有的key（键）都是字符串**。我们在谈基础数据结构时，讨论的是**存储值的数据类型**，主要包括常见的5种数据类型，分别是：String、List、Set、Zset、Hash。

string

list

hash

set

sorted set

![image-20220417161429480](D:\mdImage\image-20220417161429480.png)

### String字符串

String类型是二进制安全的，意思是 redis 的 string 可以包含任何数据。如数字，字符串，jpg图片或者序列化的对象。



- **命令使用**

| 命令   | 简述                   | 使用              |
| ------ | ---------------------- | ----------------- |
| GET    | 获取存储在给定键中的值 | GET name          |
| SET    | 设置存储在给定键中的值 | SET name value    |
| DEL    | 删除存储在给定键中的值 | DEL name          |
| INCR   | 将键存储的值加1        | INCR key          |
| DECR   | 将键存储的值减1        | DECR key          |
| INCRBY | 将键存储的值加上整数   | INCRBY key amount |
| DECRBY | 将键存储的值减去整数   | DECRBY key amount |

```C
127.0.0.1:6379> set hello world
OK
127.0.0.1:6379> get hello
"world"
127.0.0.1:6379> del hello
(integer) 1
127.0.0.1:6379> get hello
(nil)
127.0.0.1:6379> get counter
"2"
127.0.0.1:6379> incr counter
(integer) 3
127.0.0.1:6379> get counter
"3"
127.0.0.1:6379> incrby counter 100
(integer) 103
127.0.0.1:6379> get counter
"103"
127.0.0.1:6379> decr counter
(integer) 102
127.0.0.1:6379> get counter
"102"

```



**实战场景**

- **缓存**： 经典使用场景，把常用信息，字符串，图片或者视频等信息放到redis中，redis作为缓存层，mysql做持久化层，降低mysql的读写压力。
- **计数器**：redis是单线程模型，一个命令执行完才会执行下一个，同时数据可以一步落地到其他的数据源。
- **session**：常见方案spring session + redis实现session共享，

### List列表

Redis中的List其实就是链表（Redis用双端链表实现List）。

使用List结构，我们可以轻松地实现最新消息排队功能（比如新浪微博的TimeLine）。List的另一个应用就是消息队列，可以利用List的 PUSH 操作，将任务存放在List中，然后工作线程再用 POP 操作将任务取出进行执行。

- **命令使用**

| 命令   | 简述                                                         | 使用            |
| ------ | ------------------------------------------------------------ | --------------- |
| RPUSH  | 将给定值推入到列表右端                                       | RPUSH key value |
| LPUSH  | 将给定值推入到列表左端                                       | LPUSH key value |
| RPOP   | 从列表的右端弹出一个值，并返回被弹出的值                     | RPOP key        |
| LPOP   | 从列表的左端弹出一个值，并返回被弹出的值                     | LPOP key        |
| LRANGE | 获取列表在给定范围上的所有值                                 | LRANGE key 0 -1 |
| LINDEX | 通过索引获取列表中的元素。你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推。 | LINEX key index |

```C
127.0.0.1:6379> lpush mylist 1 2 ll ls mem
(integer) 5
127.0.0.1:6379> lrange mylist 0 -1
1) "mem"
2) "ls"
3) "ll"
4) "2"
5) "1"
127.0.0.1:6379> lindex mylist -1
"1"
127.0.0.1:6379> lindex mylist 10        # index不在 mylist 的区间范围内
(nil)

```

- 实战场景
  - **微博TimeLine**: 有人发布微博，用lpush加入时间轴，展示新的列表信息。
  - **消息队列**

### Set集合

redis 的 Set 是 String 类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。

Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。



- **命令使用**

| 命令      | 简述                                  | 使用                 |
| --------- | ------------------------------------- | -------------------- |
| SADD      | 向集合添加一个或多个成员              | SADD key value       |
| SCARD     | 获取集合的成员数                      | SCARD key            |
| SMEMBERS  | 返回集合中的所有成员                  | SMEMBERS key member  |
| SISMEMBER | 判断 member 元素是否是集合 key 的成员 | SISMEMBER key member |

```
127.0.0.1:6379> sadd myset hao hao1 xiaohao hao
(integer) 3
127.0.0.1:6379> smembers myset
1) "xiaohao"
2) "hao1"
3) "hao"
127.0.0.1:6379> sismember myset hao
(integer) 1
```

- 实战场景
  - **标签**（tag）,给用户添加标签，或者用户给消息添加标签，这样有同一标签或者类似标签的可以给推荐关注的事或者关注的人。
  - **点赞，或点踩，收藏等**，可以放到set中实现



### Hash散列

Redis hash 是一个 string 类型的 field（字段） 和 value（值） 的映射表，hash 特别适合用于存储对象。



- **命令使用**

| 命令    | 简述                                     | 使用                          |
| ------- | ---------------------------------------- | ----------------------------- |
| HSET    | 添加键值对                               | HSET hash-key sub-key1 value1 |
| HGET    | 获取指定散列键的值                       | HGET hash-key key1            |
| HGETALL | 获取散列中包含的所有键值对               | HGETALL hash-key              |
| HDEL    | 如果给定键存在于散列中，那么就移除这个键 | HDEL hash-key sub-key1        |

- 实战场景
  - **缓存**： 能直观，相比string更节省空间（？），的维护缓存信息，如用户信息，视频信息等。

著作权归https://pdai.tech所有。 链接：https://www.pdai.tech/md/db/nosql-redis/db-redis-data-types.html

### Zset有序集合

> Redis 有序集合和集合一样也是 string 类型元素的集合,且不允许重复的成员。**不同的是每个元素都会关联一个 double 类型的分数**。redis 正是通过分数来为集合中的成员进行从小到大的排序。

有序集合的成员是唯一的,但**分数(score)却可以重复**。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。



- **命令使用**

| 命令   | 简述                                                     | 使用                           |
| ------ | -------------------------------------------------------- | ------------------------------ |
| ZADD   | 将一个带有给定分值的成员添加到有序集合里面               | ZADD zset-key 178 member1      |
| ZRANGE | 根据元素在有序集合中所处的位置，从有序集合中获取多个元素 | ZRANGE zset-key 0-1 withccores |
| ZREM   | 如果给定元素成员存在于有序集合中，那么就移除这个元素     | ZREM zset-key member1          |

- 实战场景
  - **排行榜**：有序集合经典使用场景。例如小说视频等网站需要对用户上传的小说视频做排行榜，榜单可以按照用户关注数，更新时间，字数等打分，做排行。

## Redis 3种特殊数据类型

### HyperLogLogs（基数统计）

Redis 2.8.9 版本更新了 Hyperloglog 数据结构

- **什么是基数？**

举个例子，A = {1, 2, 3, 4, 5}， B = {3, 5, 6, 7, 9}；那么基数（不重复的元素）= 1, 2, 4, 6, 7, 9； （允许容错，即可以接受一定误差）

- **HyperLogLogs 基数统计用来解决什么问题**？

这个结构可以非常省内存的去统计各种计数，比如注册 IP 数、每日访问 IP 数、页面实时UV、在线用户数，共同好友数等。

- **它的优势体现在哪**？

一个大型的网站，每天 IP 比如有 100 万，粗算一个 IP 消耗 15 字节，那么 100 万个 IP 就是 15M。而 HyperLogLog 在 Redis 中每个键占用的内容都是 12K，理论存储近似接近 2^64 个值，不管存储的内容是什么，它一个基于基数估算的算法，只能比较准确的估算出基数，可以使用少量固定的内存去存储并识别集合中的唯一元素。而且这个估算的基数并不一定准确，是一个带有 0.81% 标准错误的近似值（对于可以接受一定容错的业务场景，比如IP数统计，UV等，是可以忽略不计的）。

- **相关命令使用**

```bash
127.0.0.1:6379> pfadd key1 a b c d e f g h i	# 创建第一组元素
(integer) 1
127.0.0.1:6379> pfcount key1					# 统计元素的基数数量
(integer) 9
127.0.0.1:6379> pfadd key2 c j k l m e g a		# 创建第二组元素
(integer) 1
127.0.0.1:6379> pfcount key2
(integer) 8
127.0.0.1:6379> pfmerge key3 key1 key2			# 合并两组：key1 key2 -> key3 并集
OK
127.0.0.1:6379> pfcount key3
(integer) 13
```

著作权归https://pdai.tech所有。 链接：https://www.pdai.tech/md/db/nosql-redis/db-redis-data-type-special.html

### Bitmap （位存储）

> Bitmap 即位图数据结构，都是操作二进制位来进行记录，只有0 和 1 两个状态。

- **用来解决什么问题**？

比如：统计用户信息，活跃，不活跃！ 登录，未登录！ 打卡，不打卡！ **两个状态的，都可以使用 Bitmaps**！

如果存储一年的打卡状态需要多少内存呢？ 365 天 = 365 bit 1字节 = 8bit 46 个字节左右！

- **相关命令使用**

使用bitmap 来记录 周一到周日的打卡！ 周一：1 周二：0 周三：0 周四：1 ......

```bash
127.0.0.1:6379> setbit sign 0 1
(integer) 0
127.0.0.1:6379> setbit sign 1 1
(integer) 0
127.0.0.1:6379> setbit sign 2 0
(integer) 0
127.0.0.1:6379> setbit sign 3 1
(integer) 0
127.0.0.1:6379> setbit sign 4 0
(integer) 0
127.0.0.1:6379> setbit sign 5 0
(integer) 0
127.0.0.1:6379> setbit sign 6 1
(integer) 0   
```

查看某一天是否有打卡！

```bash
127.0.0.1:6379> getbit sign 3
(integer) 1
127.0.0.1:6379> getbit sign 5
(integer) 0 
```



统计操作，统计 打卡的天数！

```bash
127.0.0.1:6379> bitcount sign # 统计这周的打卡记录，就可以看到是否有全勤！
(integer) 3
```

### geospatial (地理位置)

> Redis 的 Geo 在 Redis 3.2 版本就推出了! 这个功能可以推算地理位置的信息: 两地之间的距离, 方圆几里的人

## Redis Stream数据类型

Redis5.0 中还增加了一个数据结构Stream，从字面上看是流类型，但其实从功能上看，应该是Redis对消息队列（MQ，Message Queue）的完善实现。





## Redis 底层数据结构

6.0：

String `SDS`

List `QuickList` 

Hash `ZipList` `HashTable`  （这里的HashTable是不是应该是dict，或许版本不同，叫法不同）

> 哈希对象的编码可以是 ziplist 或者 hashtable；对应的底层实现有两种, 一种是ziplist, 一种是dict。(整理一下)   编码方式和底层实现

Set `IntSet` `HashTable`

Sorted Set `ZipList` `ZSkipList`

---

### SDS 简单动态字符串

`simple dynamic string`

这是一种用于存储二进制数据的一种结构, 具有**动态扩容**的特点. 其实现位于src/sds.h与src/sds.c中。

优点：

+ **常数复杂度获取字符串长度**
+ **杜绝缓冲区溢出**
+ **减少修改字符串的内存重新分配次数**
+ **二进制安全**
+ **兼容部分 C 字符串函数**



**常数复杂度获取字符串长度**

由于 len 属性的存在，我们获取 SDS 字符串的长度只需要读取 len 属性，时间复杂度为 O(1)。而对于 C 语言，获取字符串的长度通常是经过遍历计数来实现的，时间复杂度为 O(n)。通过 `strlen key` 命令可以获取 key 的字符串长度。

**杜绝缓冲区溢出**

我们知道在 C 语言中使用 `strcat`  函数来进行两个字符串的拼接，一旦没有分配足够长度的内存空间，就会造成缓冲区溢出。而对于 SDS 数据类型，在进行字符修改的时候，**会首先根据记录的 len 属性检查内存空间是否满足需求**，如果不满足，会进行相应的空间扩展，然后在进行修改操作，所以不会出现缓冲区溢出。

**减少修改字符串的内存重新分配次数**

C语言由于不记录字符串的长度，所以如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。

而对于SDS，由于`len`属性和`alloc`属性的存在，对于修改字符串SDS实现了**空间预分配**和**惰性空间释放**两种策略：

1、`空间预分配`：对字符串进行空间扩展的时候，扩展的内存比实际需要的多，这样可以减少连续执行字符串增长操作所需的内存重分配次数。

2、`惰性空间释放`：对字符串进行缩短操作时，程序不立即使用内存重新分配来回收缩短后多余的字节，而是使用 `alloc` 属性将这些字节的数量记录下来，等待后续使用。（当然SDS也提供了相应的API，当我们有需要时，也可以手动释放这些未使用的空间。）



**二进制安全**

因为C字符串以空字符作为字符串结束的标识，而对于一些二进制文件（如图片等），内容可能包括空字符串，因此C字符串无法正确存取；而所有 SDS 的API 都是以处理二进制的方式来处理 `buf` 里面的元素，并且 SDS 不是以空字符串来判断是否结束，而是以 len 属性表示的长度来判断字符串是否结束。

**兼容部分 C 字符串函数**

虽然 SDS 是二进制安全的，但是一样遵从每个字符串都是以空字符串结尾的惯例，这样可以重用 C 语言库`<string.h>` 中的一部分函数。



###  QuickList 快表

quicklist这个结构是Redis在3.2版本后新加的, 之前的版本是list(即linkedlist)， 用于String数据类型中。

![image-20220417154423129](D:\mdImage\image-20220417154423129.png)



`quicklistNode`, 宏观上, quicklist是一个链表, 这个结构描述的就是链表中的结点. 它通过zl字段持有底层的ziplist. 简单来讲, 它描述了一个ziplist实例

`quicklistLZF`, ziplist是一段连续的内存, 用LZ4算法压缩后, 就可以包装成一个quicklistLZF结构. 是否压缩quicklist中的每个ziplist实例是一个可配置项. 若这个配置项是开启的, 那么quicklistNode.zl字段指向的就不是一个ziplist实例, 而是一个压缩后的quicklistLZF实例

`quicklistBookmark`, 在quicklist尾部增加的一个书签，它只有在大量节点的多余内存使用量可以忽略不计的情况且确实需要分批迭代它们，才会被使用。当不使用它们时，它们不会增加任何内存开销。

`quicklist`. 这就是一个双链表的定义. head, tail分别指向头尾指针. len代表链表中的结点. count指的是整个quicklist中的所有ziplist中的entry的数目. fill字段影响着每个链表结点中ziplist的最大占用空间, compress影响着是否要对每个ziplist以LZ4算法进行进一步压缩以更节省内存空间.

`quicklistIter`是一个迭代器

`quicklistEntry`是对ziplist中的entry概念的封装. quicklist作为一个封装良好的数据结构, 不希望使用者感知到其内部的实现, 所以需要把ziplist.entry的概念重新包装一下



### ZipList 压缩列表



ziplist是为了提高存储效率而设计的一种特殊编码的双向链表。它可以存储字符串或者整数，存储整数时是采用整数的二进制而不是字符串形式存储。他能在O(1)的时间复杂度下完成list两端的push和pop操作。但是因为每次操作都需要重新分配ziplist的内存，所以实际复杂度和ziplist的内存使用量相关。

为什么ZipList特别省内存？

所以只有理解上面的Entry结构，我们才会真正理解ZipList为什么是特别节省内存的数据结构。

ziplist节省内存是相对于普通的list来说的，如果是普通的数组，那么它每个元素占用的内存是一样的且取决于最大的那个元素（很明显它是需要预留空间的）；

所以ziplist在设计时就很容易想到要尽量让每个元素按照实际的内容大小存储，**所以增加encoding字段**，针对不同的encoding来细化存储大小；

这时候还需要解决的一个问题是遍历元素时如何定位下一个元素呢？在普通数组中每个元素定长，所以不需要考虑这个问题；但是ziplist中每个data占据的内存不一样，所以为了解决遍历，需要增加记录上一个元素的length，**所以增加了prelen字段**。



ziplist的缺点

最后我们再看看它的一些缺点：

- ziplist也不预留内存空间, 并且在移除结点后, 也是立即缩容, 这代表每次写操作都会进行内存分配操作.
- 结点如果扩容, 导致结点占用的内存增长, 并且超过254字节的话, 可能会导致链式反应: 其后一个结点的entry.prevlen需要从一字节扩容至五字节. **最坏情况下, 第一个结点的扩容, 会导致整个ziplist表中的后续所有结点的entry.prevlen字段扩容**. 虽然这个内存重分配的操作依然只会发生一次, 但代码中的时间复杂度是o(N)级别, 因为链式扩容只能一步一步的计算. 但这种情况的概率十分的小, 一般情况下链式扩容能连锁反映五六次就很不幸了. 之所以说这是一个蛋疼问题, 是因为, 这样的坏场景下, 其实时间复杂度并不高: 依次计算每个entry新的空间占用, 也就是o(N), 总体占用计算出来后, 只执行一次内存重分配, 与对应的memmove操作, 就可以了.



### Dict 字典/哈希表

```C
typedef struct dictht{
    //哈希表数组
    dictEntry **table;
    //哈希表大小
    unsigned long size;
    //哈希表大小掩码，用于计算索引值
    //总是等于 size-1
    unsigned long sizemask;
    //该哈希表已有节点的数量
    unsigned long used;
 
}dictht

```

哈希表是由数组 table 组成，table 中每个元素都是指向 dict.h/dictEntry 结构，dictEntry 结构定义如下：

```C
typedef struct dictEntry{
     //键
     void *key;
     //值
     union{
          void *val;
          uint64_tu64;
          int64_ts64;
     }v;
 
     //指向下一个哈希表节点，形成链表
     struct dictEntry *next;
}dictEntry

```

key 用来保存键，val 属性用来保存值，值可以是一个指针，也可以是uint64_t整数，也可以是int64_t整数。



注意这里还有一个指向下一个哈希表节点的指针，我们知道哈希表最大的问题是存在哈希冲突，如何解决哈希冲突，有开放地址法和链地址法。这里采用的便是**链地址法**，通过next这个指针可以将多个哈希值相同的键值对连接在一起，用来**解决哈希冲突**。

著作权归https://pdai.tech所有。 链接：https://www.pdai.tech/md/db/nosql-redis/db-redis-x-redis-ds.html

- **哈希算法**：Redis计算哈希值和索引值方法如下：

```bash
#1、使用字典设置的哈希函数，计算键 key 的哈希值
hash = dict->type->hashFunction(key);

#2、使用哈希表的sizemask属性和第一步得到的哈希值，计算索引值
index = hash & dict->ht[x].sizemask; 
```



- **解决哈希冲突**：这个问题上面我们介绍了，方法是链地址法。通过字典里面的 *next 指针指向下一个具有相同索引值的哈希表节点。
- **扩容和收缩**：当哈希表保存的键值对太多或者太少时，就要通过 rerehash(重新散列）来对哈希表进行相应的扩展或者收缩。具体步骤：

1、如果执行扩展操作，会基于原哈希表创建一个大小等于 ht[0].used*2n 的哈希表（也就是每次扩展都是根据原哈希表已使用的空间扩大一倍创建另一个哈希表）。相反如果执行的是收缩操作，每次收缩是根据已使用空间缩小一倍创建一个新的哈希表。

2、重新利用上面的哈希算法，计算索引值，然后将键值对放到新的哈希表位置上。

3、所有键值对都迁徙完毕后，释放原哈希表的内存空间。

- **触发扩容的条件**：

1、服务器目前**没有**执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于1。

2、服务器目前**正在**执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于5。

ps：负载因子 = 哈希表已保存节点数量 / 哈希表大小。

- **渐近式 rehash**

什么叫渐进式 rehash？也就是说**扩容和收缩操作不是一次性、集中式完成的，而是分多次、渐进式完成的**。如果保存在Redis中的键值对只有几个几十个，那么 rehash 操作可以瞬间完成，但是如果键值对有几百万，几千万甚至几亿，那么要**一次性的进行 rehash，势必会造成Redis一段时间内不能进行别的操作**。所以Redis采用渐进式 rehash,这样在进行渐进式rehash期间，**字典的删除查找更新等操作可能会在两个哈希表上进行，第一个哈希表没有找到，就会去第二个哈希表上进行查找。但是进行 增加操作，一定是在新的哈希表上进行的**。

### IntSet 整数集 

整数集合（intset）是集合类型的底层实现之一，当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis 就会使用整数集合作为集合键的底层实现。

```
typedef struct intset {
    uint32_t encoding;
    uint32_t length;
    int8_t contents[];
} intset;

```



`encoding` 表示编码方式，的取值有三个：INTSET_ENC_INT16, INTSET_ENC_INT32, INTSET_ENC_INT64

`length` 代表其中存储的整数的个数

`contents` 指向实际存储数值的连续内存区域, 就是一个数组；整数集合的每个元素都是 contents 数组的一个数组项（item），各个项在数组中按值得大小**从小到大有序排序**，且数组中不包含任何重复项。（虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组，但实际上 contents 数组并不保存任何 int8_t 类型的值，contents 数组的真正类型取决于 encoding 属性的值）

内存布局图：

![image-20220417155922708](D:\mdImage\image-20220417155922708.png)

我们可以看到，content数组里面每个元素的数据类型是由encoding来决定的，那么如果原来的数据类型是int16, 当我们再插入一个int32类型的数据时怎么办呢？这就是下面要说的intset的升级。

整数集合的升级：

当在一个int16类型的整数集合中插入一个int32类型的值，整个集合的所有元素都会转换成32类型。 整个过程有三步：

- 根据新元素的类型（比如int32），扩展整数集合底层数组的空间大小，并为新元素分配空间。
- 将底层数组现有的所有元素都转换成与新元素相同的类型， 并将类型转换后的元素放置到正确的位上， 而且在放置元素的过程中， 需要继续维持底层数组的有序性质不变。
- 最后改变encoding的值，length+1。

**那么如果我们删除掉刚加入的int32类型时，会不会做一个降级操作呢**？

不会。主要还是减少开销的权衡。



### ZSkipList 跳表



跳跃表结构在 Redis 中的运用场景只有一个，那就是作为有序列表 (Zset) 的使用。跳跃表的性能可以保证在查找，删除，添加等操作的时候在对数期望时间内完成，这个性能是可以和平衡树来相比较的，而且在实现方面比平衡树要优雅，这就是跳跃表的长处。跳跃表的缺点就是需要的存储空间比较大，属于**利用空间来换取时间**的数据结构。

![image-20220417160054731](D:\mdImage\image-20220417160054731.png)

redis跳跃表并没有在单独的类（比如skplist.c)中定义，而是其定义在server.h中, 如下:

```C
/* ZSETs use a specialized version of Skiplists */
typedef struct zskiplistNode {
    sds ele;
    double score;
    struct zskiplistNode *backward;
    struct zskiplistLevel {
        struct zskiplistNode *forward;
        unsigned int span;
    } level[];
} zskiplistNode;

typedef struct zskiplist {
    struct zskiplistNode *header, *tail;
    unsigned long length;
    int level;
} zskiplist;

```



为什么不用平衡树或者哈希表

简而言之就是实现简单且达到了类似效果。



## Redis对象机制

![image-20220417171300981](D:\mdImage\image-20220417171300981.png)

![image-20220417171308006](D:\mdImage\image-20220417171308006.png)

![image-20220417171314608](D:\mdImage\image-20220417171314608.png)

![image-20220417170922043](D:\mdImage\image-20220417170922043.png)



它反映了Redis的每种对象其实都由**对象结构(redisObject)** 与 **对应编码的数据结构**组合而成，而每种对象类型对应若干编码方式，不同的编码方式所对应的底层数据结构是不同的。

所以，我们需要从几个个角度来着手底层研究：

- **对象设计机制**: 对象结构(redisObject)
- **编码类型和底层数据结构**: 对应编码的数据结构

为什么Redis会设计redisObject对象？

在redis的命令中，用于对键进行处理的命令占了很大一部分，而对于键所保存的值的类型（键的类型），键能执行的命令又各不相同。如： `LPUSH` 和 `LLEN` 只能用于列表键, 而 `SADD` 和 `SRANDMEMBER` 只能用于集合键, 等等; 另外一些命令, 比如 `DEL`、 `TTL` 和 `TYPE`, 可以用于任何类型的键；但是要正确实现这些命令, 必须为不同类型的键设置不同的处理方式: 比如说, 删除一个列表键和删除一个字符串键的操作过程就不太一样。

以上的描述说明, **Redis 必须让每个键都带有类型信息, 使得程序可以检查键的类型, 并为它选择合适的处理方式**.

比如说， **集合类型就可以由字典（dict）和整数集合（IntSet）两种不同的数据结构实现， 但是， 当用户执行 ZADD 命令时， 他/她应该不必关心集合使用的是什么编码， 只要 Redis 能按照 ZADD 命令的指示， 将新元素添加到集合就可以了**。

这说明, **操作数据类型的命令除了要对键的类型进行检查之外, 还需要根据数据类型的不同编码进行多态处理**.

为了解决以上问题, **Redis 构建了自己的类型系统**, 这个系统的主要功能包括:

- redisObject 对象.
- 基于 redisObject 对象的类型检查.
- 基于 redisObject 对象的显式多态函数.
- 对 redisObject 进行分配、共享和销毁的机制

redisObject数据结构：

redisObject 是 Redis 类型系统的核心, 数据库中的每个键、值, 以及 Redis 本身处理的参数, 都表示为这种数据类型.

```C
/*
 * Redis 对象
 */
typedef struct redisObject {

    // 类型
    unsigned type:4;

    // 编码方式
    unsigned encoding:4;

    // LRU - 24位, 记录最末一次访问时间（相对于lru_clock）; 或者 LFU（最少使用的数据：8位频率，16位访问时间）
    unsigned lru:LRU_BITS; // LRU_BITS: 24

    // 引用计数
    int refcount;

    // 指向底层数据结构实例
    void *ptr;

} robj;

```



**其中type、encoding和ptr是最重要的三个属性**。

- **type记录了对象所保存的值的类型**，它的值可能是以下常量中的一个：

```c
/*
* 对象类型
*/
#define OBJ_STRING 0 // 字符串
#define OBJ_LIST 1 // 列表
#define OBJ_SET 2 // 集合
#define OBJ_ZSET 3 // 有序集
#define OBJ_HASH 4 // 哈希表
    
```

- **encoding记录了对象所保存的值的编码**，它的值可能是以下常量中的一个：

```c
/*
* 对象编码
*/
#define OBJ_ENCODING_RAW 0     /* Raw representation */
#define OBJ_ENCODING_INT 1     /* Encoded as integer */
#define OBJ_ENCODING_HT 2      /* Encoded as hash table */
#define OBJ_ENCODING_ZIPMAP 3  /* 注意：版本2.6后不再使用. */
#define OBJ_ENCODING_LINKEDLIST 4 /* 注意：不再使用了，旧版本2.x中String的底层之一. */
#define OBJ_ENCODING_ZIPLIST 5 /* Encoded as ziplist */
#define OBJ_ENCODING_INTSET 6  /* Encoded as intset */
#define OBJ_ENCODING_SKIPLIST 7  /* Encoded as skiplist */
#define OBJ_ENCODING_EMBSTR 8  /* Embedded sds string encoding */
#define OBJ_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */
#define OBJ_ENCODING_STREAM 10 /* Encoded as a radix tree of listpacks */
```



- **ptr是一个指针，指向实际保存值的数据结构**，这个数据结构由type和encoding属性决定。举个例子， 如果一个redisObject 的type 属性为`OBJ_LIST` ， encoding 属性为`OBJ_ENCODING_QUICKLIST` ，那么这个对象就是一个Redis 列表（List)，它的值保存在一个QuickList的数据结构内，而ptr 指针就指向quicklist的对象；

下图展示了redisObject 、Redis 所有数据类型、Redis 所有编码方式以及底层数据结构之间的关系（pdai：从6.0版本中梳理而来）：

![image-20220417171641798](D:\mdImage\image-20220417171641798.png)



注意：`OBJ_ENCODING_ZIPMAP`没有出现在图中，因为在redis2.6开始，它不再是任何数据类型的底层结构(虽然还有zipmap.c的代码); `OBJ_ENCODING_LINKEDLIST`也不支持了，相关代码也删除了。

- **lru属性: 记录了对象最后一次被命令程序访问的时间**





**空转时长**：当前时间减去键的值对象的lru时间，就是该键的空转时长。Object idletime命令可以打印出给定键的空转时长

如果服务器打开了maxmemory选项，并且服务器用于回收内存的算法为volatile-lru或者allkeys-lru，那么当服务器占用的内存数超过了maxmemory选项所设置的上限值时，空转时长较高的那部分键会优先被服务器释放，从而回收内存。



### 命令的类型检查和多态



**当执行一个处理数据类型命令的时候，redis执行以下步骤**

- 根据给定的key，在数据库字典中查找和他相对应的redisObject，如果没找到，就返回NULL；
- 检查redisObject的type属性和执行命令所需的类型是否相符，如果不相符，返回类型错误；
- 根据redisObject的encoding属性所指定的编码，选择合适的操作函数来处理底层的数据结构；
- 返回数据结构的操作结果作为命令的返回值。

比如现在执行LPOP命令：

![image-20220417171802150](D:\mdImage\image-20220417171802150.png)

### 对象共享

redis一般会把一些常见的值放到一个共享对象中，这样可使程序避免了重复分配的麻烦，也节约了一些CPU时间。



**redis预分配的值对象如下**：

- 各种命令的返回值，比如成功时返回的OK，错误时返回的ERROR，命令入队事务时返回的QUEUE，等等
- 包括0 在内，小于REDIS_SHARED_INTEGERS的所有整数（REDIS_SHARED_INTEGERS的默认值是10000）

![image-20220417171844389](D:\mdImage\image-20220417171844389.png)

注意：共享对象只能被字典和双向链表这类能带有指针的数据结构使用。



**为什么redis不共享列表对象、哈希对象、集合对象、有序集合对象，只共享字符串对象**？

- 列表对象、哈希对象、集合对象、有序集合对象，本身可以包含字符串对象，复杂度较高。
- 如果共享对象是保存字符串对象，那么验证操作的复杂度为O(1)
- 如果共享对象是保存字符串值的字符串对象，那么验证操作的复杂度为O(N)
- 如果共享对象是包含多个值的对象，其中值本身又是字符串对象，即其它对象中嵌套了字符串对象，比如列表对象、哈希对象，那么验证操作的复杂度将会是O(N的平方)

如果对复杂度较高的对象创建共享对象，需要消耗很大的CPU，用这种消耗去换取内存空间，是不合适的

### 引用计数以及对象的消毁

redisObject中有refcount属性，是对象的引用计数，显然计数0那么就是可以回收。



每个redisObject结构都带有一个refcount属性，指示这个对象被引用了多少次；

当新创建一个对象时，它的refcount属性被设置为1；

当对一个对象进行共享时，redis将这个对象的refcount加一；

当使用完一个对象后，或者消除对一个对象的引用之后，程序将对象的refcount减一；

当对象的refcount降至0 时，这个RedisObject结构，以及它引用的数据结构的内存都会被释放。

著作权归https://pdai.tech所有。 链接：https://www.pdai.tech/md/db/nosql-redis/db-redis-x-redis-object.html

### 小结

- redis使用自己实现的对象机制（redisObject)来实现类型判断、命令多态和基于引用次数的垃圾回收；
- redis会预分配一些常用的数据对象，并通过共享这些对象来减少内存占用，和避免频繁的为小对象分配内存。



## redis对象与编码(底层结构)对应关系详解

### 字符串对象

字符串是Redis最基本的数据类型，不仅所有key都是字符串类型，其它几种数据类型构成的元素也是字符串。注意字符串的长度不能超过512M。

编码

字符串对象的编码可以是int，raw或者embstr。

- `int 编码`：保存的是可以用 long 类型表示的整数值。
- `embstr 编码`：保存长度小于44字节的字符串（redis3.2版本之前是39字节，之后是44字节）。
- `raw 编码`：保存长度大于44字节的字符串（redis3.2版本之前是39字节，之后是44字节）。

![image-20220417172929972](D:\mdImage\image-20220417172929972.png)

由上可以看出，int 编码是用来保存整数值，而embstr是用来保存短字符串，raw编码是用来保存长字符串。

**内存布局**

字符串对象支持三种编码方式: RAW, INT, EMBSTR, 三种方式的内存布局分别如下:

![image-20220417173043171](D:\mdImage\image-20220417173043171.png)



- **raw 和 embstr 的区别**

其实 embstr 编码是专门用来保存短字符串的一种优化编码，raw 和 embstr 的区别：

embstr与raw都使用redisObject和sds保存数据，区别在于，embstr的使用只分配一次内存空间（因此redisObject和sds是连续的），而raw需要分配两次内存空间（分别为redisObject和sds分配空间）。因此与raw相比，embstr的好处在于创建时少分配一次空间，删除时少释放一次空间，以及对象的所有数据连在一起，寻找方便。而embstr的坏处也很明显，如果字符串的长度增加需要重新分配内存时，整个redisObject和sds都需要重新分配空间，因此redis中的embstr实现为只读。

ps：**Redis中对于浮点数类型也是作为字符串保存的，在需要的时候再将其转换成浮点数类型**。

- **编码的转换**

当 int 编码保存的值不再是整数，或大小超过了long的范围时，自动转化为raw。

对于 embstr 编码，由于 Redis 没有对其编写任何的修改程序（embstr 是只读的），在对embstr对象进行修改时，都会先转化为raw再进行修改，因此，只要是修改embstr对象，修改后的对象一定是raw的，无论是否达到了44个字节



### 列表对象

list 列表，它是简单的字符串列表，按照插入顺序排序，你可以添加一个元素到列表的头部（左边）或者尾部（右边），它的底层实际上是个链表结构。

- **编码**

列表对象的编码是quicklist。 (之前版本中有linked和ziplist这两种编码。进一步的, 目前Redis定义的10个对象编码方式宏名中, 有两个被完全闲置了, 分别是: `OBJ_ENCODING_ZIPMAP`与`OBJ_ENCODING_LINKEDLIST`。 从Redis的演进历史上来看, 前者是后续可能会得到支持的编码值（代码还在）, 后者则应该是被彻底淘汰了)

- **内存布局**

列表对象的内存布局如下图所示:

![image-20220417173421427](D:\mdImage\image-20220417173421427.png)

### 哈希对象

哈希对象的键是一个字符串类型，值是一个键值对集合。

- **编码**

哈希对象的编码可以是 ziplist 或者 hashtable；对应的底层实现有两种, 一种是ziplist, 一种是dict。

两种编码**内存布局**分别如下:

![image-20220417173615801](D:\mdImage\image-20220417173615801.png)



上图中不严谨的地方有:

1. ziplist中每个entry, 除了键与值本身的二进制数据, 还包括其它字段, 图中没有画出来
2. dict底层可能持有两个dictht实例
3. 没有画出dict的哈希冲突

需要注意的是: 当采用HT编码, 即使用dict作为哈希对象的底层数据结构时, 键与值均是以sds的形式存储的.





- **编码转换**

和上面列表对象使用 ziplist 编码一样，当同时满足下面两个条件时，使用ziplist（压缩列表）编码：

1、列表保存元素个数小于512个

2、每个元素长度小于64字节

不能满足这两个条件的时候使用 hashtable 编码。以上两个条件也可以通过Redis配置文件`zset-max-ziplist-entries` 选项和 `zset-max-ziplist-value` 进行修改。

### 集合对象

集合对象 set 是 string 类型（整数也会转换成string类型进行存储）的无序集合。注意集合和列表的区别：集合中的元素是无序的，因此不能通过索引来操作元素；集合中的元素不能有重复。



- **编码**

集合对象的编码可以是 intset 或者 hashtable; 底层实现有两种, 分别是intset和dict。 显然当使用intset作为底层实现的数据结构时, 集合中存储的只能是数值数据, 且必须是整数; 而当使用dict作为集合对象的底层实现时, 是将数据全部存储于dict的键中, 值字段闲置不用.

集合对象的内存布局如下图所示:

![image-20220417173846281](D:\mdImage\image-20220417173846281.png)



- **编码转换**

当集合同时满足以下两个条件时，使用 intset 编码：

1、集合对象中所有元素都是整数

2、集合对象所有元素数量不超过512

不能满足这两个条件的就使用 hashtable 编码。第二个条件可以通过配置文件的 `set-max-intset-entries` 进行配置。

### 有序集合对象

和上面的集合对象相比，有序集合对象是有序的。与列表使用索引下标作为排序依据不同，有序集合为每个元素设置一个分数（score）作为排序依据。

- **编码**

有序集合的底层实现依然有两种, 一种是使用ziplist作为底层实现, 另外一种比较特殊, 底层使用了两种数据结构: dict与skiplist. 前者对应的编码值宏为ZIPLIST, 后者对应的编码值宏为SKIPLIST

使用ziplist来实现在序集合很容易理解, 只需要在ziplist这个数据结构的基础上做好排序与去重就可以了. 使用zskiplist来实现有序集合也很容易理解, Redis中实现的这个跳跃表似乎天然就是为了实现有序集合对象而实现的, 那么为什么还要辅助一个dict实例呢? 我们先看来有序集合对象在这两种编码方式下的内存布局, 然后再做解释:

首先是编码为ZIPLIST时, 有序集合的内存布局如下:

![image-20220417174007971](D:\mdImage\image-20220417174007971.png)

然后是编码为SKIPLIST时, 有序集合的内存布局如下:

![image-20220417174027241](D:\mdImage\image-20220417174027241.png)



说明：其实有序集合单独使用字典或跳跃表其中一种数据结构都可以实现，但是这里使用两种数据结构组合起来，原因是假如我们单独使用 字典，虽然能以 O(1) 的时间复杂度查找成员的分值，但是因为字典是以无序的方式来保存集合元素，所以每次进行范围操作的时候都要进行排序；假如我们单独使用跳跃表来实现，虽然能执行范围操作，但是查找操作有 O(1)的复杂度变为了O(logN)。因此Redis使用了两种数据结构来共同实现有序集合。

- **编码转换**

当有序集合对象同时满足以下两个条件时，对象使用 ziplist 编码：

1、保存的元素数量小于128；

2、保存的所有元素长度都小于64字节。

不能满足上面两个条件的使用 skiplist 编码。以上两个条件也可以通过Redis配置文件`zset-max-ziplist-entries` 选项和 `zset-max-ziplist-value` 进行修改。



## Redis持久化

为了防止数据丢失以及服务重启时能够恢复数据，Redis支持数据的持久化，主要分为两种方式，分别是RDB和AOF; 当然实际场景下还会使用这两种的混合模式。



- **为什么需要持久化**？

**Redis是个基于内存的数据库。那服务一旦宕机，内存中的数据将全部丢失。**通常的解决方案是从后端数据库恢复这些数据，但后端数据库有性能瓶颈，如果是大数据量的恢复，1、会对数据库带来巨大的压力，2、数据库的性能不如Redis。导致程序响应慢。所以对Redis来说，实现数据的持久化，避免从后端数据库中恢复数据，是至关重要的。



- **Redis持久化有哪些方式呢**？**为什么我们需要重点学RDB和AOF**？

从严格意义上说，Redis服务提供四种持久化存储方案：`RDB`、`AOF`、`虚拟内存（VM）`和　`DISKSTORE`。虚拟内存（VM）方式，从Redis Version 2.4开始就被官方明确表示不再建议使用，Version 3.2版本中更找不到关于虚拟内存（VM）的任何配置范例，Redis的主要作者Salvatore Sanfilippo还专门写了一篇论文，来反思Redis对虚拟内存（VM）存储技术的支持问题。

至于DISKSTORE方式，是从Redis Version 2.8版本开始提出的一个存储设想，到目前为止Redis官方也没有在任何stable版本中明确建议使用这用方式。在Version 3.2版本中同样找不到对于这种存储方式的明确支持。从网络上能够收集到的各种资料来看，DISKSTORE方式和RDB方式还有着一些千丝万缕的联系，不过各位读者也知道，除了官方文档以外网络资料很多就是大抄。

最关键的是目前官方文档上能够看到的Redis对持久化存储的支持明确的就只有两种方案（https://redis.io/topics/persistence）：RDB和AOF。

### RDB 持久化



> RDB 就是 Redis DataBase 的缩写，中文名为快照/内存快照，RDB持久化是把当前进程数据生成快照保存到磁盘上的过程，由于是某一时刻的快照，那么快照中的值要早于或者等于内存中的值。

#### 触发方式

> 触发rdb持久化的方式有2种，分别是**手动触发**和**自动触发**。

##### 手动触发

> 手动触发分别对应save和bgsave命令

- **save命令**：阻塞当前Redis服务器，直到RDB过程完成为止，对于内存 比较大的实例会造成长时间**阻塞**，线上环境不建议使用
- **bgsave命令**：Redis进程执行fork操作创建子进程，RDB持久化过程由子 进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短

bgsave流程图如下所示

![image-20220417174514938](D:\mdImage\image-20220417174514938.png)



具体流程如下：

- redis客户端执行bgsave命令或者自动触发bgsave命令；
- 主进程判断当前是否已经存在正在执行的子进程，如果存在，那么主进程直接返回；
- 如果不存在正在执行的子进程，那么就fork一个新的子进程进行持久化数据，fork过程是阻塞的，fork操作完成后主进程即可执行其他操作；
- 子进程先将数据写入到临时的rdb文件中，待快照数据写入完成后再原子替换旧的rdb文件；
- 同时发送信号给主进程，通知主进程rdb持久化完成，主进程更新相关的统计信息（info Persitence下的rdb_*相关选项）



##### 自动触发

> 在以下4种情况时会自动触发

- redis.conf中配置`save m n`，即在m秒内有n次修改时，自动触发bgsave生成rdb文件；
- 主从复制时，从节点要从主节点进行全量复制时也会触发bgsave操作，生成当时的快照发送到从节点；
- 执行debug reload命令重新加载redis时也会触发bgsave操作；
- 默认情况下执行shutdown命令时，如果没有开启aof持久化，那么也会触发bgsave操作；

#### redis.conf中配置RDB

**快照周期**：内存快照虽然可以通过技术人员手动执行SAVE或BGSAVE命令来进行，但生产环境下多数情况都会设置其周期性执行条件。

- **Redis中默认的周期新设置**

```C
# 周期性执行条件的设置格式为
save <seconds> <changes>

# 默认的设置为：
save 900 1
save 300 10
save 60 10000

# 以下设置方式为关闭RDB快照功能
save ""

```



以上三项默认信息设置代表的意义是：

- 如果900秒内有1条Key信息发生变化，则进行快照；
- 如果300秒内有10条Key信息发生变化，则进行快照；
- 如果60秒内有10000条Key信息发生变化，则进行快照。读者可以按照这个规则，根据自己的实际请求压力进行设置调整。
- **其它相关配置**

```C
# 文件名称
dbfilename dump.rdb

# 文件保存路径
dir /home/work/app/redis/data/

# 如果持久化出错，主进程是否停止写入
stop-writes-on-bgsave-error yes

# 是否压缩
rdbcompression yes

# 导入时是否检查
rdbchecksum yes

```



`dbfilename`：RDB文件在磁盘上的名称。

`dir`：RDB文件的存储路径。默认设置为“./”，也就是Redis服务的主目录。

`stop-writes-on-bgsave-error`：上文提到的在快照进行过程中，主进程照样可以接受客户端的任何写操作的特性，是指在快照操作正常的情况下。如果快照操作出现异常（例如操作系统用户权限不够、磁盘空间写满等等）时，Redis就会禁止写操作。这个特性的主要目的是使运维人员在第一时间就发现Redis的运行错误，并进行解决。一些特定的场景下，您可能需要对这个特性进行配置，这时就可以调整这个参数项。该参数项默认情况下值为yes，如果要关闭这个特性，指定即使出现快照错误Redis一样允许写操作，则可以将该值更改为no。

`rdbcompression`：该属性将在字符串类型的数据被快照到磁盘文件时，启用LZF压缩算法。Redis官方的建议是请保持该选项设置为yes，因为“it’s almost always a win”。

`rdbchecksum`：从RDB快照功能的version 5 版本开始，一个64位的CRC冗余校验编码会被放置在RDB文件的末尾，以便对整个RDB文件的完整性进行验证。这个功能大概会多损失10%左右的性能，但获得了更高的数据可靠性。所以如果您的Redis服务需要追求极致的性能，就可以将这个选项设置为no。



#### RDB 更深入理解



+ **由于生产环境中我们为Redis开辟的内存区域都比较大（例如6GB），那么将内存中的数据同步到硬盘的过程可能就会持续比较长的时间，而实际情况是这段时间Redis服务一般都会收到数据写操作请求。那么如何保证数据一致性呢**？
+ **在进行快照操作的这段时间，如果发生服务崩溃怎么办**？
+ **可以每秒做一次快照吗**？

著作权归https://pdai.tech所有。 链接：https://www.pdai.tech/md/db/nosql-redis/db-redis-x-rdb-aof.html

#### RDB优缺点

- **优点**
  - RDB文件是某个时间节点的快照，默认使用LZF算法进行压缩，压缩后的文件体积远远小于内存大小，适用于备份、全量复制等场景；
  - Redis加载RDB文件恢复数据要远远快于AOF方式；
- **缺点**
  - RDB方式实时性不够，无法做到秒级的持久化；
  - 每次调用bgsave都需要fork子进程，fork子进程属于重量级操作，频繁执行成本较高；
  - RDB文件是二进制的，没有可读性，AOF文件在了解其结构的情况下可以手动修改或者补全；
  - 版本兼容RDB文件问题；

针对RDB不适合实时持久化的问题，Redis提供了AOF持久化方式来解决

### AOF 持久化

Redis是“写后”日志，Redis先执行命令，把数据写入内存，然后才记录日志。日志里记录的是Redis收到的每一条命令，这些命令是以文本形式保存。PS: 大多数的数据库采用的是写前日志（WAL），例如MySQL，通过写前日志和两阶段提交，实现数据和逻辑的一致性。

而AOF日志采用写后日志，即**先写内存，后写日志**。

![image-20220417191620171](D:\mdImage\image-20220417191620171.png)



**为什么采用写后日志**？

Redis要求高性能，采用写日志有两方面好处：

- **避免额外的检查开销**：Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。
- 不会阻塞当前的写操作

但这种方式存在潜在风险：

- 如果命令执行完成，写日志之前宕机了，会丢失数据。
- 主线程写磁盘压力大，导致写盘慢，阻塞后续操作。



#### 如何实现AOF

AOF日志记录Redis的每个写命令，步骤分为：命令追加（append）、文件写入（write）和文件同步（sync）。

- **命令追加** 当AOF持久化功能打开了，服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器的 aof_buf 缓冲区。

- **文件写入和同步** 关于何时将 aof_buf 缓冲区的内容写入AOF文件中，Redis提供了三种写回策略：

  ![image-20220417191737364](D:\mdImage\image-20220417191737364.png)



`Always`，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；

`Everysec`，每秒写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；

`No`，操作系统控制的写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。

- **三种写回策略的优缺点**

上面的三种写回策略体现了一个重要原则：**trade-off**，取舍，指在性能和可靠性保证之间做取舍。



#### 深入理解AOF重写

Redis通过创建一个新的AOF文件来替换现有的AOF，新旧两个AOF文件保存的数据相同，但新AOF文件没有了冗余命令。

![image-20220417191858078](D:\mdImage\image-20220417191858078.png)





- **AOF重写会阻塞吗**？

AOF重写过程是由后台进程bgrewriteaof来完成的。主线程fork出后台的bgrewriteaof子进程，fork会把主线程的内存拷贝一份给bgrewriteaof子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。

所以aof在重写时，在fork进程时是会阻塞住主线程的。

- **AOF日志何时会重写**？

有两个配置项控制AOF重写的触发：

`auto-aof-rewrite-min-size`:表示运行AOF重写时文件的最小大小，默认为64MB。

`auto-aof-rewrite-percentage`:这个值的计算方式是，当前aof文件大小和上一次重写后aof文件大小的差值，再除以上一次重写后aof文件大小。也就是当前aof文件比上一次重写后aof文件的增量大小，和上一次重写后aof文件大小的比值。

- **重写日志时，有新数据写入咋整**？

重写过程总结为：“一个拷贝，两处日志”。在fork出子进程时的拷贝，以及在重写时，如果有新数据写入，主线程就会将命令记录到两个aof日志内存缓冲区中。如果AOF写回策略配置的是always，则直接将命令写回旧的日志文件，并且保存一份命令至AOF重写缓冲区，这些操作对新的日志文件是不存在影响的。（旧的日志文件：主线程使用的日志文件，新的日志文件：bgrewriteaof进程使用的日志文件）

而在bgrewriteaof子进程完成会日志文件的重写操作后，会提示主线程已经完成重写操作，主线程会将AOF重写缓冲中的命令追加到新的日志文件后面。这时候在高并发的情况下，AOF重写缓冲区积累可能会很大，这样就会造成阻塞，Redis后来通过Linux管道技术让aof重写期间就能同时进行回放，这样aof重写结束后只需回放少量剩余的数据即可。

最后通过修改文件名的方式，保证文件切换的原子性。

在AOF重写日志期间发生宕机的话，因为日志文件还没切换，所以恢复数据时，用的还是旧的日志文件。

**总结操作**：

- 主线程fork出子进程重写aof日志
- 子进程重写日志完成后，主线程追加aof日志缓冲
- 替换日志文件



温馨提示

这里的进程和线程的概念有点混乱。因为后台的bgreweiteaof进程就只有一个线程在操作，而主线程是Redis的操作进程，也是单独一个线程。这里想表达的是Redis主进程在fork出一个后台进程之后，后台进程的操作和主进程是没有任何关联的，也不会阻塞主线程。

![image-20220417192023621](D:\mdImage\image-20220417192023621.png)



### RDB和AOF混合方式（4.0版本)



### 从持久化中恢复数据

数据的备份、持久化做完了，我们如何从这些持久化文件中恢复数据呢？如果一台服务器上有既有RDB文件，又有AOF文件，该加载谁呢？

![image-20220417192151861](D:\mdImage\image-20220417192151861.png)



- redis重启时判断是否开启aof，如果开启了aof，那么就优先加载aof文件；
- 如果aof存在，那么就去加载aof文件，加载成功的话redis重启成功，如果aof文件加载失败，那么会打印日志表示启动失败，此时可以去修复aof文件后重新启动；
- 若aof文件不存在，那么redis就会转而去加载rdb文件，如果rdb文件不存在，redis直接启动成功；
- 如果rdb文件存在就会去加载rdb文件恢复数据，如加载失败则打印日志提示启动失败，如加载成功，那么redis重启成功，且使用rdb文件恢复数据；

那么为什么会优先加载AOF呢？因为AOF保存的数据更完整，通过上面的分析我们知道AOF基本上最多损失1s的数据



## 消息传递：发布订阅模式详解

Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。

### Redis发布订阅简介

Redis 的 SUBSCRIBE 命令可以让客户端订阅任意数量的频道， 每当有新信息发送到被订阅的频道时， 信息就会被发送给所有订阅指定频道的客户端。

作为例子， 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系：

![image-20220417193432638](D:\mdImage\image-20220417193432638.png)

当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端：

![image-20220417193449017](D:\mdImage\image-20220417193449017.png)



### 发布/订阅使用

Redis有两种发布/订阅模式：

- 基于频道(Channel)的发布/订阅
- 基于模式(pattern)的发布/订阅



#### 基于频道(Channel)的发布/订阅

"发布/订阅"模式包含两种角色，分别是发布者和订阅者。发布者可以向指定的频道(channel)发送消息; 订阅者可以订阅一个或者多个频道(channel),所有订阅此频道的订阅者都会收到此消息。

![image-20220417193931129](D:\mdImage\image-20220417193931129.png)

- **发布者发布消息**

发布者发布消息的命令是 publish,用法是 publish channel message，如向 channel1.1说一声hi

这样消息就发出去了。返回值表示接收这条消息的订阅者数量。发出去的消息不会被持久化，也就是有客户端订阅channel:1后只能接收到后续发布到该频道的消息，之前的就接收不到了。



- **订阅者订阅频道**

订阅频道的命令是 subscribe，可以同时订阅多个频道，用法是 subscribe channel1 [channel2 ...],例如新开一个客户端订阅上面频道:(不会收到消息，因为不会收到订阅之前就发布到该频道的消息)

```
127.0.0.1:6379> subscribe channel:1
Reading messages... (press Ctrl-C to quit)
1) "subscribe" // 消息类型
2) "channel:1" // 频道
3) "hi" // 消息内容

```



执行上面命令客户端会进入订阅状态，处于此状态下客户端不能使用除`subscribe`、`unsubscribe`、`psubscribe`和`punsubscribe`这四个属于"发布/订阅"之外的命令，否则会报错。

进入订阅状态后客户端可能收到3种类型的回复。每种类型的回复都包含3个值，第一个值是消息的类型，根据消类型的不同，第二个和第三个参数的含义可能不同。

消息类型的取值可能是以下3个:

- subscribe。表示订阅成功的反馈信息。第二个值是订阅成功的频道名称，第三个是当前客户端订阅的频道数量。
- message。表示接收到的消息，第二个值表示产生消息的频道名称，第三个值是消息的内容。
- unsubscribe。表示成功取消订阅某个频道。第二个值是对应的频道名称，第三个值是当前客户端订阅的频道数量，当此值为0时客户端会退出订阅状态，之后就可以执行其他非"发布/订阅"模式的命令了。

![image-20220417193835843](D:\mdImage\image-20220417193835843.png)





#### 基于模式(pattern)的发布/订阅

如果有某个/某些模式和这个频道匹配的话，那么所有订阅这个/这些频道的客户端也同样会收到信息。

- **用图例解释什么是基于模式的发布订阅**

下图展示了一个带有频道和模式的例子， 其中 tweet.shop.* 模式匹配了 tweet.shop.kindle 频道和 tweet.shop.ipad 频道， 并且有不同的客户端分别订阅它们三个：

![image-20220417194049507](D:\mdImage\image-20220417194049507.png)



当有信息发送到 tweet.shop.kindle 频道时， 信息除了发送给 clientX 和 clientY 之外， 还会发送给订阅 tweet.shop.* 模式的 client123 和 client256 ：

![image-20220417194110769](D:\mdImage\image-20220417194110769.png)

另一方面， 如果接收到信息的是频道 tweet.shop.ipad ， 那么 client123 和 client256 同样会收到信息：

![image-20220417194128727](D:\mdImage\image-20220417194128727.png)

- **基于模式的例子**

通配符中?表示1个占位符，*表示任意个占位符(包括0)，?*表示1个以上占位符。

publish发布

```
127.0.0.1:6379> publish c m1
(integer) 0
127.0.0.1:6379> publish c1 m1
(integer) 1
127.0.0.1:6379> publish c11 m1
(integer) 0
127.0.0.1:6379> publish b m1
(integer) 1
127.0.0.1:6379> publish b1 m1
(integer) 1
127.0.0.1:6379> publish b11 m1
(integer) 1
127.0.0.1:6379> publish d m1
(integer) 0
127.0.0.1:6379> publish d1 m1
(integer) 1
127.0.0.1:6379> publish d11 m1
(integer) 1

```

subscribe订阅

```
127.0.0.1:6379> psubscribe c? b* d?*
Reading messages... (press Ctrl-C to quit)
1) "psubscribe"
2) "c?"
3) (integer) 1
1) "psubscribe"
2) "b*"
3) (integer) 2
1) "psubscribe"
2) "d?*"
3) (integer) 3
1) "pmessage"
2) "c?"
3) "c1"
4) "m1"
1) "pmessage"
2) "b*"
3) "b"
4) "m1"
1) "pmessage"
2) "b*"
3) "b1"
4) "m1"
1) "pmessage"
2) "b*"
3) "b11"
4) "m1"
1) "pmessage"
2) "d?*"
3) "d1"
4) "m1"
1) "pmessage"
2) "d?*"
3) "d11"
4) "m1"

```



- **注意点**

(1)使用psubscribe命令可以重复订阅同一个频道，如客户端执行了`psubscribe c? c?*`。这时向c1发布消息客户端会接受到两条消息，而同时publish命令的返回值是2而不是1。同样的，如果有另一个客户端执行了`subscribe c1` 和`psubscribe c?*`的话，向c1发送一条消息该客户顿也会受到两条消息(但是是两种类型:message和pmessage)，同时publish命令也返回2.

(2)punsubscribe命令可以退订指定的规则，用法是: `punsubscribe [pattern [pattern ...]]`,如果没有参数则会退订所有规则。

(3)使用punsubscribe只能退订通过psubscribe命令订阅的规则，不会影响直接通过subscribe命令订阅的频道；同样unsubscribe命令也不会影响通过psubscribe命令订阅的规则。另外需要注意punsubscribe命令退订某个规则时不会将其中的通配符展开，而是进行严格的字符串匹配，所以`punsubscribe *` 无法退订`c*`规则，而是必须使用`punsubscribe c*`才可以退订。（它们是相互独立的，后文可以看到数据结构上看也是两种实现）

### 基于频道(Channel)的发布/订阅如何实现的？

底层是通过**字典**（图中的pubsub_channels）实现的，这个字典就用于保存订阅频道的信息：字典的键为正在被订阅的频道， 而字典的值则是一个链表， 链表中保存了所有订阅这个频道的客户端。



- **数据结构**

比如说，在下图展示的这个 pubsub_channels 示例中， client2 、 client5 和 client1 就订阅了 channel1 ， 而其他频道也分别被别的客户端所订阅：

![image-20220417194303670](D:\mdImage\image-20220417194303670.png)



- **订阅**

当客户端调用 SUBSCRIBE 命令时， 程序就将客户端和要订阅的频道在 pubsub_channels 字典中关联起来。

举个例子，如果客户端 client10086 执行命令 `SUBSCRIBE channel1 channel2 channel3` ，那么前面展示的 pubsub_channels 将变成下面这个样子：

![image-20220417194319154](D:\mdImage\image-20220417194319154.png)



- **发布**

当调用 `PUBLISH channel message` 命令， 程序首先根据 channel 定位到字典的键， 然后将信息发送给字典值链表中的所有客户端。

比如说，对于以下这个 pubsub_channels 实例， 如果某个客户端执行命令 `PUBLISH channel1 "hello moto"` ，那么 client2 、 client5 和 client1 三个客户端都将接收到 "hello moto" 信息：

- **退订**

使用 UNSUBSCRIBE 命令可以退订指定的频道， 这个命令执行的是订阅的反操作： 它从 pubsub_channels 字典的给定频道（键）中， 删除关于当前客户端的信息， 这样被退订频道的信息就不会再发送给这个客户端。



### 基于模式(Pattern)的发布/订阅如何实现的？

底层是pubsubPattern节点的链表。

- **数据结构** redisServer.pubsub_patterns 属性是一个链表，链表中保存着所有和模式相关的信息：

```
struct redisServer {
    // ...
    list *pubsub_patterns;
    // ...
};

```

链表中的每个节点都包含一个 redis.h/pubsubPattern 结构：

```
typedef struct pubsubPattern {
    redisClient *client;
    robj *pattern;
} pubsubPattern;
```



client 属性保存着订阅模式的客户端，而 pattern 属性则保存着被订阅的模式。

每当调用 PSUBSCRIBE 命令订阅一个模式时， 程序就创建一个包含客户端信息和被订阅模式的 pubsubPattern 结构， 并将该结构添加到 redisServer.pubsub_patterns 链表中。

作为例子，下图展示了一个包含两个模式的 pubsub_patterns 链表， 其中 client123 和 client256 都正在订阅 tweet.shop.* 模式：

![image-20220417194417368](D:\mdImage\image-20220417194417368.png)

- **订阅**

如果这时客户端 client10086 执行 `PSUBSCRIBE broadcast.list.*` ， 那么 pubsub_patterns 链表将被更新成这样：

![image-20220417194430263](D:\mdImage\image-20220417194430263.png)



通过遍历整个 pubsub_patterns 链表，程序可以检查所有正在被订阅的模式，以及订阅这些模式的客户端。

- **发布**

发送信息到模式的工作也是由 PUBLISH 命令进行的, 显然就是匹配模式获得Channels，然后再把消息发给客户端。

- **退订**

使用 PUNSUBSCRIBE 命令可以退订指定的模式， 这个命令执行的是订阅模式的反操作： 程序会删除 redisServer.pubsub_patterns 链表中， 所有和被退订模式相关联的 pubsubPattern 结构， 这样客户端就不会再收到和模式相匹配的频道发来的信息。

### SpringBoot结合Redis发布/订阅实例？

```
// 发布
redisTemplate.convertAndSend("my_topic_name", "message_content");

// 配置订阅
RedisMessageListenerContainer container = new RedisMessageListenerContainer();
container.setConnectionFactory(connectionFactory);
container.addMessageListener(xxxMessageListenerAdapter, "my_topic_name");

```



## 事件：Redis事件机制详解

Redis 采用事件驱动机制来处理大量的网络IO。它并没有使用 libevent 或者 libev 这样的成熟开源方案，而是自己实现一个非常简洁的事件驱动库。

### 事件机制

Redis中的事件驱动库只关注网络IO，以及定时器。

该事件库处理下面两类事件：

- **文件事件**(file  event)：用于处理 Redis 服务器和客户端之间的网络IO。
- **时间事件**(time  eveat)：Redis 服务器中的一些操作（比如serverCron函数）需要在给定的时间点执行，而时间事件就是处理这类定时操作的。

事件驱动库的代码主要是在`src/ae.c`中实现的，其示意图如下所示。

![image-20220417195243250](D:\mdImage\image-20220417195243250.png)

`aeEventLoop`是整个事件驱动的核心，它管理着文件事件表和时间事件列表，不断地循环处理着就绪的文件事件和到期的时间事件。



#### 文件事件

> Redis基于**Reactor模式**开发了自己的网络事件处理器，也就是文件事件处理器。文件事件处理器使用**IO多路复用技术**（建议先看下 [Java IO多路复用详解]() ），同时监听多个套接字，并为套接字关联不同的事件处理函数。当套接字的可读或者可写事件触发时，就会调用相应的事件处理函数。



- **1. 为什么单线程的 Redis 能那么快**？

Redis的瓶颈主要在IO而不是CPU，所以为了省开发量，在6.0版本前是单线程模型；其次，Redis 是单线程主要是指 **Redis 的网络 IO 和键值对读写是由一个线程来完成的**，这也是 Redis 对外提供键值存储服务的主要流程。（但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的）。

Redis 采用了多路复用机制使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率。

- **2. Redis事件响应框架ae_event及文件事件处理器**

Redis并没有使用 libevent 或者 libev 这样的成熟开源方案，而是自己实现一个非常简洁的事件驱动库 ae_event。

Redis 使用的IO多路复用技术主要有：`select`、`epoll`、`evport`和`kqueue`等。每个IO多路复用函数库在 Redis 源码中都对应一个单独的文件，比如`ae_select.c`，`ae_epoll.c`， `ae_kqueue.c`等。Redis 会根据不同的操作系统，按照不同的优先级选择多路复用技术。事件响应框架一般都采用该架构，比如 netty 和 libevent。

![image-20220417195446725](D:\mdImage\image-20220417195446725.png)

如下图所示，文件事件处理器有四个组成部分，它们分别是套接字、I/O多路复用程序、文件事件分派器以及事件处理器。

![image-20220417195502338](D:\mdImage\image-20220417195502338.png)



文件事件是对套接字操作的抽象，每当一个套接字准备好执行 `accept`、`read`、`write`和 `close` 等操作时，就会产生一个文件事件。因为 Redis 通常会连接多个套接字，所以多个文件事件有可能并发的出现。

I/O多路复用程序负责监听多个套接字，并向文件事件派发器传递那些产生了事件的套接字。

尽管多个文件事件可能会并发地出现，但I/O多路复用程序总是会将所有产生的套接字都放到同一个队列(也就是后文中描述的aeEventLoop的fired就绪事件表)里边，然后文件事件处理器会以有序、同步、单个套接字的方式处理该队列中的套接字，也就是处理就绪的文件事件。

![image-20220417195610641](D:\mdImage\image-20220417195610641.png)



所以，一次 Redis 客户端与服务器进行连接并且发送命令的过程如上图所示。

- 客户端向服务端发起**建立 socket 连接的请求**，那么监听套接字将产生 AE_READABLE 事件，触发连接应答处理器执行。处理器会对客户端的连接请求
- 进行**应答**，然后创建客户端套接字，以及客户端状态，并将客户端套接字的 AE_READABLE 事件与命令请求处理器关联。
- 客户端建立连接后，向服务器**发送命令**，那么客户端套接字将产生 AE_READABLE 事件，触发命令请求处理器执行，处理器读取客户端命令，然后传递给相关程序去执行。
- **执行命令获得相应的命令回复**，为了将命令回复传递给客户端，服务器将客户端套接字的 AE_WRITEABLE 事件与命令回复处理器关联。当客户端试图读取命令回复时，客户端套接字产生 AE_WRITEABLE 事件，触发命令回复处理器将命令回复全部写入到套接字中。

**3. Redis IO多路复用模型**

在 Redis 只运行单线程的情况下，**该机制允许内核中，同时存在多个监听套接字和已连接套接字**。内核会一直监听这些套接字上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。

下图就是基于多路复用的 Redis IO 模型。图中的多个 FD 就是刚才所说的多个套接字。Redis 网络框架调用 epoll 机制，让内核监听这些套接字。此时，Redis 线程不会阻塞在某一个特定的监听或已连接套接字上，也就是说，不会阻塞在某一个特定的客户端请求处理上。正因为此，Redis 可以同时和多个客户端连接并处理请求，从而提升并发性。

![image-20220417195824802](D:\mdImage\image-20220417195824802.png)



基于多路复用的Redis高性能IO模型为了在请求到达时能通知到 Redis 线程，select/epoll 提供了基于事件的回调机制，即针对不同事件的发生，调用相应的处理函数。那么，回调机制是怎么工作的呢？

其实，select/epoll 一旦监测到 FD 上有请求到达时，就会触发相应的事件。这些事件会被放进一个事件队列，Redis 单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时，Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升 Redis 的响应性能。

为了方便你理解，我再以连接请求和读数据请求为例，具体解释一下。

这两个请求分别对应 Accept 事件和 Read 事件，Redis 分别对这两个事件注册 accept 和 get 回调函数。当 Linux 内核监听到有连接请求或读数据请求时，就会触发 Accept 事件和 Read 事件，此时，内核就会回调 Redis 相应的 accept 和 get 函数进行处理。

这就像病人去医院瞧病。在医生实际诊断前，每个病人（等同于请求）都需要先分诊、测体温、登记等。如果这些工作都由医生来完成，医生的工作效率就会很低。所以，医院都设置了分诊台，分诊台会一直处理这些诊断前的工作（类似于 Linux 内核监听请求），然后再转交给医生做实际诊断。这样即使一个医生（相当于 Redis 单线程），效率也能提升。



著作权归https://pdai.tech所有。 链接：https://www.pdai.tech/md/db/nosql-redis/db-redis-x-event.html

#### 时间事件

> Redis 的时间事件分为以下两类：

- **定时事件**：让一段程序在指定的时间之后执行一次。
- **周期性事件**：让一段程序每隔指定时间就执行一次。

Redis 的时间事件的具体定义结构如下所示。

```c
typedef struct aeTimeEvent {
    /* 全局唯一ID */
    long long id; /* time event identifier. */
    /* 秒精确的UNIX时间戳，记录时间事件到达的时间*/
    long when_sec; /* seconds */
    /* 毫秒精确的UNIX时间戳，记录时间事件到达的时间*/
    long when_ms; /* milliseconds */
    /* 时间处理器 */
    aeTimeProc *timeProc;
    /* 事件结束回调函数，析构一些资源*/
    aeEventFinalizerProc *finalizerProc;
    /* 私有数据 */
    void *clientData;
    /* 前驱节点 */
    struct aeTimeEvent *prev;
    /* 后继节点 */
    struct aeTimeEvent *next;
} aeTimeEvent;
   
```



一个时间事件是定时事件还是周期性事件取决于时间处理器的返回值：

- 如果返回值是 `AE_NOMORE`，那么这个事件是一个定时事件，该事件在达到后删除，之后不会再重复。
- 如果返回值是非 `AE_NOMORE` 的值，那么这个事件为周期性事件，当一个时间事件到达后，服务器会根据时间处理器的返回值，对时间事件的 when 属性进行更新，让这个事件在一段时间后再次达到。

![image-20220417200020649](D:\mdImage\image-20220417200020649.png)



服务器所有的时间事件都放在一个无序链表中，每当时间事件执行器运行时，它就遍历整个链表，查找所有已到达的时间事件，并调用相应的事件处理器。正常模式下的Redis服务器只使用serverCron一个时间事件，而在benchmark模式下，服务器也只使用两个时间事件，所以不影响事件执行的性能。

### aeEventLoop的具体实现



## 事务：Redis事务详解

Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。

### 什么是Redis事务

Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。

总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。

### Redis事务相关命令和使用

> MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。

MULTI ：开启事务，redis会将后续的命令逐个放入队列中，然后使用EXEC命令来原子化执行这个命令系列。

EXEC：执行事务中的所有操作命令。

DISCARD：取消事务，放弃执行事务块中的所有命令。

WATCH：监视一个或多个key,如果事务在执行前，这个key(或多个key)被其他命令修改，则事务被中断，不会执行事务中的任何命令。

UNWATCH：取消WATCH对所有key的监视。

#### 标准的事务执行

给k1、k2分别赋值，在事务中修改k1、k2，执行事务后，查看k1、k2值都被修改。

```C
127.0.0.1:6379> set k1 v1
OK
127.0.0.1:6379> set k2 v2
OK
127.0.0.1:6379> MULTI
OK
127.0.0.1:6379> set k1 11
QUEUED
127.0.0.1:6379> set k2 22
QUEUED
127.0.0.1:6379> EXEC
1) OK
2) OK
127.0.0.1:6379> get k1
"11"
127.0.0.1:6379> get k2
"22"
127.0.0.1:6379>

```

#### 事务取消

```C
127.0.0.1:6379> MULTI
OK
127.0.0.1:6379> set k1 33
QUEUED
127.0.0.1:6379> set k2 34
QUEUED
127.0.0.1:6379> DISCARD
OK

```

#### 事务出现错误的处理

- **语法错误（编译器错误）**

在开启事务后，修改k1值为11，k2值为22，但k2语法错误，最终导致事务提交失败，k1、k2保留原值。

```C
127.0.0.1:6379> set k1 v1
OK
127.0.0.1:6379> set k2 v2
OK
127.0.0.1:6379> MULTI
OK
127.0.0.1:6379> set k1 11
QUEUED
127.0.0.1:6379> sets k2 22
(error) ERR unknown command `sets`, with args beginning with: `k2`, `22`, 
127.0.0.1:6379> exec
(error) EXECABORT Transaction discarded because of previous errors.
127.0.0.1:6379> get k1
"v1"
127.0.0.1:6379> get k2
"v2"
127.0.0.1:6379>

```

- **Redis类型错误（运行时错误）**

在开启事务后，修改k1值为11，k2值为22，但将k2的类型作为List，在运行时检测类型错误，最终导致事务提交失败，此时事务并没有回滚，而是跳过错误命令继续执行， 结果k1值改变、k2保留原值

```C
127.0.0.1:6379> set k1 v1
OK
127.0.0.1:6379> set k1 v2
OK
127.0.0.1:6379> MULTI
OK
127.0.0.1:6379> set k1 11
QUEUED
127.0.0.1:6379> lpush k2 22
QUEUED
127.0.0.1:6379> EXEC
1) OK
2) (error) WRONGTYPE Operation against a key holding the wrong kind of value
127.0.0.1:6379> get k1
"11"
127.0.0.1:6379> get k2
"v2"
127.0.0.1:6379>

```

#### CAS操作实现乐观锁

WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。



- **CAS? 乐观锁**？Redis官方的例子帮你理解

被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回nil-reply来表示事务已经失败。

举个例子， 假设我们需要原子性地为某个值进行增 1 操作（假设 INCR 不存在）。

首先我们可能会这样做：

```bash
val = GET mykey
val = val + 1
SET mykey $val
    
```



上面的这个实现在只有一个客户端的时候可以执行得很好。 但是， 当多个客户端同时对同一个键进行这样的操作时， 就会产生竞争条件。举个例子， 如果客户端 A 和 B 都读取了键原来的值， 比如 10 ， 那么两个客户端都会将键的值设为 11 ， 但正确的结果应该是 12 才对。

有了 WATCH ，我们就可以轻松地解决这类问题了：

```bash
WATCH mykey
val = GET mykey
val = val + 1
MULTI
SET mykey $val
EXEC
   
```



使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。

这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。

- **watch是如何监视实现的呢**？

Redis使用WATCH命令来决定事务是继续执行还是回滚，那就需要在MULTI之前使用WATCH来监控某些键值对，然后使用MULTI命令来开启事务，执行对数据结构操作的各种命令，此时这些命令入队列。

当使用EXEC执行事务时，首先会比对WATCH所监控的键值对，如果没发生改变，它会执行事务队列中的命令，提交事务；如果发生变化，将不会执行事务中的任何命令，同时事务回滚。当然无论是否回滚，Redis都会取消执行事务前的WATCH命令。

![image-20220417200900142](D:\mdImage\image-20220417200900142.png)



- **watch 命令实现监视**

在事务开始前用WATCH监控k1，之后修改k1为11，说明事务开始前k1值被改变，MULTI开始事务，修改k1值为12，k2为22，执行EXEC，发回nil，说明事务回滚；查看下k1、k2的值都没有被事务中的命令所改变。

```C
127.0.0.1:6379> set k1 v1
OK
127.0.0.1:6379> set k2 v2
OK
127.0.0.1:6379> WATCH k1
OK
127.0.0.1:6379> set k1 11
OK
127.0.0.1:6379> MULTI
OK
127.0.0.1:6379> set k1 12
QUEUED
127.0.0.1:6379> set k2 22
QUEUED
127.0.0.1:6379> EXEC
(nil)
127.0.0.1:6379> get k1
"11"
127.0.0.1:6379> get k2
"v2"
127.0.0.1:6379>

```

- **UNWATCH取消监视**

```
127.0.0.1:6379> set k1 v1
OK
127.0.0.1:6379> set k2 v2
OK
127.0.0.1:6379> WATCH k1
OK
127.0.0.1:6379> set k1 11
OK
127.0.0.1:6379> UNWATCH
OK
127.0.0.1:6379> MULTI
OK
127.0.0.1:6379> set k1 12
QUEUED
127.0.0.1:6379> set k2 22
QUEUED
127.0.0.1:6379> exec
1) OK
2) OK
127.0.0.1:6379> get k1
"12"
127.0.0.1:6379> get k2
"22"
127.0.0.1:6379>

```



### Redis事务执行步骤

通过上文命令执行，很显然Redis事务执行是三个阶段：

- **开启**：以MULTI开始一个事务
- **入队**：将多个命令入队到事务中，接到这些命令并不会立即执行，而是放到等待执行的事务队列里面
- **执行**：由EXEC命令触发事务

当一个客户端切换到事务状态之后， 服务器会根据这个客户端发来的不同命令执行不同的操作：

- 如果客户端发送的命令为 EXEC 、 DISCARD 、 WATCH 、 MULTI 四个命令的其中一个， 那么服务器立即执行这个命令。
- 与此相反， 如果客户端发送的命令是 EXEC 、 DISCARD 、 WATCH 、 MULTI 四个命令以外的其他命令， 那么服务器并不立即执行这个命令， 而是将这个命令放入一个事务队列里面， 然后向客户端返回 QUEUED 回复。

![image-20220417201310923](D:\mdImage\image-20220417201310923.png)

### 为什么 Redis 不支持回滚？

如果你有使用关系式数据库的经验， 那么 “**Redis 在事务失败时不进行回滚，而是继续执行余下的命令**”这种做法可能会让你觉得有点奇怪。



以下是这种做法的优点：

- Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。
- 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。

有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， **回滚并不能解决编程错误带来的问题**。 举个例子， 如果你本来想通过 INCR 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 INCR ， 回滚是没有办法处理这些情况的。



### 如何理解Redis与事务的ACID？

> 一般来说，事务有四个性质称为ACID，分别是原子性，一致性，隔离性和持久性。这是基础，但是很多文章对Redis 是否支持ACID有一些异议，我觉的有必要梳理下：

- **原子性atomicity**

首先通过上文知道 运行期的错误是不会回滚的，**很多文章由此说Redis事务违背原子性的；而官方文档认为是遵从原子性的**。

Redis官方文档给的理解是，**Redis的事务是原子性的：所有的命令，要么全部执行，要么全部不执行**。而不是完全成功。

- **一致性consistency**

redis事务可以保证命令失败的情况下得以回滚，数据能恢复到没有执行之前的样子，是保证一致性的，除非redis进程意外终结。

- **隔离性Isolation**

redis事务是严格遵守隔离性的，原因是redis是单进程单线程模式(v6.0之前），可以保证命令执行过程中不会被其他客户端命令打断。

但是，Redis不像其它结构化数据库有隔离级别这种设计。

- **持久性Durability**

**redis事务是不保证持久性的**，这是因为redis持久化策略中不管是RDB还是AOF都是**异步执行**的，不保证持久性是出于对性能的考虑。

### Redis事务其它实现

- 基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行，其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完
- 基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐

## 高可用：主从复制详解

我们知道要避免单点故障，即保证高可用，便需要冗余（副本）方式提供集群服务。而Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。本文主要阐述Redis的主从复制。

### 主从复制概述

主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave)；数据的复制是单向的，只能由主节点到从节点。



**主从复制的作用**主要包括：

- **数据冗余**：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。
- **故障恢复**：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。
- **负载均衡**：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。
- **高可用基石**：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。

主从库之间采用的是**读写分离**的方式。

- **读操作：主库、从库都可以接收**；
- 写操作：首先到主库执行，然后，主库将写操作同步给从库。

![image-20220417201935878](D:\mdImage\image-20220417201935878.png)

### 主从复制原理

> 注意：在2.8版本之前只有全量复制，而2.8版本后有全量和增量复制：

- `全量（同步）复制`：比如第一次同步时
- `增量（同步）复制`：只会把主从库网络断连期间主库收到的命令，同步给从库



#### 全量复制

> 当我们启动多个 Redis 实例的时候，它们相互之间就可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。

- **确立主从关系**

例如，现在有实例 1（ip：172.16.19.3）和实例 2（ip：172.16.19.5），我们在实例 2 上执行以下这个命令后，实例 2 就变成了实例 1 的从库，并从实例 1 上复制数据：

```
replicaof 172.16.19.3 6379

```

- **全量复制的三个阶段**

![image-20220417202112944](D:\mdImage\image-20220417202112944.png)





**第一阶段是主从库间建立连接、协商同步的过程**，主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。

具体来说，从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID 和复制进度 offset 两个参数。runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”。offset，此时设为 -1，表示第一次复制。主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。这里有个地方需要注意，FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说，主库会把当前所有的数据都复制给从库。

**第二阶段，主库将所有数据同步给从库**。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的 RDB 文件。

具体来说，主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空。在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis 的服务就被中断了。但是，这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作。

**第三个阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库**。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。



#### 增量复制

在 Redis 2.8 版本引入了增量复制。



- **为什么会设计增量复制**？

如果主从库在命令传播时出现了网络闪断，那么，从库就会和主库重新进行一次全量复制，开销非常大。从 Redis 2.8 开始，网络断了之后，主从库会采用增量复制的方式继续同步。

- **增量复制的流程**

![image-20220417202254810](D:\mdImage\image-20220417202254810.png)



先看两个概念： `replication buffer` 和 `repl_backlog_buffer`

`repl_backlog_buffer`：它是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量复制带来的性能开销。如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量复制，所以**repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量复制的概率**。而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer。

`replication buffer`：Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个 内存buffer进行数据交互，客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的：Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互。所以主从在增量同步时，从库作为一个client，也会分配一个buffer，只不过这个buffer专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer。

- **如果在网络断开期间，repl_backlog_size环形缓冲区写满之后，从库是会丢失掉那部分被覆盖掉的数据，还是直接进行全量复制呢**？

对于这个问题来说，有两个关键点：

1. 一个从库如果和主库断连时间过长，造成它在主库repl_backlog_buffer的slave_repl_offset位置上的数据已经被覆盖掉了，此时从库和主库间将进行全量复制。
2. 每个从库会记录自己的slave_repl_offset，每个从库的复制进度也不一定相同。在和主库重连进行恢复时，从库会通过psync命令把自己记录的slave_repl_offset发给主库，主库会根据从库各自的复制进度，来决定这个从库可以进行增量复制，还是全量复制。

### 当主服务器不进行持久化时复制的安全性

在进行主从复制设置时，强烈建议在主服务器上开启持久化，当不能这么做时，比如考虑到延迟的问题，应该将实例配置为避免自动重启。

### 为什么主从全量复制使用RDB而不使用AOF？



1、RDB文件内容是经过压缩的二进制数据（不同数据类型数据做了针对性优化），文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗，从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量复制的成本最低。

2、假设要使用AOF做全量复制，意味着必须打开AOF功能，打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量复制数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的。





### 读写分离及其中的问题

在主从复制基础上实现的读写分离，可以实现Redis的读负载均衡：由主节点提供写服务，由一个或多个从节点提供读服务（多个从节点既可以提高数据冗余程度，也可以最大化读负载能力）；在读负载较大的应用场景下，可以大大提高Redis服务器的并发量。下面介绍在使用Redis读写分离时，需要注意的问题。

- **延迟与不一致问题**

前面已经讲到，由于主从复制的命令传播是异步的，延迟与数据的不一致不可避免。如果应用对数据不一致的接受程度程度较低，可能的优化措施包括：优化主从节点之间的网络环境（如在同机房部署）；监控主从节点延迟（通过offset）判断，如果从节点延迟过大，通知应用不再通过该从节点读取数据；使用集群同时扩展写负载和读负载等。

在命令传播阶段以外的其他情况下，从节点的数据不一致可能更加严重，例如连接在数据同步阶段，或从节点失去与主节点的连接时等。从节点的slave-serve-stale-data参数便与此有关：它控制这种情况下从节点的表现；如果为yes（默认值），则从节点仍能够响应客户端的命令，如果为no，则从节点只能响应info、slaveof等少数命令。该参数的设置与应用对数据一致性的要求有关；如果对数据一致性要求很高，则应设置为no。

- **数据过期问题**

在单机版Redis中，存在两种删除策略：

- `惰性删除`：服务器不会主动删除数据，只有当客户端查询某个数据时，服务器判断该数据是否过期，如果过期则删除。
- `定期删除`：服务器执行定时任务删除过期数据，但是考虑到内存和CPU的折中（删除会释放内存，但是频繁的删除操作对CPU不友好），该删除的频率和执行时间都受到了限制。

在主从复制场景下，为了主从节点的数据一致性，从节点不会主动删除数据，而是由主节点控制从节点中过期数据的删除。由于主节点的惰性删除和定期删除策略，都不能保证主节点及时对过期数据执行删除操作，因此，当客户端通过Redis从节点读取数据时，很容易读取到已经过期的数据。

Redis 3.2中，从节点在读取数据时，增加了对数据是否过期的判断：如果该数据已过期，则不返回给客户端；将Redis升级到3.2可以解决数据过期问题。

- **故障切换问题**

在没有使用哨兵的读写分离场景下，应用针对读和写分别连接不同的Redis节点；当主节点或从节点出现问题而发生更改时，需要及时修改应用程序读写Redis数据的连接；连接的切换可以手动进行，或者自己写监控程序进行切换，但前者响应慢、容易出错，后者实现复杂，成本都不算低。

- **总结**

在使用读写分离之前，可以考虑其他方法增加Redis的读负载能力：如尽量优化主节点（减少慢查询、减少持久化等其他情况带来的阻塞等）提高负载能力；使用Redis集群同时提高读负载能力和写负载能力等。如果使用读写分离，可以使用哨兵，使主从节点的故障切换尽可能自动化，并减少对应用程序的侵入。



## 高可用：哨兵机制（Redis Sentinel）详解

在上文主从复制的基础上，如果主节点出现故障该怎么办呢？ 在 Redis 主从集群中，哨兵机制是实现主从库自动切换的关键机制，它有效地解决了主从复制模式下故障转移的问题。

### 哨兵机制（Redis Sentinel）

Redis Sentinel，即Redis哨兵，在Redis 2.8版本开始引入。哨兵的核心功能是主节点的自动故障转移。

下图是一个典型的哨兵集群监控的逻辑图：

![image-20220417203734491](D:\mdImage\image-20220417203734491.png)



哨兵实现了什么功能呢？下面是Redis官方文档的描述：

- **监控（Monitoring）**：哨兵会不断地检查主节点和从节点是否运作正常。
- **自动故障转移（Automatic failover）**：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。
- **配置提供者（Configuration provider）**：客户端在初始化时，通过连接哨兵来获得当前Redis服务的主节点地址。
- **通知（Notification）**：哨兵可以将故障转移的结果发送给客户端。

其中，监控和自动故障转移功能，使得哨兵可以及时发现主节点故障并完成转移；而配置提供者和通知功能，则需要在与客户端的交互中才能体现。

#### 哨兵集群的组建

上图中哨兵集群是如何组建的呢？哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub/sub 机制，也就是发布 / 订阅机制。



在主从集群中，主库上有一个名为`__sentinel__:hello`的频道，不同哨兵就是通过它来相互发现，实现互相通信的。在下图中，哨兵 1 把自己的 IP（172.16.19.3）和端口（26579）发布到`__sentinel__:hello`频道上，哨兵 2 和 3 订阅了该频道。那么此时，哨兵 2 和 3 就可以从这个频道直接获取哨兵 1 的 IP 地址和端口号。然后，哨兵 2、3 可以和哨兵 1 建立网络连接。

![image-20220417203845925](D:\mdImage\image-20220417203845925.png)

通过这个方式，哨兵 2 和 3 也可以建立网络连接，这样一来，哨兵集群就形成了。它们相互间可以通过网络连接进行通信，比如说对主库有没有下线这件事儿进行判断和协商。

#### 哨兵监控Redis库



> 哨兵监控什么呢？怎么监控呢？

这是由哨兵向主库发送 INFO 命令来完成的。就像下图所示，哨兵 2 给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵 1 和 3 可以通过相同的方法和从库建立连接。

![image-20220417203955219](D:\mdImage\image-20220417203955219.png)

#### 主库下线的判定



> 哨兵如何判断主库已经下线了呢？

首先要理解两个概念：**主观下线**和**客观下线**

- **主观下线**：任何一个哨兵都是可以监控探测，并作出Redis节点下线的判断；
- **客观下线**：有哨兵集群共同决定Redis节点是否下线；

当某个哨兵（如下图中的哨兵2）判断主库“主观下线”后，就会给其他哨兵发送 `is-master-down-by-addr` 命令。接着，其他哨兵会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。

![image-20220417204052975](D:\mdImage\image-20220417204052975.png)

如果赞成票数（这里是2）是大于等于哨兵配置文件中的 `quorum` 配置项（比如这里如果是quorum=2）, 则可以判定**主库客观下线**了。

#### 哨兵集群的选举

判断完主库下线后，由哪个哨兵节点来执行主从切换呢？这里就需要哨兵集群的选举机制了。

- **为什么必然会出现选举/共识机制**？

为了避免哨兵的单点情况发生，所以需要一个哨兵的分布式集群。作为分布式集群，必然涉及共识问题（即选举问题）；同时故障的转移和通知都只需要一个主的哨兵节点就可以了。

- **哨兵的选举机制是什么样的**？

哨兵的选举机制其实很简单，就是一个Raft选举算法： **选举的票数大于等于num(sentinels)/2+1时，将成为领导者，如果没有超过，继续选举**

Raft算法你可以参看这篇文章[分布式算法 - Raft算法]()

- 任何一个想成为 Leader 的哨兵，要满足两个条件
  - 第一，拿到半数以上的赞成票；
  - 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。

以 3 个哨兵为例，假设此时的 quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以了。

更进一步理解

这里很多人会搞混 **判定客观下线** 和 **是否能够主从切换（用到选举机制）** 两个概念，我们再看一个例子。

Redis 1主4从，5个哨兵，哨兵配置quorum为2，如果3个哨兵故障，当主库宕机时，哨兵能否判断主库“客观下线”？能否自动切换？

经过实际测试：

1、哨兵集群可以判定主库“主观下线”。由于quorum=2，所以当一个哨兵判断主库“主观下线”后，询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，**哨兵集群可以判定主库为“客观下线”**。

2、**但哨兵不能完成主从切换**。哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5/2+1=3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到`N/2+1`选票的结果。

#### 新主库的选出

> 主库既然判定客观下线了，那么如何从剩余的从库中选择一个新的主库呢？

- 过滤掉不健康的（下线或断线），没有回复过哨兵ping响应的从节点
- 选择`salve-priority`从节点优先级最高（redis.conf）的
- 选择复制偏移量最大，指复制最完整的从节点

![image-20220417204249780](D:\mdImage\image-20220417204249780.png)

#### 故障的转移

> 新的主库选择出来后，就可以开始进行故障的转移了。

假设根据我们一开始的图：（我们假设：判断主库客观下线了，同时选出`sentinel 3`是哨兵leader）

![image-20220417204319452](D:\mdImage\image-20220417204319452.png)

**故障转移流程如下**：

![image-20220417204332916](D:\mdImage\image-20220417204332916.png)



- 将slave-1脱离原从节点（PS: 5.0 中应该是`replicaof no one`)，升级主节点，
- 将从节点slave-2指向新的主节点
- 通知客户端主节点已更换
- 将原主节点（oldMaster）变成从节点，指向新的主节点

**转移之后**

![image-20220417204438772](D:\mdImage\image-20220417204438772.png)



## 高可拓展：分片技术（Redis Cluster）详解

前面两篇文章，[主从复制]()和[哨兵机制]()保障了高可用，就读写分离而言虽然slave节点扩展了主从的**读**并发能力，但是**写能力**和**存储能力**是无法进行扩展，就只能是master节点能够承载的上限。如果面对海量数据那么必然需要构建master（主节点分片)之间的集群，同时必然需要吸收高可用（主从复制和哨兵机制）能力，即每个master分片节点还需要有slave节点，这是分布式系统中典型的纵向扩展（集群的分片技术）的体现；所以在Redis 3.0版本中对应的设计就是Redis Cluster。

### 主要模块介绍

#### 哈希槽(Hash Slot)

Redis-cluster没有使用一致性hash，而是引入了**哈希槽**的概念。Redis-cluster中有16384(即2的14次方）个哈希槽，每个key通过CRC16校验后对16383取模来决定放置哪个槽。Cluster中的每个节点负责一部分hash槽（hash slot）。

比如集群中存在三个节点，则可能存在的一种分配如下：

- 节点A包含0到5500号哈希槽；
- 节点B包含5501到11000号哈希槽；
- 节点C包含11001 到 16384号哈希槽。

### 请求重定向



> Redis cluster采用去中心化的架构，集群的主节点各自负责一部分槽，客户端如何确定key到底会映射到哪个节点上呢？这就是我们要讲的请求重定向。

在cluster模式下，**节点对请求的处理过程**如下：

- 检查当前key是否存在当前NODE？
  - 通过crc16（key）/16384计算出slot
  - 查询负责该slot负责的节点，得到节点指针
  - 该指针与自身节点比较
- 若slot不是由自身负责，则返回MOVED重定向
- 若slot由自身负责，且key在slot中，则返回该key对应结果
- 若key不存在此slot中，检查该slot是否正在迁出（MIGRATING）？
- 若key正在迁出，返回ASK错误重定向客户端到迁移的目的服务器上
- 若Slot未迁出，检查Slot是否导入中？
- 若Slot导入中且有ASKING标记，则直接操作
- 否则返回MOVED重定向

这个过程中有两点需要具体理解下： **MOVED重定向** 和 **ASK重定向**

#### Moved 重定向

![image-20220417205336712](D:\mdImage\image-20220417205336712.png)



- 槽命中：直接返回结果
- 槽不命中：即当前键命令所请求的键不在当前请求的节点中，则当前节点会向客户端发送一个Moved 重定向，客户端根据Moved 重定向所包含的内容找到目标节点，再一次发送命令。

从下面可以看出 php 的槽位9244不在当前节点中，所以会重定向到节点 192.168.2.23:7001中。redis-cli会帮你自动重定向（如果没有集群方式启动，即没加参数 -c，redis-cli不会自动重定向），并且编写程序时，寻找目标节点的逻辑需要交予程序员手动完成。

```
cluster keyslot keyName # 得到keyName的槽

```

![image-20220417205410695](D:\mdImage\image-20220417205410695.png)

#### ASK 重定向

Ask重定向发生于集群伸缩时，集群伸缩会导致槽迁移，当我们去源节点访问时，此时数据已经可能已经迁移到了目标节点，使用Ask重定向来解决此种情况。



![image-20220417205441967](D:\mdImage\image-20220417205441967.png)



#### smart客户端

上述两种重定向的机制使得客户端的实现更加复杂，提供了smart客户端（JedisCluster）来**减低复杂性，追求更好的性能**。客户端内部负责计算/维护键-> 槽 -> 节点映射，用于快速定位目标节点。

实现原理：

- 从集群中选取一个可运行节点，使用 cluster slots得到槽和节点的映射关系

  ![image-20220417205511536](D:\mdImage\image-20220417205511536.png)



- 将上述映射关系存到本地，通过映射关系就可以直接对目标节点进行操作（CRC16(key) -> slot -> node），很好地避免了Moved重定向，并为每个节点创建JedisPool
- 至此就可以用来进行命令操作

![image-20220417205544506](D:\mdImage\image-20220417205544506.png)

……内容太多了

## 缓存问题：一致性, 穿击, 穿透, 雪崩, 污染等



### 



### 缓存淘汰策略

Redis共支持八种淘汰策略，分别是noeviction、volatile-random、volatile-ttl、volatile-lru、volatile-lfu、allkeys-lru、allkeys-random 和 allkeys-lfu 策略。

**怎么理解呢**？主要看分三类看：

- 不淘汰
  - noeviction （v4.0后默认的）
- 对设置了过期时间的数据中进行淘汰
  - 随机：volatile-random
  - ttl：volatile-ttl
  - lru：volatile-lru
  - lfu：volatile-lfu
- 全部数据进行淘汰
  - 随机：allkeys-random
  - lru：allkeys-lru
  - lfu：allkeys-lfu

> 具体对照下：

1. **noeviction**

该策略是Redis的默认策略。在这种策略下，一旦缓存被写满了，再有写请求来时，Redis 不再提供服务，而是直接返回错误。这种策略不会淘汰数据，所以无法解决缓存污染问题。一般生产环境不建议使用。

其他七种规则都会根据自己相应的规则来选择数据进行删除操作。

2. **volatile-random**

这个算法比较简单，在设置了过期时间的键值对中，进行随机删除。因为是随机删除，无法把不再访问的数据筛选出来，所以可能依然会存在缓存污染现象，无法解决缓存污染问题。

3. **volatile-ttl**

这种算法判断淘汰数据时参考的指标比随机删除时多进行一步过期时间的排序。Redis在筛选需删除的数据时，越早过期的数据越优先被选择。

4. **volatile-lru**

LRU算法：LRU 算法的全称是 Least Recently Used，按照最近最少使用的原则来筛选数据。这种模式下会使用 LRU 算法筛选设置了过期时间的键值对。

Redis优化的**LRU算法实现**：

Redis会记录每个数据的最近一次被访问的时间戳。在Redis在决定淘汰的数据时，第一次会随机选出 N 个数据，把它们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 lru 字段，把 lru 字段值最小的数据从缓存中淘汰出去。通过随机读取待删除集合，可以让Redis不用维护一个巨大的链表，也不用操作链表，进而提升性能。

Redis 选出的数据个数 N，通过 配置参数 maxmemory-samples 进行配置。个数N越大，则候选集合越大，选择到的最久未被使用的就更准确，N越小，选择到最久未被使用的数据的概率也会随之减小。

5. **volatile-lfu**

会使用 LFU 算法选择设置了过期时间的键值对。

**LFU 算法**：LFU 缓存策略是在 LRU 策略基础上，为每个数据增加了一个计数器，来统计这个数据的访问次数。当使用 LFU 策略筛选淘汰数据时，首先会根据数据的访问次数进行筛选，把访问次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU 策略再比较这两个数据的访问时效性，把距离上一次访问时间更久的数据淘汰出缓存。 Redis的LFU算法实现:

当 LFU 策略筛选数据时，Redis 会在候选集合中，根据数据 lru 字段的后 8bit 选择访问次数最少的数据进行淘汰。当访问次数相同时，再根据 lru 字段的前 16bit 值大小，选择访问时间最久远的数据进行淘汰。

Redis 只使用了 8bit 记录数据的访问次数，而 8bit 记录的最大值是 255，这样在访问快速的情况下，如果每次被访问就将访问次数加一，很快某条数据就达到最大值255，可能很多数据都是255，那么退化成LRU算法了。所以Redis为了解决这个问题，实现了一个更优的计数规则，并可以通过配置项，来控制计数器增加的速度。

**参数** ：

`lfu-log-factor` ，用计数器当前的值乘以配置项 lfu_log_factor 再加 1，再取其倒数，得到一个 p 值；然后，把这个 p 值和一个取值范围在（0，1）间的随机数 r 值比大小，只有 p 值大于 r 值时，计数器才加 1。

`lfu-decay-time`， 控制访问次数衰减。LFU 策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。然后，LFU 策略再把这个差值除以 lfu_decay_time 值，所得的结果就是数据 counter 要衰减的值。

`lfu-log-factor`设置越大，递增概率越低，lfu-decay-time设置越大，衰减速度会越慢。

我们在应用 LFU 策略时，一般可以将 lfu_log_factor 取值为 10。 如果业务应用中有短时高频访问的数据的话，建议把 lfu_decay_time 值设置为 1。可以快速衰减访问次数。

volatile-lfu 策略是 Redis 4.0 后新增。

6. **allkeys-lru**

使用 LRU 算法在所有数据中进行筛选。具体LFU算法跟上述 volatile-lru 中介绍的一致，只是筛选的数据范围是全部缓存，这里就不在重复。

7. **allkeys-random**

从所有键值对中随机选择并删除数据。volatile-random 跟 allkeys-random算法一样，随机删除就无法解决缓存污染问题。

8. **allkeys-lfu** 使用 LFU 算法在所有数据中进行筛选。具体LFU算法跟上述 volatile-lfu 中介绍的一致，只是筛选的数据范围是全部缓存，这里就不在重复。

allkeys-lfu 策略是 Redis 4.0 后新增。

## Redis 单线程模型详解

**Redis 基于Reactor 模式来设计开发了自己的一套高效的事件处理模型**

这套事件处理模型对应的是 Redis 中的文件事件处理器（file event handler）。由于文件事件处理器（file event handler）是单线程方式运行的，所以我们一般都说 Redis 是单线程模型。 

**既然是单线程，那怎么监听大量的客户端连接呢？**

Redis 通过**IO 多路复用程序** 来监听来自客户端的大量连接（或者说是监听多个 socket），它会将感兴趣的事件及类型（读、写）注册到内核中并监听每个事件是否发生。

这样的好处非常明显： **I/O 多路复用技术的使用让 Redis 不需要额外创建多余的线程来监听客户端的大量连接，降低了资源的消耗**

## Redis 没有使用多线程？为什么不使用多线程？

1. 单线程编程容易并且更容易维护；
2. Redis 的性能瓶颈不在 CPU ，主要在内存和网络；
3. 多线程就会存在死锁、线程上下文切换等问题，甚至会影响性能。

**Redis6.0 引入多线程主要是为了提高网络 IO 读写性能**

## Redis 给缓存数据设置过期时间有啥用？

一般情况下，我们设置保存的缓存数据的时候都会设置一个过期时间。为什么呢？

因为内存是有限的，如果缓存中的所有数据都是一直保存的话，分分钟直接 Out of memory。

**过期时间除了有助于缓解内存的消耗，还有什么其他用么？**很多时候，我们的业务场景就是需要某个数据只在某一时间段内存在，比如我们的短信验证码可能只在 1 分钟内有效，用户登录的 token 可能只在 1 天内有效。

## 过期的数据的删除策略

1. **惰性删除** ：只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。
2. **定期删除** ： 每隔一段时间抽取一批 key 执行删除过期 key 操作。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。

定期删除对内存更加友好，惰性删除对 CPU 更加友好。两者各有千秋，所以 Redis 采用的是 **定期删除+惰性/懒汉式删除** 。

但是，仅仅通过给 key 设置过期时间还是有问题的。因为还是可能存在定期删除和惰性删除漏掉了很多过期 key 的情况。这样就导致大量过期 key 堆积在内存里，然后就 Out of memory 了。

怎么解决这个问题呢？答案就是：**Redis 内存淘汰机制**

## Redis 内存淘汰机制

Redis 提供 6 种数据淘汰策略：

（1. 已经设置过期时间的数据集 2. 数据集 ）

（1. 最少使用的 2.快要过期的 3.随机）

1. **volatile-lru（least recently used）**：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰
2. **volatile-ttl**：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰(回收在过期集合的键，并且优先回收存活时间（TTL）较短的键)
3. **volatile-random**：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
4. **allkeys-lru（least recently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）
5. **allkeys-random**：从数据集（server.db[i].dict）中任意选择数据淘汰
6. **no-eviction**(/ɪˈvɪkʃn/)：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！

4.0 版本后增加以下两种：

1. **volatile-lfu（least frequently used）**：从已设置过期时间的数据集（server.db[i].expires）中挑选最不经常使用的数据淘汰
2. **allkeys-lfu（least frequently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key

## Redis 持久化机制(怎么保证 Redis 挂掉之后再重启数据可以进行恢复)

**Redis 的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file, AOF）**

**快照（snapshotting）持久化（RDB）**(Redis Database)

Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis 主从结构，主要用来提高 Redis 性能），还可以将快照留在原地以便重启服务器的时候使用。

**AOF（append-only file）持久化**

与快照持久化相比，AOF 持久化的**实时性**更好，因此已成为主流的持久化方案。开启 AOF 持久化后**每执行一条会更改 Redis 中的数据的命令**，Redis 就会将该命令写入到内存缓存 `server.aof_buf` 中，然后再根据 `appendfsync` 配置来决定何时将其同步到硬盘中的 AOF 文件。

AOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 `appendonly.aof`。

---

https://cloud.tencent.com/developer/article/1592779

RDB : 将内存中的数据集快照写入磁盘

![image-20220330110555474](D:\mdImage\image-20220330110555474.png)

RDB文件生成的方式有两种，一种是通过命令手动生成快照，还有一个是通过配置自动生成快照。

通过save和bgsava命令生成RDB文件：

save命令，会阻塞服务器进程，只有当RDB文件生成成功才会接着响应服务端的其他命令。

bgsave不会阻塞服务器进程，会创建一个子进程来创建RDB文件。虽然不会阻塞服务进程，其他的命令可以执行，但是有几个命令是不能执行的。

```javascript
save 
bgsave
bgrewriteaof
```

在bgsave 期间 服务器拒绝这三个命令，主要是方式线程间竞争产生问题。

通过配置文件自动生成RDB：

除了手动执行这两个命令外，还可以在配置文件中配合参数，达到条件的时候就会自动的生成RDB 打开我们的配置文件redis.conf,找到如下图，这个是默认的配置。

![image-20220330110834514](D:\mdImage\image-20220330110834514.png)

```javascript
save 900 1
表示在900秒内，如果发生了一次写操作，就触发bgsave命令生成RDB

同理 
save 300 10 在300秒内，发生了10次写操作，就触发bgsave

save 60 10000  在60秒内发生了10000次写操作，就触发bgsave
```



---

https://developer.aliyun.com/article/541097

AOF持久化以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。

---

全称是append only file. AOF 持久化的方式是通过redis服务器记录保存下所有的写命令到AOF文件存放在磁盘上，实现持久化的

![image-20220330111104381](D:\mdImage\image-20220330111104381.png)



## 缓存穿透

---

https://segmentfault.com/a/1190000039688578

缓存穿透是指，**缓存和数据库都没有的数据**，被大量请求，比如订单号不可能为`-1`，但是用户请求了大量订单号为`-1`的数据，由于数据不存在，缓存就也不会存在该数据，所有的请求都会直接穿透到数据库。
如果被恶意用户利用，疯狂请求不存在的数据，就会导致数据库压力过大，甚至垮掉。

注意：穿透的意思是，都没有，直接一路打到数据库。

**那对于这种情况，我们该如何解决呢？**

1. 接口增加业务层级的`Filter`，进行合法校验，这可以有效拦截大部分不合法的请求。
2. 作为第一点的补充，最常见的是使用布隆过滤器，针对一个或者多个维度，把可能存在的数据值hash到bitmap中，bitmap证明该数据不存在则该数据一定不存在，但是bitmap证明该数据存在也只能是可能存在，因为不同的数值hash到的bit位很有可能是一样的，hash冲突会导致误判，多个hash方法也只能是降低冲突的概率，无法做到避免。
3. 另外一个常见的方法，则是针对数据库与缓存都没有的数据，对空的结果进行缓存，但是过期时间设置得较短，一般五分钟内。而这种数据，如果数据库有写入，或者更新，必须同时刷新缓存，否则会导致不一致的问题存在。

---



缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。

有哪些解决办法？

最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。

(1)缓存无效 key **（2）布隆过滤器**

## 缓存雪崩（）

存在同一时间大面积的失效，后面的请求都直接落到了数据库上，造成数据库短时间内承受大量请求。** 

**针对 Redis 服务不可用的情况：**

1. 采用 Redis 集群，避免单机出现问题整个缓存服务都没办法使用。
2. 限流，避免同时处理大量的请求。

**针对热点缓存失效的情况：**

1. 设置不同的失效时间比如随机设置缓存的失效时间。
2. 缓存永不失效。

---

缓存雪崩

缓存雪崩是指缓存中有大量的数据，在同一个时间点，或者较短的时间段内，全部过期了，这个时候请求过来，缓存没有数据，都会请求数据库，则数据库的压力就会突增，扛不住就会宕机。

针对这种情况，一般我们都是使用以下方案：

1. 如果是热点数据，那么可以考虑设置永远不过期。
2. 缓存的过期时间除非比较严格，要不考虑设置一个波动随机值，比如理论十分钟，那这类key的缓存时间都加上一个1~3分钟，过期时间在7~13分钟内波动，有效防止都在同一个时间点上大量过期。
3. 方法1避免了有效过期的情况，但是要是所有的热点数据在一台redis服务器上，也是极其危险的，如果网络有问题，或者redis服务器挂了，那么所有的热点数据也会雪崩（查询不到），因此将热点数据打散分不到不同的机房中，也可以有效减少这种情况。
4. 也可以考虑双缓存的方式，数据库数据同步到缓存A和B，A设置过期时间，B不设置过期时间，如果A为空的时候去读B，同时异步去更新缓存，但是更新的时候需要同时更新两个缓存。

比如设置产品的缓存时间：

```java
redis.set(id,value,60*60 + Math.random()*1000);
```

缓存穿透是指数据库原本就没有的数据，请求如入无人之境，直奔数据库，而缓存击穿，则是指数据库有数据，缓存也本应该有数据，但是突然缓存过期了，这层保护屏障被击穿了，请求直奔数据库，缓存雪崩则是指很多缓存同一个时间失效了，流量全部涌入数据库，造成数据库极大的压力。



##  **缓存击穿**

缓存击穿：大量的请求同时查询一个 key 时，此时这个 key 正好失效了，就会导致大量的请求都落到数
据库。**缓存击穿是查询缓存中失效的 key，而缓存穿透是查询不存在的 key**。



---



缓存击穿是指数据库原本有得数据，但是缓存中没有（缓存穿透是都没有），一般是缓存突然失效了，这时候如果有大量用户请求该数据，缓存没有则会去数据库请求，会引发数据库压力增大，可能会瞬间打垮。

针对这类问题，一般有以下做法：

1. 如果是热点数据，那么可以考虑设置永远不过期。
2. 如果数据一定会过期，那么就需要在数据为空的时候，设置一个互斥的锁，只让一个请求通过，只有一个请求去数据库拉取数据，取完数据，不管如何都需要释放锁，异常的时候也需要释放锁，要不其他线程会一直拿不到锁。

下面是缓存击穿的时候互斥锁的写法，注意：获取锁之后操作，不管成功或者失败，都应该释放锁，而其他的请求，如果没有获取到锁，应该等待，再重试。当然，如果是需要更加全面一点，应该加上一个等待次数，比如1s中，那么也就是睡眠五次，达到这个阈值，则直接返回空，不应该过度消耗机器，以免当个不可用的场景把整个应用的服务器带挂了。

```java
    public static String getProductDescById(String id) {
        String desc = redis.get(id);
        // 缓存为空，过期了
        if (desc == null) {
            // 互斥锁，只有一个请求可以成功
            if (redis.setnx(lock_id, 1, 60) == 1) {
                try {
                    // 从数据库取出数据
                    desc = getFromDB(id);
                    redis.set(id, desc, 60 * 60 * 24);
                } catch (Exception ex) {
                    LogHelper.error(ex);
                } finally {
                    // 确保最后删除，释放锁
                    redis.del(lock_id);
                    return desc;
                }
            } else {
                // 否则睡眠200ms，接着获取锁
                Thread.sleep(200);
                return getProductDescById(id);
            }
        }
    }
```

---





## Redis 主从复制

主从复制架构仅仅用来解决数据的冗余备份,从节点仅仅用来同步数据

**无法解决: master节点出现故障的自动故障转移**

## 哨兵Sentinel机制

Sentinel（哨兵）是Redis 的高可用性解决方案：由一个或多个Sentinel 实例 组成的Sentinel 系统可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器。简单的说哨兵就是带有**自动故障转移功能的主从架构**。

**无法解决: 1.单节点并发压力问题   2.单节点内存和磁盘物理上限**



## [如何保证缓存与数据库双写时的数据一致性？](https://www.javalearn.cn/#/doc/Redis/面试题?id=_14-如何保证缓存与数据库双写时的数据一致性？)

### 1

https://www.bilibili.com/video/BV1aF411e7ur?spm_id_from=333.337.search-card.all.click

![image-20220415165127734](D:\mdImage\image-20220415165127734.png)

不要更新缓存

原因：并发时出现问题

![image-20220415165217166](D:\mdImage\image-20220415165217166.png)

出现缓存不一致



![image-20220415165413604](D:\mdImage\image-20220415165413604.png)

![image-20220415165418895](D:\mdImage\image-20220415165418895.png)



![image-20220415165637153](D:\mdImage\image-20220415165637153.png)

线程2查k为1的缓存，没查到，去查数据库



一个比较好的解决方案：

![image-20220415165904228](D:\mdImage\image-20220415165904228.png)



Cache Aside Pattern

最经典的缓存 数据库读写的模式，就是 Cache Aside Pattern。

- 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。
- 更新的时候，**先更新数据库，然后再删除缓存**。



这种模式下还会有一种不一致情况：

![image-20220415170116899](D:\mdImage\image-20220415170116899.png)

写缓存在删缓存之后完成，出现数据不一致



解决：延时双删  （我们想一个办法，让 删缓存 在 写缓存 之后发生不就OK了嘛，比如说，我们在5秒之后对k等于1的缓存，再删除一遍，只要线程2的处理时间没有超过这5秒，就可以保证删缓存 在 写缓存 之后发生）

![image-20220415170251338](D:\mdImage\image-20220415170251338.png)

### 2

IT老哥

![image-20220415170710957](D:\mdImage\image-20220415170710957.png)

更新数据的全部可能方案：

![image-20220415170732697](D:\mdImage\image-20220415170732697.png)

**为什么是删除缓存，而不是更新缓存？**

原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。

比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。

另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于**比较复杂的缓存数据计算的场景**，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，**这个缓存到底会不会被频繁访问到？**https://dockone.io/article/1282026



![image-20220415171124655](D:\mdImage\image-20220415171124655.png)

如果是读写分离的情况下，可能发生不一致的情况

解决：

![image-20220415171318640](D:\mdImage\image-20220415171318640.png)

阿里的中间件canal



更新缓存失败了如何处理：

![image-20220415171434231](D:\mdImage\image-20220415171434231.png)

有序



### 3

- **问题来源**

使用redis做一个缓冲操作，让请求先访问到redis，而不是直接访问MySQL等数据库：



![image-20220417210145285](D:\mdImage\image-20220417210145285.png)

著作权归https://pdai.tech所有。 链接：https://www.pdai.tech/md/db/nosql-redis/db-redis-x-cache.html

读取缓存步骤一般没有什么问题，但是一旦涉及到数据更新：数据库和缓存更新，就容易出现缓存(Redis)和数据库（MySQL）间的数据一致性问题。

**不管是先写MySQL数据库，再删除Redis缓存；还是先删除缓存，再写库，都有可能出现数据不一致的情况**。举一个例子：

1.如果删除了缓存Redis，还没有来得及写库MySQL，另一个线程就来读取，发现缓存为空，则去数据库中读取数据写入缓存，此时缓存中为脏数据。

2.如果先写了库，在删除缓存前，写库的线程宕机了，没有删除掉缓存，则也会出现数据不一致情况。

因为写和读是并发的，没法保证顺序,就会出现缓存和数据库的数据不一致的问题。

#### 4种相关模式

> 更新缓存的的Design Pattern有四种：Cache aside, Read through, Write through, Write behind caching; 我强烈建议你看看这篇，左耳朵耗子的文章：[缓存更新的套路  (opens new window)](https://coolshell.cn/articles/17416.html)

**节选最最常用的Cache Aside Pattern, 总结来说就是**

- **读的时候**，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。
- **更新的时候**，先更新数据库，然后再删除缓存。

其具体逻辑如下：

- **失效**：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。
- **命中**：应用程序从cache中取数据，取到后返回。
- **更新**：先把数据存到数据库中，成功后，再让缓存失效。

![image-20220417210226027](D:\mdImage\image-20220417210226027.png)

著作权归https://pdai.tech所有。 链接：https://www.pdai.tech/md/db/nosql-redis/db-redis-x-cache.html

注意，我们的更新是先更新数据库，成功后，让缓存失效。那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。

一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。

这是标准的design pattern，包括Facebook的论文《[Scaling Memcache at Facebook  (opens new window)](https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf)》也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答《[Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?  (opens new window)](https://www.quora.com/Why-does-Facebook-use-delete-to-remove-the-key-value-pair-in-Memcached-instead-of-updating-the-Memcached-during-write-request-to-the-backend)》，主要是怕两个并发的写操作导致脏数据。

那么，是不是Cache Aside这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。

但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。

所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。

#### 方案：队列 + 重试机制

![image-20220417210255837](D:\mdImage\image-20220417210255837.png)



流程如下所示

- 更新数据库数据；
- 缓存因为种种问题删除失败
- 将需要删除的key发送至消息队列
- 自己消费消息，获得需要删除的key
- 继续重试删除操作，直到成功

然而，该方案有一个缺点，对业务线代码造成大量的侵入。于是有了方案二，在方案二中，启动一个订阅程序去订阅数据库的binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信息，进行删除缓存操作。

#### 方案：异步更新缓存(基于订阅binlog的同步机制)

![image-20220417210334228](D:\mdImage\image-20220417210334228.png)



1. **技术整体思路**：

MySQL binlog增量订阅消费+消息队列+增量数据更新到redis

1）读Redis：热数据基本都在Redis

2）写MySQL: 增删改都是操作MySQL

3）更新Redis数据：MySQ的数据操作binlog，来更新到Redis

1. **Redis更新**

1）**数据操作**主要分为两大块：

- 一个是全量(将全部数据一次写入到redis)
- 一个是增量（实时更新）

这里说的是增量,指的是mysql的update、insert、delate变更数据。

2）**读取binlog后分析 ，利用消息队列,推送更新各台的redis缓存数据**。

这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。

其实这种机制，很类似MySQL的主从备份机制，因为MySQL的主备也是通过binlog来实现的数据一致性。

这里可以结合使用canal(阿里的一款开源框架)，通过该框架可以对MySQL的binlog进行订阅，而canal正是模仿了mysql的slave数据库的备份请求，使得Redis的数据更新达到了相同的效果。

当然，这里的消息推送工具你也可以采用别的第三方：kafka、rabbitMQ等来实现推送更新Redis



https://juejin.cn/post/6850418121754050567

https://dockone.io/article/1282026

https://dbaplus.cn/news-160-3389-1.html

https://www.bilibili.com/video/BV1aF411e7ur?spm_id_from=333.337.search-card.all.click

https://www.bilibili.com/video/BV1dU4y1f7Pa?spm_id_from=333.337.search-card.all.click

# 设计模式

## **单例模式的常见写法有哪些？**

单例模式属于创建型模式的一种，单例模式能够**确保一个类只有一个实例，并提供一个全局访问入口**







https://www.liaoxuefeng.com/wiki/1252599548343744/1281319214514210

> 因为这个类只有一个实例，因此，自然不能让调用方使用`new Xyz()`来创建实例了。所以，单例的构造方法必须是`private`，这样就防止了调用方自己创建实例，但是在类的内部，是可以用一个静态字段来引用唯一创建的实例的.
>
> 廖雪峰这里说的很奇怪。不能让调用方使用`new Xyz()`来创建实例
>
> ![image-20220314155033423](D:\mdImage\image-20220314155033423.png)
>
> 那我这里不就可以调用？
>
> 然后这里创建一个otherClass类
>
> ![image-20220314155109453](D:\mdImage\image-20220314155109453.png)
>
> 在这里是new不出来的
>
> **私有化构造方法，避免外部类通过 new 创建对象**（注意这里的外部类）





---

https://www.bilibili.com/video/BV1af4y1y7sS?from=search&seid=10588661161849130032&spm_id_from=333.337.0.0

![image-20220314155144514](D:\mdImage\image-20220314155144514.png)

确保一个类只有一个实例，那么就要求它的构造函数一定不能是public，就是说不能被外界进行实例化。那它的方法只能是private。接着，它只有一个实例，这个实例属于当前类，就是说这个实例是当前类的类成员变量，也就是静态变量，需要用static修饰。

综上所述呢，这种设计模式要求构造方法是private，并且拥有一个当前类的静态成员变量。

后面它要求向整个系统提供这个实例，即我们要再提供一个静态的方法，向外界提供当前类的实例。当前实例只能在内部进行实例化。

单例模式的主要作用是确保一个类只有一个实例存在，比如说序列号生成器、Web页面的计数器等等都可以使用单例模式。同时如果创建某个对象需要消耗较多资源的话，比如说访问IO，或者数据库资源的时候，我们也可以使用单例模式来减少资源的消耗。





## 单例模式的实现

实现单例模式的步骤如下：

1. **私有化构造方法，避免外部类通过 new 创建对象**（注意这里的外部类）
2. **定义一个私有的静态变量持有自己的类型**
3. **对外提供一个静态的公共方法来获取实例**
4. **如果实现了序列化接口需要保证反序列化不会重新创建对象**



## 饿汉式 线程安全

类一加载就创建对象，但容易产生垃圾对象，浪费内存空间。

优点：线程安全，没有加锁，执行效率较高

缺点：不是懒加载，类加载时就初始化，浪费内存空间

> 饿汉式单例是如何保证线程安全的呢？它是基于类加载机制避免了多线程的同步问题，但是如果类被不同的类加载器加载就会创建不同的实例。



```java
public class SingletonHunger
{
    // 1、私有化构造方法
    private SingletonHunger(){}
    // 2、定义一个静态变量指向自己类型
    private final static SingletonHunger instance = new SingletonHunger();
    // 3、对外提供一个公共的方法获取实例
    public static SingletonHunger getInstance() {
        return instance;
    }
}

```

> 为什么要加final?
>
> 声明为final的变量，必须在类加载完成时已经赋值
>
> 饿汉式加不加编译都不会报错，但是懒汉式加了final会报错
>
> ![image-20220314161219819](D:\mdImage\image-20220314161219819.png)
>
> 用 final 更多的意义在于提供语法约束。毕竟你是单例，就只有这一个实例，不可能再指向另一个实例。instance有了 final 的约束，后面再有人不小心编写了修改其指向的代码就会报语法错误。
>
> 这就好比 @Override 注解，你能保证写对方法名和参数，那不写注解也没问题，但是有了注解的约束，编译器就会帮你检查，还能防止别人乱改—— 公众号《Java课代表》作者
>
> https://www.cnblogs.com/gaohanghang/p/13575765.html



饿汉式的线程安全会被破坏：

1. 反射
2. 序列化+反序列化

反射如何破坏：

我们通过反射的方式获取到Singleton的构造函数，设置其访问权限，然后通过该方法创建一个新的对象。[参考](https://blog.csdn.net/hollis_chuang/article/details/106293699)

```java
import java.lang.reflect.Constructor;

public class reflectionDestory
{
    public static void main(String[] args) throws Exception{
        // 使用反射破坏单例
        // 获取空参构造方法
        Constructor<Singleton> declaredConstructor = Singleton.class.getDeclaredConstructor(null);
        // 设置强制访问
        declaredConstructor.setAccessible(true);
        // 创建实例
        Singleton singleton = declaredConstructor.newInstance();
        System.out.println("反射创建的实例" + singleton);
        System.out.println("正常创建的实例" + SingletonHunger.getInstance());
        System.out.println("正常创建的实例" + SingletonHunger.getInstance());
    }
}

```

```
反射创建的实例Singleton@74a14482
正常创建的实例SingletonHunger@1540e19d
正常创建的实例SingletonHunger@1540e19d
```

如何避免？

[在构造函数中加一些判断就行了](https://blog.csdn.net/hollis_chuang/article/details/106293699) 在构造函数里抛出异常

```java
    private Singleton()
    {
        if (Singleton != null)
        {
            throw new RuntimeException("Singleton constructor is called... ");
        }
    }
```

(感觉是判断类是不是已经加载过了，类加载完成后必然就已经触发 new语句了)



2. 通过序列化+反序列化的方式

```java
import java.io.*;

public class serializableDestroy
{

    public static void main(String[] args)
    {
        SingletonHunger singleton = SingletonHunger.getInstance();
        //Write Obj to file
        ObjectOutputStream oos = null;
        try {
            oos = new ObjectOutputStream(new FileOutputStream("tempFile"));
            oos.writeObject(singleton);
            //Read Obj from file
            File file = new File("tempFile");
            ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file));
            SingletonHunger singletonBySerialize = (SingletonHunger)ois.readObject();            //判断是否是同一个对象
            System.out.println("singleton : " + singleton);
            System.out.println("singletonBySerialize : " + singletonBySerialize);
            System.out.println("singleton == singletonBySerialize : " + (singleton == singletonBySerialize));
        } catch (Exception e)
        {
            e.printStackTrace();
        }
    }
}


```



如上，通过先序列化再反序列化的方式，可获取到一个新的单例对象，这就破坏了单例。

因为在对象反序列化的过程中，序列化会通过反射调用无参数的构造方法创建一个新的对象，所以，通过反序列化也能破坏单例。

如何避免？

修改反序列化策略





## 懒汉式 单线程安全 多线程不安全



```java
public class SingletonLazyUnsafe
{
    // 1、私有化构造方法
    private SingletonLazyUnsafe(){ }
    // 2、定义一个静态变量指向自己类型
    private  static SingletonLazyUnsafe instance;
    // 3、对外提供一个公共的方法获取实例
    public static SingletonLazyUnsafe getInstance() {
        // 判断为 null 的时候再创建对象
        if (instance == null) {
            instance = new SingletonLazyUnsafe();
        }
        return instance;
    }
}

```

多线程下测试

```java
public class SingletonLazyUnsafeTest
{
    public static void main(String[] args) {
        for (int i = 0; i < 3; i++) {
            new Thread(() -> {
                System.out.println("多线程创建的单例：" + SingletonLazyUnsafe.getInstance());
            }).start();
        }
    }
}

```

```
多线程创建的单例：SingletonLazyUnsafe@42be98f3
多线程创建的单例：SingletonLazyUnsafe@4359e82b
多线程创建的单例：SingletonLazyUnsafe@42be98f3
```



## 懒汉式 线程安全

### 直接加synchronized

懒汉式单例如何保证线程安全呢？通过 `synchronized` 关键字加锁保证线程安全，`synchronized` 可以添加在方法上面，也可以添加在代码块上面，这里演示添加在方法上面，存在的问题是`每一次调用 getInstance` 获取实例时都需要加锁和释放锁，这样是非常影响性能的。

```java
public class SingletonLazySafe
{
    // 1、私有化构造方法
    private SingletonLazySafe(){ }
    // 2、定义一个静态变量指向自己类型
    private static SingletonLazySafe instance;
    // 3、对外提供一个公共的方法获取实例
    public synchronized static SingletonLazySafe getInstance() { //这里加在方法上了
        if (instance == null) {
            instance = new SingletonLazySafe();
        }
        return instance;
    }

}

```





### 双重检查锁（DCL， 即 double-checked locking）

```java
public class SingletonLazySafeDCL
{
    // 1、私有化构造方法
    private SingletonLazySafeDCL() {
    }

    // 2、定义一个静态变量指向自己类型
    private volatile static SingletonLazySafeDCL instance;

    // 3、对外提供一个公共的方法获取实例
    public static SingletonLazySafeDCL getInstance() {
        // 第一重检查是否为 null
        if (instance == null) {
            // 使用 synchronized 加锁
            synchronized (SingletonLazySafeDCL.class) { //锁类对象
                // 第二重检查是否为 null
                if (instance == null) {
                    // new 关键字创建对象不是原子操作
                    instance = new SingletonLazySafeDCL();
                }
            }
        }
        return instance;
    }
}

```

为什么要二次判空：

```java
public static SingletonLazySafeDCL getInstance() {

        if (instance == null) {
			//线程B在外面等待
            synchronized (SingletonLazySafeDCL.class) { 
				//线程A获取到锁
                if (instance == null) {
                  
                    instance = new SingletonLazySafeDCL();
                }
            }
        }
        return instance;
    }
```

```java
public static SingletonLazySafeDCL getInstance() {

        if (instance == null) {
			
            synchronized (SingletonLazySafeDCL.class) { 
				//线程A释放锁，线程B获得锁，进入内部，发现不为null，直接释放锁
                if (instance == null) {
                //我感觉也是刚初始化那会会出现这种情况，创建完以后在第一层判null就已经结束了
                    instance = new SingletonLazySafeDCL();
                }
            }
        }
        return instance;
    }
```

双重检查锁中使用 `volatile` 的两个重要特性：**可见性、禁止指令重排序**

这是因为 `new` 关键字创建对象不是原子操作，创建一个对象会经历下面的步骤：

1. 在堆内存开辟内存空间
2. 调用构造方法，初始化对象
3. 引用变量指向堆内存空间

对应字节码指令如下：

![image.png](D:\mdImage\1460000040146576)

为了提高性能，编译器和处理器常常会对既定的代码执行顺序进行指令重排序，从源码到最终执行指令会经历如下流程：

源码编译器优化重排序指令级并行重排序内存系统重排序最终执行指令序列

所以经过指令重排序之后，创建对象的执行顺序可能为 `1 2 3` 或者 `1 3 2 `，因此当某个线程在乱序运行 `1 3 2` 指令的时候，引用变量指向堆内存空间，这个对象不为 null，但是没有初始化，其他线程有可能这个时候进入了 `getInstance` 的第一个 `if(instance == null)` 判断不为 nulll ，**导致错误使用了没有初始化的非 null 实例**，这样的话就会出现异常，这个就是著名的 DCL 失效问题。

当我们在引用变量上面添加 `volatile` 关键字以后，会通过在创建对象指令的前后添加内存屏障来禁止指令重排序，就可以避免这个问题，而且对 `volatile` 修饰的变量的修改对其他任何线程都是可见的。





## 静态内部类

## 枚举单例



## 单例模式深度总结文章

https://www.cnblogs.com/gaohanghang/p/13575765.html

https://segmentfault.com/a/1190000040146574





# 数据结构及算法

快排





# Spring IOC & AOP



## Spring 中的bean 是线程安全的吗？

https://cloud.tencent.com/developer/article/1743283

Spring[容器](https://cloud.tencent.com/product/tke?from=10680)中的Bean是否线程安全，容器本身并没有提供Bean的线程安全策略，因此可以说Spring容器中的Bean本身不具备线程安全的特性，但是具体还是要结合具体scope的Bean去研究。

## **Spring Bean作用域**

Spring 的 bean 作用域（scope）类型有5种：

1、singleton:单例，默认作用域。

2、prototype:原型，每次创建一个新对象。

3、request:请求，每次Http请求创建一个新对象，适用于WebApplicationContext环境下。

4、session:会话，同一个会话共享一个实例，不同会话使用不用的实例。

5、global-session:全局会话，所有会话共享一个实例。

线程安全这个问题，要从单例与原型Bean分别进行说明。

**「原型Bean」**对于原型Bean,每次创建一个新对象，也就是线程之间并不存在Bean共享，自然是不会有线程安全的问题。

**「单例Bean」**对于单例Bean,所有线程都共享一个单例实例Bean,因此是存在资源的竞争。

如果单例Bean,是一个无状态Bean，也就是线程中的操作不会对Bean的成员执行**「查询」**以外的操作，那么这个单例Bean是线程安全的。比如Spring mvc 的 Controller、Service、Dao等，这些Bean大多是无状态的，只关注于方法本身。

## Spring IOC

![img](D:\系统学习笔记\a-面试背诵\img\SpringIOC001.png)

**IoC（Inverse of Control:控制反转）** 是一种设计思想，而不是一个具体的技术实现。IoC 的思想就是将原本在程序中手动创建对象的控制权，交由 Spring 框架来管理。不过， IoC 并非 Spring 特有，在其他语言中也有应用。

**为什么叫控制反转？**

- **控制** ：指的是对象创建（实例化、管理）的权力
- **反转** ：控制权交给外部环境（Spring 框架、IoC 容器）



![img](D:\系统学习笔记\a-面试背诵\img\frc-365faceb5697f04f31399937c059c162.png)



将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注入。这样可以很大程度上简化应用的开发，把应用从复杂的依赖关系中解放出来。 **IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的**。

在实际项目中一个 Service 类可能依赖了很多其他的类，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。

**在 Spring 中， IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个 Map（key，value），Map 中存放的是各种对象**。

Spring 时代我们一般通过 XML 文件来配置 Bean，后来开发人员觉得 XML 文件来配置不太好，于是 **SpringBoot 注解配置就慢慢开始流行起来**。



## Spring AOP

AOP(Aspect-Oriented Programming:面向切面编程)**能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性**。

Spring AOP 就是**基于动态代理**的，如果要代理的对象，实现了某个接口，那么 Spring AOP 会使用 **JDK Proxy**，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候 Spring AOP 会使用 **Cglib** 生成一个被代理对象的子类来作为代理，如下图所示：

![SpringAOPProcess](D:\系统学习笔记\a-面试背诵\img\926dfc549b06d280a37397f9fd49bf9d.jpg)

当然你也可以使用 **AspectJ** ！Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。

## Spring bean

简单来说，bean 代指的就是那些被 **IoC 容器所管理的对象**。

我们需要告诉 IoC 容器帮助我们管理哪些对象，这个是通过配置元数据来定义的。配置元数据可以是 XML 文件、注解或者 Java 配置类。

### bean 的作用域有哪些?

Spring 中 Bean 的作用域通常有下面几种：

- **singleton** : 唯一 bean 实例，Spring 中的 bean 默认都是单例的，对单例设计模式的应用。
- **prototype** : 每次请求都会创建一个新的 bean 实例。
- **request** : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP request 内有效。
- **session** : 每一次来自新 session 的 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP session 内有效。
- **global-session** ： 全局 session 作用域，仅仅在基于 portlet 的 web 应用中才有意义，Spring5 已经没有了。Portlet 是能够生成语义代码(例如：HTML)片段的小型 Java Web 插件。它们基于 portlet 容器，可以像 servlet 一样处理 HTTP 请求。但是，与 servlet 不同，每个 portlet 都有不同的会话。

### 单例 bean 的线程安全问题了解吗？

大部分时候我们并没有在项目中使用多线程，所以很少有人会关注这个问题。单例 bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候是存在资源竞争的。

常见的有两种解决办法：

1. 在 bean 中尽量避免定义可变的成员变量。
2. 在类中定义一个 `ThreadLocal` 成员变量，将需要的可变成员变量保存在 `ThreadLocal` 中（推荐的一种方式）。

不过，大部分 bean 实际都是无状态（没有实例变量）的（比如 Dao、Service），这种情况下， bean 是线程安全的。

### @Component 和 @Bean 的区别是什么？

1. **`@Component` 注解作用于类，而`@Bean`注解作用于方法**。
2. `@Component`通常是通过类路径扫描来自动侦测以及自动装配到 Spring 容器中（我们可以使用 `@ComponentScan` 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。`@Bean` 注解通常是我们在标有该注解的方法中定义产生这个 bean,`@Bean`告诉了 Spring 这是某个类的实例，当我需要用它的时候还给我。
3. `@Bean` 注解比 `@Component` 注解的自定义性更强，而且很多地方我们只能通过 `@Bean` 注解来注册 bean。比如当我们引用第三方库中的类需要装配到 `Spring`容器时，则只能通过 `@Bean`来实现。

### 将一个类声明为 bean 的注解有哪些?

我们一般使用 `@Autowired` （auto 歪儿）注解**自动装配 bean**，要想把类标识成可用于 `@Autowired` 注解自动装配的 bean 的类,采用以下注解可实现：

- `@Component` ：通用的注解，可标注任意类为 `Spring` 组件。如果一个 Bean 不知道属于哪个层，可以使用`@Component` 注解标注。
- `@Repository` （[ ri'pɔzitəri ]）: 对应持久层即 Dao 层，主要用于数据库相关操作。
- `@Service` : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。
- `@Controller` : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。

## SpringMVC 工作原理

1. 客户端（浏览器）发送请求，直接请求到 `DispatcherServlet`。
2. `DispatcherServlet` 根据请求信息调用 `HandlerMapping`，解析请求对应的 `Handler`。
3. 解析到对应的 `Handler`（也就是我们平常说的 `Controller` 控制器）后，开始由 `HandlerAdapter` 适配器处理。
4. `HandlerAdapter` 会根据 `Handler`来调用真正的处理器开处理请求，并处理相应的业务逻辑。
5. 处理器处理完业务后，会返回一个 `ModelAndView` 对象，`Model` 是返回的数据对象，`View` 是个逻辑上的 `View`。
6. `ViewResolver` 会根据逻辑 `View` 查找实际的 `View`。
7. `DispaterServlet` 把返回的 `Model` 传给 `View`（视图渲染）。
8. 把 `View` 返回给请求者（浏览器）

## Spring 框架中用到了哪些设计模式？

关于下面一些设计模式的详细介绍，可以看笔主前段时间的原创文章[《面试官:“谈谈 Spring 中都用到了那些设计模式?”。》open in new window](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247485303&idx=1&sn=9e4626a1e3f001f9b0d84a6fa0cff04a&chksm=cea248bcf9d5c1aaf48b67cc52bac74eb29d6037848d6cf213b0e5466f2d1fda970db700ba41&token=255050878&lang=zh_CN#rd) 。

- **工厂设计模式** : Spring 使用工厂模式通过 `BeanFactory`、`ApplicationContext` 创建 bean 对象。
- **代理设计模式** : Spring AOP 功能的实现。
- **单例设计模式** : Spring 中的 Bean 默认都是单例的。
- **模板方法模式** : Spring 中 `jdbcTemplate`、`hibernateTemplate` 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。
- **包装器设计模式** : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。
- **观察者模式:** Spring 事件驱动模型就是观察者模式很经典的一个应用。
- **适配器模式** : Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配`Controller`。
- ......

# Spring/Spring Boot 常用注解总结

`@SpringBootApplication`

这个注解是 Spring Boot 项目的基石，创建 SpringBoot 项目之后会默认在主类加上。

我们可以把 `@SpringBootApplication`看作是 `@Configuration`、`@EnableAutoConfiguration`、`@ComponentScan` 注解的集合。

- `@EnableAutoConfiguration`：启用 SpringBoot 的自动配置机制
- `@ComponentScan`： 扫描被`@Component` (`@Service`,`@Controller`)注解的 bean，注解默认会扫描该类所在的包下所有的类。
- `@Configuration`：允许在 Spring 上下文中注册额外的 bean 或导入其他配置类





### `@Autowired`

自动导入对象到类中，被注入进的类同样要被 Spring 容器管理比如：Service 类注入到 Controller 类中。



```java
@Service
public class UserService {
  ......
}

@RestController
@RequestMapping("/users")
public class UserController {
   @Autowired
   private UserService userService;
   ......
}
```



###  `@Component`,`@Repository`,`@Service`, `@Controller`

我们一般使用 `@Autowired` 注解让 Spring 容器帮我们自动装配 bean。要想把类标识成可用于 `@Autowired` 注解自动装配的 bean 的类,可以采用以下注解实现：

- `@Component` ：通用的注解，可标注任意类为 `Spring` 组件。如果一个 Bean 不知道属于哪个层，可以使用`@Component` 注解标注。
- `@Repository` : 对应持久层即 Dao 层，主要用于数据库相关操作。
- `@Service` : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。
- `@Controller` : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面。

### `@RestController`

`@RestController`注解是`@Controller`和`@ResponseBody`的合集,表示这是个控制器 bean,并且是将函数的返回值直接填入 HTTP 响应体中,是 REST 风格的控制器。

单独使用 `@Controller` 不加 `@ResponseBody`的话一般是用在要返回一个视图的情况，这种情况属于比较传统的 Spring MVC 的应用，对应于前后端不分离的情况。`@Controller` +`@ResponseBody` 返回 JSON 或 XML 形式数据

关于`@RestController` 和 `@Controller`的对比，请看这篇文章：[@RestController vs @Controlleropen in new window](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247485544&idx=1&sn=3cc95b88979e28fe3bfe539eb421c6d8&chksm=cea247a3f9d5ceb5e324ff4b8697adc3e828ecf71a3468445e70221cce768d1e722085359907&token=1725092312&lang=zh_CN#rd)。

###  `@Scope`

声明 Spring Bean 的作用域，使用方法:



```java
@Bean
@Scope("singleton")
public Person personSingleton() {
    return new Person();
}
```



**四种常见的 Spring Bean 的作用域：**

- singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。
- prototype : 每次请求都会创建一个新的 bean 实例。
- request : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP request 内有效。
- session : 每一个 HTTP Session 会产生一个新的 bean，该 bean 仅在当前 HTTP session 内有效。

###  `@Configuration`

一般用来声明配置类，可以使用 `@Component`注解替代，不过使用`@Configuration`注解声明配置类更加语义化。



```java
@Configuration
public class AppConfig {
    @Bean
    public TransferService transferService() {
        return new TransferServiceImpl();
    }

}
```
